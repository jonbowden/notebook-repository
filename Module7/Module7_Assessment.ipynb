{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Module 7 Assessment — RAG Pipelines\n",
    "\n",
    "This assessment tests both your **practical skills** (coding tasks) and **conceptual understanding** (written task).\n",
    "\n",
    "## Assessment Structure\n",
    "- **5 Coding Tasks** (80 points): Implement RAG pipeline components\n",
    "- **1 Written Task** (20 points): Explain RAG failure modes and guardrails\n",
    "\n",
    "## Instructions\n",
    "- **Coding tasks**: Complete the code cells with the exact variable names shown\n",
    "- **Written task**: Fill in the string variable with full sentences\n",
    "- Do **not** rename variables\n",
    "- Ensure the notebook runs top-to-bottom without errors\n",
    "- You may use the module content for reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup\n",
    "\n",
    "Run this cell first to install required packages and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install sentence-transformers scikit-learn faiss-cpu\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import faiss\n",
    "from typing import List, Tuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Load the embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corpus-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Knowledge Base for All Tasks\n",
    "\n",
    "Use this knowledge base for all coding tasks. Do not modify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corpus-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knowledge base (do not modify)\n",
    "knowledge_base = [\n",
    "    {\n",
    "        \"id\": \"doc_001\",\n",
    "        \"text\": \"The central bank raised interest rates by 25 basis points to combat inflation.\",\n",
    "        \"source\": \"monetary_policy.pdf\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_002\",\n",
    "        \"text\": \"Higher borrowing costs are expected to slow consumer spending and reduce inflationary pressure.\",\n",
    "        \"source\": \"monetary_policy.pdf\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_003\",\n",
    "        \"text\": \"Mortgage rates have risen to their highest level in two decades.\",\n",
    "        \"source\": \"housing_report.pdf\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_004\",\n",
    "        \"text\": \"The Federal Reserve's dual mandate requires balancing employment with price stability.\",\n",
    "        \"source\": \"fed_overview.pdf\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_005\",\n",
    "        \"text\": \"Bank earnings improved as net interest margins widened due to higher rates.\",\n",
    "        \"source\": \"earnings_summary.pdf\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_006\",\n",
    "        \"text\": \"The championship football match ended in a dramatic penalty shootout.\",\n",
    "        \"source\": \"sports_news.pdf\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Pre-computed embeddings (provided for consistency)\n",
    "texts = [doc[\"text\"] for doc in knowledge_base]\n",
    "doc_embeddings = model.encode(texts, normalize_embeddings=True)\n",
    "\n",
    "print(f\"Knowledge base loaded: {len(knowledge_base)} documents\")\n",
    "print(f\"Embedding shape: {doc_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataclass-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Class Definition\n",
    "\n",
    "Use this data class for Tasks 1-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataclass-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RetrievedChunk:\n",
    "    \"\"\"A chunk retrieved from the knowledge base.\"\"\"\n",
    "    text: str\n",
    "    score: float\n",
    "    source: str\n",
    "    doc_id: str\n",
    "\n",
    "print(\"RetrievedChunk class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task1-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1 — Implement Retriever Function (20 points) [Coding]\n",
    "\n",
    "**Implement a retriever function that returns the top-k most similar documents.**\n",
    "\n",
    "Your function must:\n",
    "1. Encode the query using the provided model with `normalize_embeddings=True`\n",
    "2. Calculate cosine similarity between the query and all document embeddings\n",
    "3. Return a list of `RetrievedChunk` objects, sorted by score (highest first)\n",
    "\n",
    "**Function signature:**\n",
    "```python\n",
    "def retrieve_top_k(query: str, k: int = 3) -> List[RetrievedChunk]:\n",
    "```\n",
    "\n",
    "**Hints:**\n",
    "- Use `model.encode(query, normalize_embeddings=True)` to get the query embedding\n",
    "- Use `cosine_similarity()` from sklearn to compute similarities\n",
    "- Use `np.argsort()` to get indices sorted by similarity\n",
    "- Remember to access `knowledge_base` for document metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task1-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Implement the retriever function\n",
    "\n",
    "def retrieve_top_k(query: str, k: int = 3) -> List[RetrievedChunk]:\n",
    "    \"\"\"Retrieve top-k most similar documents for the query.\n",
    "    \n",
    "    Args:\n",
    "        query: The user's question\n",
    "        k: Number of documents to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        List of RetrievedChunk objects, sorted by score (highest first)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "\n",
    "# Verification (do not modify)\n",
    "if retrieve_top_k is not None:\n",
    "    test_chunks = retrieve_top_k(\"Why did the central bank raise rates?\", k=3)\n",
    "    if test_chunks:\n",
    "        print(f\"Retrieved {len(test_chunks)} chunks\")\n",
    "        print(f\"Top chunk score: {test_chunks[0].score:.3f}\")\n",
    "        print(f\"Top chunk text: {test_chunks[0].text[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task2-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2 — Build RAG Prompt (15 points) [Coding]\n",
    "\n",
    "**Implement a function that builds an evidence-first RAG prompt.**\n",
    "\n",
    "Your function must:\n",
    "1. Format retrieved chunks with numbered labels (e.g., [1], [2], [3])\n",
    "2. Include explicit grounding instructions\n",
    "3. Include permission to refuse if context is insufficient\n",
    "4. Structure: Instructions → Context → Question → Answer prompt\n",
    "\n",
    "**Function signature:**\n",
    "```python\n",
    "def build_rag_prompt(chunks: List[RetrievedChunk], question: str) -> str:\n",
    "```\n",
    "\n",
    "**Required elements in the prompt:**\n",
    "- \"CONTEXT\" section with numbered chunks\n",
    "- \"QUESTION\" section with the user's question\n",
    "- Instruction to answer ONLY using the provided context\n",
    "- Permission to say \"I don't have enough information\" if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task2-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Build RAG prompt\n",
    "\n",
    "def build_rag_prompt(chunks: List[RetrievedChunk], question: str) -> str:\n",
    "    \"\"\"Build an evidence-first RAG prompt.\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of retrieved chunks\n",
    "        question: The user's question\n",
    "        \n",
    "    Returns:\n",
    "        A formatted prompt string\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "\n",
    "# Verification (do not modify)\n",
    "if build_rag_prompt is not None and test_chunks:\n",
    "    test_prompt = build_rag_prompt(test_chunks, \"Why did the central bank raise rates?\")\n",
    "    if test_prompt:\n",
    "        print(\"Prompt built successfully!\")\n",
    "        print(f\"Prompt length: {len(test_prompt)} characters\")\n",
    "        print(f\"Contains CONTEXT: {'CONTEXT' in test_prompt}\")\n",
    "        print(f\"Contains QUESTION: {'QUESTION' in test_prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task3-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 3 — Implement Guardrails (20 points) [Coding]\n",
    "\n",
    "**Implement a retrieval function with guardrails that can refuse to answer.**\n",
    "\n",
    "Your function must:\n",
    "1. Retrieve top-k chunks using your `retrieve_top_k` function\n",
    "2. Filter chunks to only include those with score >= `min_score`\n",
    "3. If fewer than `min_chunks` remain after filtering, return a refusal\n",
    "4. Otherwise, return the filtered chunks\n",
    "\n",
    "**Function signature:**\n",
    "```python\n",
    "def retrieve_with_guardrails(\n",
    "    query: str, \n",
    "    k: int = 5, \n",
    "    min_score: float = 0.3, \n",
    "    min_chunks: int = 2\n",
    ") -> dict:\n",
    "```\n",
    "\n",
    "**Return format:**\n",
    "```python\n",
    "# If sufficient chunks:\n",
    "{\"chunks\": [...], \"refused\": False}\n",
    "\n",
    "# If insufficient chunks:\n",
    "{\"chunks\": [], \"refused\": True, \"reason\": \"...\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task3-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Implement guardrails\n",
    "\n",
    "def retrieve_with_guardrails(\n",
    "    query: str, \n",
    "    k: int = 5, \n",
    "    min_score: float = 0.3, \n",
    "    min_chunks: int = 2\n",
    ") -> dict:\n",
    "    \"\"\"Retrieve with score threshold and minimum chunk guardrails.\n",
    "    \n",
    "    Args:\n",
    "        query: The user's question\n",
    "        k: Number of chunks to initially retrieve\n",
    "        min_score: Minimum similarity score threshold\n",
    "        min_chunks: Minimum number of chunks required after filtering\n",
    "        \n",
    "    Returns:\n",
    "        dict with 'chunks', 'refused', and optionally 'reason'\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "\n",
    "# Verification (do not modify)\n",
    "if retrieve_with_guardrails is not None:\n",
    "    # Test with a relevant query (should NOT refuse)\n",
    "    result1 = retrieve_with_guardrails(\"Why did interest rates increase?\", min_score=0.3)\n",
    "    if result1:\n",
    "        print(f\"Test 1 - Relevant query: refused={result1.get('refused', 'N/A')}\")\n",
    "    \n",
    "    # Test with an irrelevant query (should refuse with high threshold)\n",
    "    result2 = retrieve_with_guardrails(\"What is the best pizza topping?\", min_score=0.5)\n",
    "    if result2:\n",
    "        print(f\"Test 2 - Irrelevant query: refused={result2.get('refused', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task4-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 4 — Complete RAG Pipeline (15 points) [Coding]\n",
    "\n",
    "**Implement a complete RAG pipeline that combines retrieval, prompt building, and guardrails.**\n",
    "\n",
    "Your function must:\n",
    "1. Use `retrieve_with_guardrails` to get chunks (or refuse)\n",
    "2. If refused, return a dict with `refused=True` and a refusal message\n",
    "3. If not refused, build a prompt using `build_rag_prompt`\n",
    "4. Return the prompt (we won't call an actual LLM in this assessment)\n",
    "\n",
    "**Function signature:**\n",
    "```python\n",
    "def rag_pipeline(question: str, min_score: float = 0.3) -> dict:\n",
    "```\n",
    "\n",
    "**Return format:**\n",
    "```python\n",
    "# If refused:\n",
    "{\"refused\": True, \"message\": \"I don't have enough information...\"}\n",
    "\n",
    "# If successful:\n",
    "{\"refused\": False, \"prompt\": \"...\", \"num_chunks\": 3}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task4-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Complete RAG pipeline\n",
    "\n",
    "def rag_pipeline(question: str, min_score: float = 0.3) -> dict:\n",
    "    \"\"\"Complete RAG pipeline with guardrails.\n",
    "    \n",
    "    Args:\n",
    "        question: The user's question\n",
    "        min_score: Minimum similarity score threshold\n",
    "        \n",
    "    Returns:\n",
    "        dict with 'refused', and either 'message' (if refused) or 'prompt' and 'num_chunks'\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "\n",
    "# Verification (do not modify)\n",
    "if rag_pipeline is not None:\n",
    "    # Test with a relevant query\n",
    "    result1 = rag_pipeline(\"Why did the central bank raise interest rates?\")\n",
    "    if result1:\n",
    "        print(f\"Test 1: refused={result1.get('refused', 'N/A')}, num_chunks={result1.get('num_chunks', 'N/A')}\")\n",
    "    \n",
    "    # Test with an irrelevant query\n",
    "    result2 = rag_pipeline(\"What is quantum computing?\", min_score=0.5)\n",
    "    if result2:\n",
    "        print(f\"Test 2: refused={result2.get('refused', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task5-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 5 — Evaluate Retrieval Quality (10 points) [Coding]\n",
    "\n",
    "**Implement a function that calculates Precision@k for retrieval evaluation.**\n",
    "\n",
    "Precision@k = (number of relevant chunks in top-k) / k\n",
    "\n",
    "A chunk is considered \"relevant\" if its text contains ANY of the expected keywords (case-insensitive).\n",
    "\n",
    "**Function signature:**\n",
    "```python\n",
    "def precision_at_k(query: str, expected_keywords: List[str], k: int = 3) -> float:\n",
    "```\n",
    "\n",
    "**Example:**\n",
    "- Query: \"Why did rates increase?\"\n",
    "- Expected keywords: [\"rate\", \"interest\", \"inflation\"]\n",
    "- If 2 out of 3 retrieved chunks contain at least one keyword, Precision@3 = 2/3 = 0.667"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task5-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5: Evaluate retrieval quality\n",
    "\n",
    "def precision_at_k(query: str, expected_keywords: List[str], k: int = 3) -> float:\n",
    "    \"\"\"Calculate Precision@k for retrieval evaluation.\n",
    "    \n",
    "    Args:\n",
    "        query: The query to evaluate\n",
    "        expected_keywords: Keywords that indicate relevance\n",
    "        k: Number of chunks to evaluate\n",
    "        \n",
    "    Returns:\n",
    "        Precision@k as a float between 0.0 and 1.0\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "\n",
    "# Verification (do not modify)\n",
    "if precision_at_k is not None:\n",
    "    p_at_k = precision_at_k(\n",
    "        \"Why did interest rates increase?\",\n",
    "        [\"rate\", \"interest\", \"inflation\", \"central bank\"],\n",
    "        k=3\n",
    "    )\n",
    "    if p_at_k is not None:\n",
    "        print(f\"Precision@3: {p_at_k:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task6-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 6 — RAG Failure Modes and Guardrails Explanation (20 points) [Written]\n",
    "\n",
    "**Prompt:** Explain RAG failure modes and why guardrails are essential for production systems.\n",
    "\n",
    "Include in your response:\n",
    "- What is a \"near-miss\" in RAG and why is it dangerous?\n",
    "- Why should RAG systems sometimes refuse to answer?\n",
    "- What guardrails would you implement in a production RAG system?\n",
    "- How does RAG shift (not eliminate) hallucination risk?\n",
    "\n",
    "Write **6–10 sentences** in your own words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task6-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6: Written explanation\n",
    "\n",
    "rag_failure_explanation = \"\"\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "submission-md",
   "metadata": {},
   "source": "---\n## Submission\n\nBefore submitting:\n1. **Restart kernel** and **Run All Cells** to ensure everything works\n2. Verify all coding tasks produce the expected outputs\n3. Verify your written explanation is complete and in your own words\n4. Save the notebook\n\n### How to Download from Colab\n1. Go to **File → Download → Download .ipynb**\n2. The file will download to your computer\n3. **Do not rename the file** — keep it as `Module7_Assessment.ipynb`\n\n### Submit\nUpload your completed notebook via the [Module 7 Assessment Form](https://docs.google.com/forms/d/e/1FAIpQLSe8mjg7U0bhqtt5qjegGbjzqhaouZl_zeKaHfzU1uBQ-UTEoQ/viewform).\n\n### Submission Checklist\n- [ ] All coding functions are implemented and working\n- [ ] Written explanation is thoughtful and **in your own words**\n- [ ] Notebook runs top-to-bottom without errors\n- [ ] Downloaded as .ipynb (not edited in a text editor)\n- [ ] File not renamed"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
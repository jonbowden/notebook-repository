{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Module 7 Assessment — RAG Pipelines (GRADING TEMPLATE)\n",
    "\n",
    "**This notebook contains hidden tests for automated grading.**\n",
    "\n",
    "DO NOT distribute to students."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install sentence-transformers scikit-learn faiss-cpu\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import faiss\n",
    "from typing import List, Tuple\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "import traceback\n",
    "\n",
    "# Load the embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corpus-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corpus-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knowledge base (do not modify)\n",
    "knowledge_base = [\n",
    "    {\n",
    "        \"id\": \"doc_001\",\n",
    "        \"text\": \"The central bank raised interest rates by 25 basis points to combat inflation.\",\n",
    "        \"source\": \"monetary_policy.pdf\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_002\",\n",
    "        \"text\": \"Higher borrowing costs are expected to slow consumer spending and reduce inflationary pressure.\",\n",
    "        \"source\": \"monetary_policy.pdf\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_003\",\n",
    "        \"text\": \"Mortgage rates have risen to their highest level in two decades.\",\n",
    "        \"source\": \"housing_report.pdf\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_004\",\n",
    "        \"text\": \"The Federal Reserve's dual mandate requires balancing employment with price stability.\",\n",
    "        \"source\": \"fed_overview.pdf\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_005\",\n",
    "        \"text\": \"Bank earnings improved as net interest margins widened due to higher rates.\",\n",
    "        \"source\": \"earnings_summary.pdf\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_006\",\n",
    "        \"text\": \"The championship football match ended in a dramatic penalty shootout.\",\n",
    "        \"source\": \"sports_news.pdf\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Pre-computed embeddings\n",
    "texts = [doc[\"text\"] for doc in knowledge_base]\n",
    "doc_embeddings = model.encode(texts, normalize_embeddings=True)\n",
    "\n",
    "print(f\"Knowledge base loaded: {len(knowledge_base)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataclass-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataclass-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RetrievedChunk:\n",
    "    \"\"\"A chunk retrieved from the knowledge base.\"\"\"\n",
    "    text: str\n",
    "    score: float\n",
    "    source: str\n",
    "    doc_id: str\n",
    "\n",
    "print(\"RetrievedChunk class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task1-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1 — Implement Retriever Function (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task1-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Implement the retriever function\n",
    "\n",
    "def retrieve_top_k(query: str, k: int = 3) -> List[RetrievedChunk]:\n",
    "    \"\"\"Retrieve top-k most similar documents for the query.\"\"\"\n",
    "    # STUDENT CODE GOES HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task2-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2 — Build RAG Prompt (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task2-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Build RAG prompt\n",
    "\n",
    "def build_rag_prompt(chunks: List[RetrievedChunk], question: str) -> str:\n",
    "    \"\"\"Build an evidence-first RAG prompt.\"\"\"\n",
    "    # STUDENT CODE GOES HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task3-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 3 — Implement Guardrails (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task3-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Implement guardrails\n",
    "\n",
    "def retrieve_with_guardrails(\n",
    "    query: str, \n",
    "    k: int = 5, \n",
    "    min_score: float = 0.3, \n",
    "    min_chunks: int = 2\n",
    ") -> dict:\n",
    "    \"\"\"Retrieve with score threshold and minimum chunk guardrails.\"\"\"\n",
    "    # STUDENT CODE GOES HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task4-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 4 — Complete RAG Pipeline (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task4-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Complete RAG pipeline\n",
    "\n",
    "def rag_pipeline(question: str, min_score: float = 0.3) -> dict:\n",
    "    \"\"\"Complete RAG pipeline with guardrails.\"\"\"\n",
    "    # STUDENT CODE GOES HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task5-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 5 — Evaluate Retrieval Quality (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task5-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5: Evaluate retrieval quality\n",
    "\n",
    "def precision_at_k(query: str, expected_keywords: List[str], k: int = 3) -> float:\n",
    "    \"\"\"Calculate Precision@k for retrieval evaluation.\"\"\"\n",
    "    # STUDENT CODE GOES HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task6-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 6 — Written Explanation (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task6-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6: Written explanation\n",
    "\n",
    "rag_failure_explanation = \"\"\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grading-header",
   "metadata": {},
   "source": [
    "---\n",
    "# HIDDEN GRADING TESTS\n",
    "\n",
    "**Everything below this line is for automated grading only.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grading-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GRADING INFRASTRUCTURE\n",
    "# ============================================================\n",
    "\n",
    "results = {\n",
    "    \"task1_retriever\": {\"points\": 0, \"max\": 20, \"feedback\": \"\"},\n",
    "    \"task2_prompt\": {\"points\": 0, \"max\": 15, \"feedback\": \"\"},\n",
    "    \"task3_guardrails\": {\"points\": 0, \"max\": 20, \"feedback\": \"\"},\n",
    "    \"task4_pipeline\": {\"points\": 0, \"max\": 15, \"feedback\": \"\"},\n",
    "    \"task5_precision\": {\"points\": 0, \"max\": 10, \"feedback\": \"\"},\n",
    "    \"task6_written\": {\"points\": 0, \"max\": 20, \"feedback\": \"\"}\n",
    "}\n",
    "\n",
    "# AI detection patterns\n",
    "AI_PHRASES = [\n",
    "    \"as an ai\", \"i'm an ai\", \"language model\", \"i cannot\", \"i can't provide\",\n",
    "    \"certainly!\", \"absolutely!\", \"great question\", \"happy to help\",\n",
    "    \"here's a comprehensive\", \"let me explain\", \"in summary\",\n",
    "    \"firstly\", \"secondly\", \"thirdly\", \"furthermore\", \"moreover\",\n",
    "    \"it's important to note\", \"it is worth noting\",\n",
    "    \"delve into\", \"dive into\", \"explore this\",\n",
    "    \"robust\", \"leverage\", \"utilize\", \"facilitate\",\n",
    "    \"comprehensive overview\", \"key takeaways\"\n",
    "]\n",
    "\n",
    "FORBIDDEN_CHARS = ['—', '–', '“', '”', '‘', '’', '…', '•']\n",
    "\n",
    "def check_ai_generated(text):\n",
    "    \"\"\"Check for AI-generated content markers.\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    issues = []\n",
    "    \n",
    "    for phrase in AI_PHRASES:\n",
    "        if phrase in text_lower:\n",
    "            issues.append(f\"AI phrase: '{phrase}'\")\n",
    "    \n",
    "    for char in FORBIDDEN_CHARS:\n",
    "        if char in text:\n",
    "            issues.append(f\"AI typography: '{char}'\")\n",
    "    \n",
    "    return issues\n",
    "\n",
    "print(\"Grading infrastructure loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grade-task1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TASK 1 GRADING: Retriever Function (20 points)\n",
    "# ============================================================\n",
    "\n",
    "def grade_task1():\n",
    "    task = results[\"task1_retriever\"]\n",
    "    feedback = []\n",
    "    \n",
    "    try:\n",
    "        # Test 1: Function exists and is callable (4 pts)\n",
    "        if retrieve_top_k is None or not callable(retrieve_top_k):\n",
    "            feedback.append(\"Function not implemented\")\n",
    "            task[\"feedback\"] = \"; \".join(feedback)\n",
    "            return\n",
    "        task[\"points\"] += 4\n",
    "        feedback.append(\"Function exists (+4)\")\n",
    "        \n",
    "        # Test 2: Returns list of RetrievedChunk (4 pts)\n",
    "        chunks = retrieve_top_k(\"Why did rates increase?\", k=3)\n",
    "        if not isinstance(chunks, list):\n",
    "            feedback.append(\"Does not return list\")\n",
    "            task[\"feedback\"] = \"; \".join(feedback)\n",
    "            return\n",
    "        if len(chunks) == 0:\n",
    "            feedback.append(\"Returns empty list\")\n",
    "            task[\"feedback\"] = \"; \".join(feedback)\n",
    "            return\n",
    "        if not isinstance(chunks[0], RetrievedChunk):\n",
    "            feedback.append(\"Does not return RetrievedChunk objects\")\n",
    "            task[\"feedback\"] = \"; \".join(feedback)\n",
    "            return\n",
    "        task[\"points\"] += 4\n",
    "        feedback.append(\"Returns RetrievedChunk list (+4)\")\n",
    "        \n",
    "        # Test 3: Returns correct number of chunks (4 pts)\n",
    "        if len(chunks) == 3:\n",
    "            task[\"points\"] += 4\n",
    "            feedback.append(\"Correct k=3 count (+4)\")\n",
    "        else:\n",
    "            feedback.append(f\"Wrong count: expected 3, got {len(chunks)}\")\n",
    "        \n",
    "        # Test 4: Sorted by score descending (4 pts)\n",
    "        scores = [c.score for c in chunks]\n",
    "        if scores == sorted(scores, reverse=True):\n",
    "            task[\"points\"] += 4\n",
    "            feedback.append(\"Correctly sorted by score (+4)\")\n",
    "        else:\n",
    "            feedback.append(\"Not sorted by score descending\")\n",
    "        \n",
    "        # Test 5: Top result is relevant (4 pts)\n",
    "        top_text = chunks[0].text.lower()\n",
    "        if \"rate\" in top_text or \"interest\" in top_text or \"central bank\" in top_text:\n",
    "            task[\"points\"] += 4\n",
    "            feedback.append(\"Top result is relevant (+4)\")\n",
    "        else:\n",
    "            feedback.append(\"Top result not relevant to query\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        feedback.append(f\"Error: {str(e)}\")\n",
    "    \n",
    "    task[\"feedback\"] = \"; \".join(feedback)\n",
    "\n",
    "grade_task1()\n",
    "print(f\"Task 1: {results['task1_retriever']['points']}/{results['task1_retriever']['max']}\")\n",
    "print(f\"Feedback: {results['task1_retriever']['feedback']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grade-task2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TASK 2 GRADING: RAG Prompt Builder (15 points)\n",
    "# ============================================================\n",
    "\n",
    "def grade_task2():\n",
    "    task = results[\"task2_prompt\"]\n",
    "    feedback = []\n",
    "    \n",
    "    try:\n",
    "        # Need Task 1 to work\n",
    "        if results[\"task1_retriever\"][\"points\"] < 8:\n",
    "            feedback.append(\"Skipped: Task 1 not working\")\n",
    "            task[\"feedback\"] = \"; \".join(feedback)\n",
    "            return\n",
    "        \n",
    "        # Test 1: Function exists and is callable (3 pts)\n",
    "        if build_rag_prompt is None or not callable(build_rag_prompt):\n",
    "            feedback.append(\"Function not implemented\")\n",
    "            task[\"feedback\"] = \"; \".join(feedback)\n",
    "            return\n",
    "        task[\"points\"] += 3\n",
    "        feedback.append(\"Function exists (+3)\")\n",
    "        \n",
    "        # Get test chunks\n",
    "        chunks = retrieve_top_k(\"Why did rates increase?\", k=3)\n",
    "        prompt = build_rag_prompt(chunks, \"Why did rates increase?\")\n",
    "        \n",
    "        # Test 2: Returns string (3 pts)\n",
    "        if not isinstance(prompt, str) or len(prompt) < 50:\n",
    "            feedback.append(\"Does not return valid string\")\n",
    "            task[\"feedback\"] = \"; \".join(feedback)\n",
    "            return\n",
    "        task[\"points\"] += 3\n",
    "        feedback.append(\"Returns valid string (+3)\")\n",
    "        \n",
    "        prompt_upper = prompt.upper()\n",
    "        \n",
    "        # Test 3: Contains CONTEXT section (3 pts)\n",
    "        if \"CONTEXT\" in prompt_upper:\n",
    "            task[\"points\"] += 3\n",
    "            feedback.append(\"Has CONTEXT section (+3)\")\n",
    "        else:\n",
    "            feedback.append(\"Missing CONTEXT section\")\n",
    "        \n",
    "        # Test 4: Contains QUESTION section (3 pts)\n",
    "        if \"QUESTION\" in prompt_upper:\n",
    "            task[\"points\"] += 3\n",
    "            feedback.append(\"Has QUESTION section (+3)\")\n",
    "        else:\n",
    "            feedback.append(\"Missing QUESTION section\")\n",
    "        \n",
    "        # Test 5: Contains grounding instruction (3 pts)\n",
    "        prompt_lower = prompt.lower()\n",
    "        if \"only\" in prompt_lower and (\"context\" in prompt_lower or \"provided\" in prompt_lower):\n",
    "            task[\"points\"] += 3\n",
    "            feedback.append(\"Has grounding instruction (+3)\")\n",
    "        else:\n",
    "            feedback.append(\"Missing grounding instruction\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        feedback.append(f\"Error: {str(e)}\")\n",
    "    \n",
    "    task[\"feedback\"] = \"; \".join(feedback)\n",
    "\n",
    "grade_task2()\n",
    "print(f\"Task 2: {results['task2_prompt']['points']}/{results['task2_prompt']['max']}\")\n",
    "print(f\"Feedback: {results['task2_prompt']['feedback']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grade-task3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TASK 3 GRADING: Guardrails (20 points)\n",
    "# ============================================================\n",
    "\n",
    "def grade_task3():\n",
    "    task = results[\"task3_guardrails\"]\n",
    "    feedback = []\n",
    "    \n",
    "    try:\n",
    "        # Need Task 1 to work\n",
    "        if results[\"task1_retriever\"][\"points\"] < 8:\n",
    "            feedback.append(\"Skipped: Task 1 not working\")\n",
    "            task[\"feedback\"] = \"; \".join(feedback)\n",
    "            return\n",
    "        \n",
    "        # Test 1: Function exists and is callable (4 pts)\n",
    "        if retrieve_with_guardrails is None or not callable(retrieve_with_guardrails):\n",
    "            feedback.append(\"Function not implemented\")\n",
    "            task[\"feedback\"] = \"; \".join(feedback)\n",
    "            return\n",
    "        task[\"points\"] += 4\n",
    "        feedback.append(\"Function exists (+4)\")\n",
    "        \n",
    "        # Test 2: Returns dict with correct keys (4 pts)\n",
    "        result = retrieve_with_guardrails(\"Why did rates increase?\", min_score=0.3)\n",
    "        if not isinstance(result, dict):\n",
    "            feedback.append(\"Does not return dict\")\n",
    "            task[\"feedback\"] = \"; \".join(feedback)\n",
    "            return\n",
    "        if \"refused\" not in result:\n",
    "            feedback.append(\"Missing 'refused' key\")\n",
    "            task[\"feedback\"] = \"; \".join(feedback)\n",
    "            return\n",
    "        task[\"points\"] += 4\n",
    "        feedback.append(\"Returns dict with refused key (+4)\")\n",
    "        \n",
    "        # Test 3: Does NOT refuse for relevant query with low threshold (4 pts)\n",
    "        result_relevant = retrieve_with_guardrails(\"Why did rates increase?\", min_score=0.2)\n",
    "        if result_relevant.get(\"refused\") == False:\n",
    "            task[\"points\"] += 4\n",
    "            feedback.append(\"Accepts relevant query (+4)\")\n",
    "        else:\n",
    "            feedback.append(\"Incorrectly refused relevant query\")\n",
    "        \n",
    "        # Test 4: DOES refuse for irrelevant query with high threshold (4 pts)\n",
    "        result_irrelevant = retrieve_with_guardrails(\"What is the best pizza topping?\", min_score=0.5, min_chunks=3)\n",
    "        if result_irrelevant.get(\"refused\") == True:\n",
    "            task[\"points\"] += 4\n",
    "            feedback.append(\"Refuses irrelevant query (+4)\")\n",
    "        else:\n",
    "            feedback.append(\"Should refuse irrelevant query with high threshold\")\n",
    "        \n",
    "        # Test 5: Has 'reason' key when refused (4 pts)\n",
    "        if result_irrelevant.get(\"refused\") == True and \"reason\" in result_irrelevant:\n",
    "            task[\"points\"] += 4\n",
    "            feedback.append(\"Provides reason on refusal (+4)\")\n",
    "        elif result_irrelevant.get(\"refused\") == True:\n",
    "            feedback.append(\"Missing 'reason' key on refusal\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        feedback.append(f\"Error: {str(e)}\")\n",
    "    \n",
    "    task[\"feedback\"] = \"; \".join(feedback)\n",
    "\n",
    "grade_task3()\n",
    "print(f\"Task 3: {results['task3_guardrails']['points']}/{results['task3_guardrails']['max']}\")\n",
    "print(f\"Feedback: {results['task3_guardrails']['feedback']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grade-task4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TASK 4 GRADING: Complete Pipeline (15 points)\n",
    "# ============================================================\n",
    "\n",
    "def grade_task4():\n",
    "    task = results[\"task4_pipeline\"]\n",
    "    feedback = []\n",
    "    \n",
    "    try:\n",
    "        # Need Tasks 2 and 3 to work\n",
    "        if results[\"task2_prompt\"][\"points\"] < 6 or results[\"task3_guardrails\"][\"points\"] < 8:\n",
    "            feedback.append(\"Skipped: Tasks 2 and 3 required\")\n",
    "            task[\"feedback\"] = \"; \".join(feedback)\n",
    "            return\n",
    "        \n",
    "        # Test 1: Function exists and is callable (3 pts)\n",
    "        if rag_pipeline is None or not callable(rag_pipeline):\n",
    "            feedback.append(\"Function not implemented\")\n",
    "            task[\"feedback\"] = \"; \".join(feedback)\n",
    "            return\n",
    "        task[\"points\"] += 3\n",
    "        feedback.append(\"Function exists (+3)\")\n",
    "        \n",
    "        # Test 2: Returns dict with correct structure (3 pts)\n",
    "        result = rag_pipeline(\"Why did rates increase?\")\n",
    "        if not isinstance(result, dict) or \"refused\" not in result:\n",
    "            feedback.append(\"Does not return dict with 'refused' key\")\n",
    "            task[\"feedback\"] = \"; \".join(feedback)\n",
    "            return\n",
    "        task[\"points\"] += 3\n",
    "        feedback.append(\"Returns dict with refused key (+3)\")\n",
    "        \n",
    "        # Test 3: Successful case has prompt and num_chunks (3 pts)\n",
    "        if result.get(\"refused\") == False:\n",
    "            if \"prompt\" in result and \"num_chunks\" in result:\n",
    "                task[\"points\"] += 3\n",
    "                feedback.append(\"Success case has prompt and num_chunks (+3)\")\n",
    "            else:\n",
    "                feedback.append(\"Missing prompt or num_chunks on success\")\n",
    "        else:\n",
    "            feedback.append(\"Unexpectedly refused relevant query\")\n",
    "        \n",
    "        # Test 4: Refusal case has message (3 pts)\n",
    "        result_refused = rag_pipeline(\"What is quantum computing?\", min_score=0.6)\n",
    "        if result_refused.get(\"refused\") == True:\n",
    "            if \"message\" in result_refused:\n",
    "                task[\"points\"] += 3\n",
    "                feedback.append(\"Refusal has message (+3)\")\n",
    "            else:\n",
    "                feedback.append(\"Missing message on refusal\")\n",
    "        else:\n",
    "            feedback.append(\"Should refuse irrelevant query\")\n",
    "        \n",
    "        # Test 5: Prompt contains context (3 pts)\n",
    "        if result.get(\"refused\") == False and result.get(\"prompt\"):\n",
    "            if \"CONTEXT\" in result[\"prompt\"].upper() or \"context\" in result[\"prompt\"].lower():\n",
    "                task[\"points\"] += 3\n",
    "                feedback.append(\"Prompt includes context (+3)\")\n",
    "            else:\n",
    "                feedback.append(\"Prompt missing context section\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        feedback.append(f\"Error: {str(e)}\")\n",
    "    \n",
    "    task[\"feedback\"] = \"; \".join(feedback)\n",
    "\n",
    "grade_task4()\n",
    "print(f\"Task 4: {results['task4_pipeline']['points']}/{results['task4_pipeline']['max']}\")\n",
    "print(f\"Feedback: {results['task4_pipeline']['feedback']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grade-task5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TASK 5 GRADING: Precision@k (10 points)\n",
    "# ============================================================\n",
    "\n",
    "def grade_task5():\n",
    "    task = results[\"task5_precision\"]\n",
    "    feedback = []\n",
    "    \n",
    "    try:\n",
    "        # Need Task 1 to work\n",
    "        if results[\"task1_retriever\"][\"points\"] < 8:\n",
    "            feedback.append(\"Skipped: Task 1 not working\")\n",
    "            task[\"feedback\"] = \"; \".join(feedback)\n",
    "            return\n",
    "        \n",
    "        # Test 1: Function exists and is callable (2 pts)\n",
    "        if precision_at_k is None or not callable(precision_at_k):\n",
    "            feedback.append(\"Function not implemented\")\n",
    "            task[\"feedback\"] = \"; \".join(feedback)\n",
    "            return\n",
    "        task[\"points\"] += 2\n",
    "        feedback.append(\"Function exists (+2)\")\n",
    "        \n",
    "        # Test 2: Returns float (2 pts)\n",
    "        p = precision_at_k(\"Why did rates increase?\", [\"rate\", \"interest\"], k=3)\n",
    "        if not isinstance(p, (int, float)):\n",
    "            feedback.append(\"Does not return numeric value\")\n",
    "            task[\"feedback\"] = \"; \".join(feedback)\n",
    "            return\n",
    "        task[\"points\"] += 2\n",
    "        feedback.append(\"Returns numeric value (+2)\")\n",
    "        \n",
    "        # Test 3: Returns value in [0, 1] range (2 pts)\n",
    "        if 0.0 <= p <= 1.0:\n",
    "            task[\"points\"] += 2\n",
    "            feedback.append(\"Value in valid range (+2)\")\n",
    "        else:\n",
    "            feedback.append(f\"Value {p} not in [0, 1]\")\n",
    "        \n",
    "        # Test 4: Reasonable value for relevant query (2 pts)\n",
    "        # With k=3 and keywords about rates, should find at least 2/3\n",
    "        if p >= 0.5:\n",
    "            task[\"points\"] += 2\n",
    "            feedback.append(f\"Reasonable precision {p:.2f} (+2)\")\n",
    "        else:\n",
    "            feedback.append(f\"Precision {p:.2f} seems too low\")\n",
    "        \n",
    "        # Test 5: Different value for irrelevant keywords (2 pts)\n",
    "        p_irrelevant = precision_at_k(\"Why did rates increase?\", [\"pizza\", \"sports\"], k=3)\n",
    "        if p_irrelevant < p:\n",
    "            task[\"points\"] += 2\n",
    "            feedback.append(\"Lower precision for irrelevant keywords (+2)\")\n",
    "        else:\n",
    "            feedback.append(\"Should have lower precision for irrelevant keywords\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        feedback.append(f\"Error: {str(e)}\")\n",
    "    \n",
    "    task[\"feedback\"] = \"; \".join(feedback)\n",
    "\n",
    "grade_task5()\n",
    "print(f\"Task 5: {results['task5_precision']['points']}/{results['task5_precision']['max']}\")\n",
    "print(f\"Feedback: {results['task5_precision']['feedback']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grade-task6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TASK 6 GRADING: Written Explanation (20 points)\n",
    "# ============================================================\n",
    "\n",
    "def grade_task6():\n",
    "    task = results[\"task6_written\"]\n",
    "    feedback = []\n",
    "    \n",
    "    try:\n",
    "        # Test 1: Variable exists and has content (4 pts)\n",
    "        if not rag_failure_explanation or len(rag_failure_explanation.strip()) < 50:\n",
    "            feedback.append(\"Explanation missing or too short\")\n",
    "            task[\"feedback\"] = \"; \".join(feedback)\n",
    "            return\n",
    "        task[\"points\"] += 4\n",
    "        feedback.append(\"Has content (+4)\")\n",
    "        \n",
    "        text = rag_failure_explanation.strip()\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Test 2: Adequate length (4 pts)\n",
    "        word_count = len(text.split())\n",
    "        if word_count >= 80:\n",
    "            task[\"points\"] += 4\n",
    "            feedback.append(f\"Good length: {word_count} words (+4)\")\n",
    "        elif word_count >= 50:\n",
    "            task[\"points\"] += 2\n",
    "            feedback.append(f\"Adequate length: {word_count} words (+2)\")\n",
    "        else:\n",
    "            feedback.append(f\"Too short: {word_count} words\")\n",
    "        \n",
    "        # Test 3: Mentions near-miss concept (3 pts)\n",
    "        near_miss_terms = [\"near-miss\", \"near miss\", \"semantically similar\", \"similar but wrong\", \"plausible but incorrect\"]\n",
    "        if any(term in text_lower for term in near_miss_terms):\n",
    "            task[\"points\"] += 3\n",
    "            feedback.append(\"Discusses near-miss (+3)\")\n",
    "        else:\n",
    "            feedback.append(\"Missing near-miss discussion\")\n",
    "        \n",
    "        # Test 4: Mentions refusal/guardrails (3 pts)\n",
    "        guardrail_terms = [\"refus\", \"guardrail\", \"threshold\", \"reject\", \"filter\"]\n",
    "        if any(term in text_lower for term in guardrail_terms):\n",
    "            task[\"points\"] += 3\n",
    "            feedback.append(\"Discusses guardrails/refusal (+3)\")\n",
    "        else:\n",
    "            feedback.append(\"Missing guardrails discussion\")\n",
    "        \n",
    "        # Test 5: Mentions risk shift (3 pts)\n",
    "        shift_terms = [\"shift\", \"not eliminate\", \"doesn't eliminate\", \"still\", \"reduce\", \"risk\"]\n",
    "        if any(term in text_lower for term in shift_terms):\n",
    "            task[\"points\"] += 3\n",
    "            feedback.append(\"Discusses risk shift (+3)\")\n",
    "        else:\n",
    "            feedback.append(\"Missing risk shift discussion\")\n",
    "        \n",
    "        # Test 6: Check for AI-generated content (-5 pts penalty)\n",
    "        ai_issues = check_ai_generated(text)\n",
    "        if ai_issues:\n",
    "            penalty = min(5, len(ai_issues) * 2)\n",
    "            task[\"points\"] = max(0, task[\"points\"] - penalty)\n",
    "            feedback.append(f\"AI markers detected (-{penalty}): {ai_issues[:3]}\")\n",
    "        else:\n",
    "            task[\"points\"] += 3\n",
    "            feedback.append(\"No AI markers (+3)\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        feedback.append(f\"Error: {str(e)}\")\n",
    "    \n",
    "    task[\"feedback\"] = \"; \".join(feedback)\n",
    "\n",
    "grade_task6()\n",
    "print(f\"Task 6: {results['task6_written']['points']}/{results['task6_written']['max']}\")\n",
    "print(f\"Feedback: {results['task6_written']['feedback']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FINAL RESULTS\n",
    "# ============================================================\n",
    "\n",
    "total_points = sum(task[\"points\"] for task in results.values())\n",
    "max_points = sum(task[\"max\"] for task in results.values())\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODULE 7 ASSESSMENT RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for task_name, task_data in results.items():\n",
    "    print(f\"\\n{task_name}:\")\n",
    "    print(f\"  Score: {task_data['points']}/{task_data['max']}\")\n",
    "    print(f\"  Feedback: {task_data['feedback']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"TOTAL: {total_points}/{max_points} ({100*total_points/max_points:.1f}%)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save results to JSON\n",
    "final_results = {\n",
    "    \"module\": \"Module 7 - RAG Pipelines\",\n",
    "    \"total_points\": total_points,\n",
    "    \"max_points\": max_points,\n",
    "    \"percentage\": round(100 * total_points / max_points, 1),\n",
    "    \"tasks\": results\n",
    "}\n",
    "\n",
    "with open(\"assessment_result.json\", \"w\") as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "\n",
    "print(\"\\nResults saved to assessment_result.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

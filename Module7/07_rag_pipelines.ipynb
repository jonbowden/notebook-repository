{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": "# Content\n\n**Module 7 — RAG Pipelines**\n\n**Retrieval-Augmented Generation as an Engineering System**\n\n---\n\n## What This Module Covers\n\n| Group | Topic | Key Skill |\n|-------|-------|----------|\n| 1 | Why RAG Exists | Understand the fundamental problem RAG solves |\n| 2 | RAG Architecture | Design component-based RAG systems |\n| 3 | Building RAG Pipelines | Implement end-to-end retrieval and generation |\n| 4 | Failure Modes & Guardrails | Handle RAG-specific failure cases |\n| 5 | Production RAG | Deploy evaluable, auditable RAG systems |\n\n---\n\n## Learning Objectives\n\nBy the end of this module, you will be able to:\n\n1. **Explain** why RAG is necessary for real-world LLM applications\n2. **Design** RAG pipelines with clear component boundaries\n3. **Implement** retrieval, prompt construction, and generation\n4. **Handle** failure modes including near-misses and low-confidence retrieval\n5. **Evaluate** both retrieval quality and generation faithfulness\n6. **Apply** RAG patterns to enterprise scenarios\n\n---\n\n## Prerequisites\n\nThis module builds directly on:\n\n| Module | Concepts Used Here |\n|--------|-------------------|\n| Module 3 | LLM behavior, hallucination patterns |\n| Module 4 | How models learn patterns, not truth |\n| Module 5 | Embeddings, vector similarity, FAISS retrieval |\n| Module 6 | LLM API clients, retries, structured output |\n\n**Module 7 is where everything comes together.**"
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "Run this cell to install dependencies and configure the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install sentence-transformers faiss-cpu requests\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "# Load embedding model (same as Module 5)\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "print(f\"Embedding model loaded: all-MiniLM-L6-v2\")\n",
    "print(f\"Embedding dimension: 384\")\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "llm-config-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## LLM Gateway Configuration\n",
    "\n",
    "Configure your LLM endpoint. Choose **one** option:\n",
    "\n",
    "| Option | When to Use | Setup |\n",
    "|--------|-------------|-------|\n",
    "| **Pinggy Tunnel** | Running Ollama locally | Start tunnel, paste URL |\n",
    "| **JBChat Server** | Classroom setting | Get API key from instructor |\n",
    "\n",
    "### Option A: Pinggy Tunnel (Local Ollama)\n",
    "```bash\n",
    "# Terminal 1: Start Ollama\n",
    "OLLAMA_HOST=0.0.0.0 ollama serve\n",
    "\n",
    "# Terminal 2: Start Pinggy tunnel\n",
    "ssh -p 443 -R0:localhost:11434 -L4300:localhost:4300 a.pinggy.io\n",
    "```\n",
    "\n",
    "### Option B: JBChat Server\n",
    "Get the API key from your instructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "llm-config-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ OPTION A: Pinggy Tunnel (for local Ollama) ------\n",
    "# LLM_BASE_URL = \"https://your-pinggy-url.a.pinggy.io\"\n",
    "# LLM_API_KEY = None\n",
    "\n",
    "# ------ OPTION B: JBChat Server (classroom) ------\n",
    "LLM_BASE_URL = \"https://jbchat.jonbowden.com.ngrok.app\"\n",
    "LLM_API_KEY = \"<provided-by-instructor>\"  # Get from instructor\n",
    "\n",
    "DEFAULT_MODEL = \"llama3.1:8b\"\n",
    "\n",
    "print(f\"LLM endpoint: {LLM_BASE_URL}\")\n",
    "print(f\"Model: {DEFAULT_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "group1-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Group 1: Why RAG Exists\n",
    "\n",
    "**The fundamental problem RAG solves**\n",
    "\n",
    "| Section | Topic |\n",
    "|---------|-------|\n",
    "| 7.1 | The Knowledge Gap Problem |\n",
    "| 7.2 | What RAG Actually Does |\n",
    "| 7.3 | RAG vs Fine-Tuning vs Prompting |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7-1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7.1 The Knowledge Gap Problem\n",
    "\n",
    "LLMs have a fundamental limitation that no amount of prompting can fix:\n",
    "\n",
    "> **LLMs have no access to your data at the moment they answer a question.**\n",
    "\n",
    "### What LLMs Know\n",
    "\n",
    "| Knowledge Type | Available? | Example |\n",
    "|---------------|------------|----------|\n",
    "| Training data (pre-cutoff) | ✅ Yes | \"What is Python?\" |\n",
    "| Recent events (post-cutoff) | ❌ No | \"What happened yesterday?\" |\n",
    "| Your internal documents | ❌ No | \"What's our refund policy?\" |\n",
    "| Your database records | ❌ No | \"What's customer #12345's status?\" |\n",
    "| Private company data | ❌ No | \"What were Q3 sales?\" |\n",
    "\n",
    "### The Hallucination Risk\n",
    "\n",
    "When asked about information they don't have, LLMs don't say \"I don't know.\"\n",
    "\n",
    "**They confidently fabricate plausible-sounding answers.**\n",
    "\n",
    "This is not a bug—it's how language models work. They generate probable continuations of text, whether or not those continuations are factually correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demo-hallucination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration: LLMs will answer questions about data they've never seen\n",
    "\n",
    "# Imagine asking an LLM about your company's internal policy\n",
    "hypothetical_question = \"What is Acme Corp's work-from-home policy?\"\n",
    "\n",
    "# Without RAG, the LLM might respond:\n",
    "hypothetical_hallucination = \"\"\"\n",
    "Acme Corp allows employees to work from home up to 3 days per week.\n",
    "Employees must be available during core hours (10am-4pm) and attend\n",
    "all mandatory team meetings in person.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Question:\", hypothetical_question)\n",
    "print(\"\\nPotential LLM response (hallucinated):\")\n",
    "print(hypothetical_hallucination)\n",
    "print(\"⚠️  This sounds authoritative but is completely fabricated!\")\n",
    "print(\"   The LLM has never seen Acme Corp's actual policy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7-2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7.2 What RAG Actually Does\n",
    "\n",
    "**RAG = Retrieval-Augmented Generation**\n",
    "\n",
    "RAG solves the knowledge gap by providing relevant information **at runtime**:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                      RAG Pipeline                           │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                                                             │\n",
    "│   User Question ─────► Retriever ─────► Relevant Chunks     │\n",
    "│                            │                   │            │\n",
    "│                            │                   ▼            │\n",
    "│                            │           Prompt Builder       │\n",
    "│                            │                   │            │\n",
    "│                            │                   ▼            │\n",
    "│                            │           LLM Generation       │\n",
    "│                            │                   │            │\n",
    "│                            │                   ▼            │\n",
    "│                            └──────────► Grounded Answer     │\n",
    "│                                                             │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### RAG Does NOT:\n",
    "\n",
    "| Misconception | Reality |\n",
    "|--------------|----------|\n",
    "| Make the model smarter | Model is unchanged |\n",
    "| Retrain the model | No training occurs |\n",
    "| Eliminate hallucinations | Reduces risk, doesn't eliminate |\n",
    "| Guarantee correctness | Still requires validation |\n",
    "\n",
    "### RAG DOES:\n",
    "\n",
    "| Capability | Benefit |\n",
    "|-----------|----------|\n",
    "| Provide runtime evidence | Answers based on actual data |\n",
    "| Ground generation | Model cites provided context |\n",
    "| Enable auditability | Can trace answer to source |\n",
    "| Support updates | New data available immediately |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7-3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7.3 RAG vs Fine-Tuning vs Prompting\n",
    "\n",
    "RAG is one of several approaches to customizing LLM behavior:\n",
    "\n",
    "| Approach | What It Does | When to Use | Limitations |\n",
    "|----------|-------------|-------------|-------------|\n",
    "| **Prompting** | Provides instructions in context | General behavior guidance | No access to external data |\n",
    "| **Fine-tuning** | Modifies model weights | Teaching new skills/patterns | Expensive, data goes stale |\n",
    "| **RAG** | Retrieves relevant data at runtime | Grounding in specific knowledge | Retrieval quality matters |\n",
    "\n",
    "### When RAG is the Right Choice\n",
    "\n",
    "✅ **Use RAG when:**\n",
    "- You need answers grounded in specific documents\n",
    "- Data changes frequently\n",
    "- You need to cite sources\n",
    "- You need auditability\n",
    "\n",
    "❌ **Don't use RAG when:**\n",
    "- Teaching the model a new task format\n",
    "- The knowledge is general/public\n",
    "- Real-time retrieval is too slow\n",
    "\n",
    "### Enterprise Reality\n",
    "\n",
    "> **Most enterprise LLM applications require RAG.**\n",
    ">\n",
    "> Fine-tuning teaches *how* to respond. RAG provides *what* to respond about."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "group2-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Group 2: RAG Architecture\n",
    "\n",
    "**Designing component-based RAG systems**\n",
    "\n",
    "| Section | Topic |\n",
    "|---------|-------|\n",
    "| 7.4 | RAG as an Architectural Pattern |\n",
    "| 7.5 | The Four Core Components |\n",
    "| 7.6 | Data Flow and Dependencies |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7-4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7.4 RAG as an Architectural Pattern\n",
    "\n",
    "RAG is not a single function or library—it's an **architectural pattern**.\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "> **RAG is a pipeline of composable components, each testable independently.**\n",
    "\n",
    "This matters because:\n",
    "\n",
    "| Principle | Benefit |\n",
    "|-----------|----------|\n",
    "| **Separation of concerns** | Each component has one job |\n",
    "| **Independent testing** | Debug retrieval separate from generation |\n",
    "| **Swappable parts** | Change embedding model without changing LLM |\n",
    "| **Clear failure attribution** | Know which component failed |\n",
    "\n",
    "### Anti-Pattern: The Monolithic RAG Function\n",
    "\n",
    "```python\n",
    "# ❌ BAD: Everything in one function\n",
    "def answer_question(query):\n",
    "    # Embed, retrieve, build prompt, call LLM, parse response...\n",
    "    # 200 lines of tangled logic\n",
    "    pass\n",
    "```\n",
    "\n",
    "### Pattern: Component-Based RAG\n",
    "\n",
    "```python\n",
    "# ✅ GOOD: Clear component boundaries\n",
    "query_embedding = embedder.encode(query)\n",
    "chunks = retriever.search(query_embedding, k=5)\n",
    "prompt = prompt_builder.build(chunks, query)\n",
    "response = generator.generate(prompt)\n",
    "answer = validator.validate(response)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7-5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7.5 The Four Core Components\n",
    "\n",
    "Every RAG system has these components (even if combined):\n",
    "\n",
    "### 1. Retriever\n",
    "\n",
    "| Responsibility | Implementation |\n",
    "|---------------|----------------|\n",
    "| Find relevant chunks | Vector similarity search |\n",
    "| Return ranked results | Top-k with scores |\n",
    "| Preserve metadata | Source, page, timestamp |\n",
    "\n",
    "### 2. Prompt Builder\n",
    "\n",
    "| Responsibility | Implementation |\n",
    "|---------------|----------------|\n",
    "| Structure the prompt | Template with placeholders |\n",
    "| Inject retrieved context | Format chunks clearly |\n",
    "| Constrain the model | Instructions for grounded answers |\n",
    "\n",
    "### 3. Generator\n",
    "\n",
    "| Responsibility | Implementation |\n",
    "|---------------|----------------|\n",
    "| Call the LLM API | HTTP client with retries |\n",
    "| Handle failures | Timeout, rate limits |\n",
    "| Return response | Raw text or structured |\n",
    "\n",
    "### 4. Validator (Optional but Recommended)\n",
    "\n",
    "| Responsibility | Implementation |\n",
    "|---------------|----------------|\n",
    "| Check response quality | Length, format, content |\n",
    "| Detect hallucination signals | Claims not in context |\n",
    "| Trigger fallback | Refusal or retry |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "component-interfaces",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Component interfaces (contracts)\n",
    "# These define what each component must do\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "\n",
    "@dataclass\n",
    "class RetrievedChunk:\n",
    "    \"\"\"A chunk retrieved from the knowledge base.\"\"\"\n",
    "    text: str\n",
    "    score: float\n",
    "    source: str = \"unknown\"\n",
    "    \n",
    "class Retriever(ABC):\n",
    "    \"\"\"Interface for retrieval components.\"\"\"\n",
    "    @abstractmethod\n",
    "    def retrieve(self, query: str, k: int = 3) -> List[RetrievedChunk]:\n",
    "        pass\n",
    "\n",
    "class PromptBuilder(ABC):\n",
    "    \"\"\"Interface for prompt construction.\"\"\"\n",
    "    @abstractmethod\n",
    "    def build(self, chunks: List[RetrievedChunk], question: str) -> str:\n",
    "        pass\n",
    "\n",
    "class Generator(ABC):\n",
    "    \"\"\"Interface for LLM generation.\"\"\"\n",
    "    @abstractmethod\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        pass\n",
    "\n",
    "print(\"Component interfaces defined:\")\n",
    "print(\"  - RetrievedChunk: data class for retrieved content\")\n",
    "print(\"  - Retriever: find relevant chunks\")\n",
    "print(\"  - PromptBuilder: construct RAG prompt\")\n",
    "print(\"  - Generator: call LLM and return response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7-6",
   "metadata": {},
   "source": "---\n\n## 7.6 Data Flow and Dependencies\n\nUnderstanding data flow helps debug RAG systems:\n\n```\n┌─────────────────────────────────────────────────────────┐\n│                    RAG Data Flow                        │\n├─────────────────────────────────────────────────────────┤\n│                                                         │\n│  [User Query]                                           │\n│       │                                                 │\n│       ▼                                                 │\n│  ┌──────────┐     ┌──────────────┐                      │\n│  │ Embedder │────►│ Query Vector │                      │\n│  └──────────┘     └──────┬───────┘                      │\n│                          │                              │\n│                          ▼                              │\n│  ┌───────────┐    ┌───────────┐    ┌─────────────────┐  │\n│  │ Vector DB │───►│ Retriever │───►│ Retrieved Chunks│  │\n│  └───────────┘    └───────────┘    └────────┬────────┘  │\n│                                             │           │\n│                                             ▼           │\n│  [User Query] ─────────────────►  ┌──────────────┐      │\n│                                   │Prompt Builder│      │\n│                                   └──────┬───────┘      │\n│                                          │              │\n│                                          ▼              │\n│                                   ┌──────────────┐      │\n│                                   │  RAG Prompt  │      │\n│                                   └──────┬───────┘      │\n│                                          │              │\n│                                          ▼              │\n│                                   ┌──────────────┐      │\n│                                   │  Generator   │      │\n│                                   └──────┬───────┘      │\n│                                          │              │\n│                                          ▼              │\n│                                  [Grounded Answer]      │\n│                                                         │\n└─────────────────────────────────────────────────────────┘\n```\n\n### Dependency Matrix\n\n| Component | Depends On | Produces |\n|-----------|-----------|----------|\n| Embedder | Query text | Query vector |\n| Retriever | Query vector, Vector DB | Ranked chunks |\n| Prompt Builder | Chunks, Query | RAG prompt string |\n| Generator | RAG prompt | LLM response |\n| Validator | LLM response, Chunks | Final answer |"
  },
  {
   "cell_type": "markdown",
   "id": "group3-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Group 3: Building RAG Pipelines\n",
    "\n",
    "**Hands-on implementation**\n",
    "\n",
    "| Section | Topic |\n",
    "|---------|-------|\n",
    "| 7.7 | Setting Up the Knowledge Base |\n",
    "| 7.8 | Implementing the Retriever |\n",
    "| 7.9 | Building Evidence-First Prompts |\n",
    "| 7.10 | Connecting to the Generator |\n",
    "| 7.11 | The Complete RAG Pipeline |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7-7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7.7 Setting Up the Knowledge Base\n",
    "\n",
    "A RAG system needs a **knowledge base**—documents to retrieve from.\n",
    "\n",
    "For this module, we'll use a corpus about central banking and interest rates (same domain as Module 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knowledge-base",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knowledge base: documents about monetary policy\n",
    "# In production, this would come from a database, files, or API\n",
    "\n",
    "knowledge_base = [\n",
    "    {\n",
    "        \"id\": \"doc_001\",\n",
    "        \"text\": \"The central bank raised interest rates by 25 basis points to combat inflation. This decision was made after reviewing economic indicators showing persistent price increases across multiple sectors.\",\n",
    "        \"source\": \"monetary_policy_report_q3.pdf\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_002\",\n",
    "        \"text\": \"Higher borrowing costs are expected to slow consumer spending and reduce inflationary pressure. The central bank indicated further rate increases may follow if inflation remains elevated.\",\n",
    "        \"source\": \"monetary_policy_report_q3.pdf\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_003\",\n",
    "        \"text\": \"Mortgage rates have risen to their highest level in two decades, causing a significant slowdown in the housing market. Home sales declined 15% compared to the previous quarter.\",\n",
    "        \"source\": \"housing_market_analysis.pdf\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_004\",\n",
    "        \"text\": \"The Federal Reserve's dual mandate requires balancing maximum employment with price stability. Current policy prioritizes inflation control over employment growth.\",\n",
    "        \"source\": \"fed_policy_overview.pdf\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_005\",\n",
    "        \"text\": \"Bank earnings improved as net interest margins widened due to higher rates. Financial sector stocks outperformed the broader market this quarter.\",\n",
    "        \"source\": \"quarterly_earnings_summary.pdf\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_006\",\n",
    "        \"text\": \"Small businesses report difficulty accessing credit as lending standards tighten. The cost of business loans has increased substantially since rate hikes began.\",\n",
    "        \"source\": \"small_business_survey.pdf\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_007\",\n",
    "        \"text\": \"International markets reacted strongly to the rate decision, with currency fluctuations affecting trade balances. Emerging markets face capital outflow pressures.\",\n",
    "        \"source\": \"global_markets_report.pdf\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_008\",\n",
    "        \"text\": \"The championship football match ended in a dramatic penalty shootout. The home team secured victory after their goalkeeper saved three consecutive penalties.\",\n",
    "        \"source\": \"sports_news.pdf\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Knowledge base loaded: {len(knowledge_base)} documents\")\n",
    "print(\"\\nSources:\")\n",
    "for source in set(doc['source'] for doc in knowledge_base):\n",
    "    count = sum(1 for doc in knowledge_base if doc['source'] == source)\n",
    "    print(f\"  - {source}: {count} document(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "index-knowledge-base",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector index from knowledge base\n",
    "# This is the \"offline\" step - done once when documents change\n",
    "\n",
    "# Extract texts and generate embeddings\n",
    "texts = [doc[\"text\"] for doc in knowledge_base]\n",
    "doc_embeddings = model.encode(texts, normalize_embeddings=True)\n",
    "\n",
    "# Create FAISS index (inner product = cosine similarity for normalized vectors)\n",
    "dimension = doc_embeddings.shape[1]  # 384\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index.add(doc_embeddings.astype('float32'))\n",
    "\n",
    "print(f\"FAISS index created:\")\n",
    "print(f\"  - Dimension: {dimension}\")\n",
    "print(f\"  - Documents indexed: {index.ntotal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7-8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7.8 Implementing the Retriever\n",
    "\n",
    "The retriever's job: **find the most relevant chunks for a query.**\n",
    "\n",
    "### Key Decisions\n",
    "\n",
    "| Parameter | Trade-off |\n",
    "|-----------|----------|\n",
    "| **k (number of results)** | More context vs. more noise |\n",
    "| **Score threshold** | Precision vs. recall |\n",
    "| **Metadata filtering** | Targeted vs. comprehensive |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retriever-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FAISSRetriever(Retriever):\n",
    "    \"\"\"Retriever using FAISS vector index.\"\"\"\n",
    "    \n",
    "    def __init__(self, index, documents, embedding_model):\n",
    "        self.index = index\n",
    "        self.documents = documents\n",
    "        self.model = embedding_model\n",
    "    \n",
    "    def retrieve(self, query: str, k: int = 3) -> List[RetrievedChunk]:\n",
    "        \"\"\"Retrieve top-k relevant chunks for the query.\"\"\"\n",
    "        # Encode query\n",
    "        query_vec = self.model.encode(\n",
    "            query, \n",
    "            normalize_embeddings=True,\n",
    "            convert_to_numpy=True\n",
    "        ).astype('float32').reshape(1, -1)\n",
    "        \n",
    "        # Search index\n",
    "        scores, indices = self.index.search(query_vec, k)\n",
    "        \n",
    "        # Build result list\n",
    "        results = []\n",
    "        for score, idx in zip(scores[0], indices[0]):\n",
    "            doc = self.documents[idx]\n",
    "            results.append(RetrievedChunk(\n",
    "                text=doc[\"text\"],\n",
    "                score=float(score),\n",
    "                source=doc[\"source\"]\n",
    "            ))\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Create retriever instance\n",
    "retriever = FAISSRetriever(index, knowledge_base, model)\n",
    "print(\"Retriever created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-retriever",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the retriever\n",
    "test_query = \"Why did the central bank raise interest rates?\"\n",
    "\n",
    "chunks = retriever.retrieve(test_query, k=3)\n",
    "\n",
    "print(f\"Query: {test_query}\")\n",
    "print(f\"\\nTop {len(chunks)} retrieved chunks:\")\n",
    "print(\"=\" * 70)\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"\\n{i}. [Score: {chunk.score:.3f}] Source: {chunk.source}\")\n",
    "    print(f\"   {chunk.text[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7-9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7.9 Building Evidence-First Prompts\n",
    "\n",
    "The prompt is where retrieval meets generation. A well-structured RAG prompt:\n",
    "\n",
    "### Evidence-First Prompting Principles\n",
    "\n",
    "| Principle | Implementation |\n",
    "|-----------|---------------|\n",
    "| **Context before question** | Retrieved evidence appears first |\n",
    "| **Explicit grounding instruction** | \"Answer ONLY based on the provided context\" |\n",
    "| **Refusal permission** | \"If the context doesn't contain the answer, say so\" |\n",
    "| **Clear structure** | Labeled sections: CONTEXT, QUESTION, ANSWER |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-builder",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGPromptBuilder(PromptBuilder):\n",
    "    \"\"\"Builds evidence-first RAG prompts.\"\"\"\n",
    "    \n",
    "    TEMPLATE = \"\"\"You are a helpful assistant that answers questions based ONLY on the provided context.\n",
    "\n",
    "IMPORTANT RULES:\n",
    "1. Answer ONLY using information from the CONTEXT below\n",
    "2. If the context does not contain enough information to answer, say \"I don't have enough information to answer this question.\"\n",
    "3. Do not use any prior knowledge or make assumptions\n",
    "4. Keep your answer concise and directly relevant to the question\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "    \n",
    "    def build(self, chunks: List[RetrievedChunk], question: str) -> str:\n",
    "        \"\"\"Build the RAG prompt from chunks and question.\"\"\"\n",
    "        # Format context from retrieved chunks\n",
    "        context_parts = []\n",
    "        for i, chunk in enumerate(chunks, 1):\n",
    "            context_parts.append(f\"[{i}] {chunk.text}\")\n",
    "        \n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "        \n",
    "        # Build final prompt\n",
    "        return self.TEMPLATE.format(\n",
    "            context=context,\n",
    "            question=question\n",
    "        )\n",
    "\n",
    "# Create prompt builder\n",
    "prompt_builder = RAGPromptBuilder()\n",
    "print(\"Prompt builder created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-prompt-builder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the prompt builder\n",
    "rag_prompt = prompt_builder.build(chunks, test_query)\n",
    "\n",
    "print(\"Generated RAG Prompt:\")\n",
    "print(\"=\" * 70)\n",
    "print(rag_prompt)\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7-10",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7.10 Connecting to the Generator\n",
    "\n",
    "The generator calls the LLM API. We'll reuse patterns from Module 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generator-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMGenerator(Generator):\n",
    "    \"\"\"Generator that calls an LLM API.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str, api_key: Optional[str] = None, model: str = \"llama3.1:8b\"):\n",
    "        self.base_url = base_url\n",
    "        self.api_key = api_key\n",
    "        self.model = model\n",
    "    \n",
    "    def generate(self, prompt: str, temperature: float = 0.1) -> str:\n",
    "        \"\"\"Generate response from LLM.\"\"\"\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"ngrok-skip-browser-warning\": \"true\"\n",
    "        }\n",
    "        \n",
    "        # Determine endpoint based on whether we have an API key\n",
    "        use_jbchat = self.api_key and self.api_key != \"<provided-by-instructor>\"\n",
    "        \n",
    "        if use_jbchat:\n",
    "            headers[\"X-API-Key\"] = self.api_key\n",
    "            endpoint = f\"{self.base_url}/chat/direct\"\n",
    "            payload = {\n",
    "                \"model\": self.model,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                \"temperature\": temperature,\n",
    "                \"stream\": False\n",
    "            }\n",
    "        else:\n",
    "            endpoint = f\"{self.base_url}/api/chat\"\n",
    "            payload = {\n",
    "                \"model\": self.model,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                \"stream\": False\n",
    "            }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(\n",
    "                endpoint, \n",
    "                headers=headers, \n",
    "                json=payload, \n",
    "                timeout=60\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            return response.json()[\"message\"][\"content\"]\n",
    "        except Exception as e:\n",
    "            return f\"[Generation Error: {e}]\"\n",
    "\n",
    "# Create generator\n",
    "generator = LLMGenerator(LLM_BASE_URL, LLM_API_KEY, DEFAULT_MODEL)\n",
    "print(f\"Generator created: {LLM_BASE_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7-11",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7.11 The Complete RAG Pipeline\n",
    "\n",
    "Now we combine all components into a complete pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGPipeline:\n",
    "    \"\"\"Complete RAG pipeline combining retrieval and generation.\"\"\"\n",
    "    \n",
    "    def __init__(self, retriever: Retriever, prompt_builder: PromptBuilder, generator: Generator):\n",
    "        self.retriever = retriever\n",
    "        self.prompt_builder = prompt_builder\n",
    "        self.generator = generator\n",
    "    \n",
    "    def answer(self, question: str, k: int = 3, verbose: bool = False) -> dict:\n",
    "        \"\"\"Answer a question using RAG.\n",
    "        \n",
    "        Returns dict with:\n",
    "        - answer: The generated response\n",
    "        - chunks: Retrieved chunks used\n",
    "        - prompt: The constructed prompt\n",
    "        \"\"\"\n",
    "        # Step 1: Retrieve relevant chunks\n",
    "        chunks = self.retriever.retrieve(question, k=k)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Retrieved {len(chunks)} chunks:\")\n",
    "            for i, c in enumerate(chunks, 1):\n",
    "                print(f\"  {i}. [{c.score:.3f}] {c.text[:50]}...\")\n",
    "            print()\n",
    "        \n",
    "        # Step 2: Build prompt\n",
    "        prompt = self.prompt_builder.build(chunks, question)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Prompt built. Calling LLM...\")\n",
    "        \n",
    "        # Step 3: Generate answer\n",
    "        answer = self.generator.generate(prompt)\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"chunks\": chunks,\n",
    "            \"prompt\": prompt\n",
    "        }\n",
    "\n",
    "# Create the complete pipeline\n",
    "rag = RAGPipeline(retriever, prompt_builder, generator)\n",
    "print(\"RAG pipeline created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the complete pipeline\n",
    "question = \"Why did the central bank raise interest rates?\"\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "result = rag.answer(question, k=3, verbose=True)\n",
    "\n",
    "print(\"\\nRAG Answer:\")\n",
    "print(\"=\" * 70)\n",
    "print(result[\"answer\"])\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-out-of-scope",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a question NOT in the knowledge base\n",
    "# A good RAG system should refuse to answer\n",
    "\n",
    "question_out_of_scope = \"What is the weather forecast for tomorrow?\"\n",
    "\n",
    "print(f\"Question: {question_out_of_scope}\")\n",
    "print(\"(This question is NOT covered by our knowledge base)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "result = rag.answer(question_out_of_scope, k=3, verbose=True)\n",
    "\n",
    "print(\"\\nRAG Answer:\")\n",
    "print(\"=\" * 70)\n",
    "print(result[\"answer\"])\n",
    "print(\"=\" * 70)\n",
    "print(\"\\n✅ A well-designed RAG system should refuse or indicate uncertainty\")\n",
    "print(\"   when the retrieved context doesn't support an answer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "group4-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Group 4: Failure Modes & Guardrails\n",
    "\n",
    "**What can go wrong in RAG systems**\n",
    "\n",
    "| Section | Topic |\n",
    "|---------|-------|\n",
    "| 7.12 | RAG-Specific Failure Modes |\n",
    "| 7.13 | The Near-Miss Problem |\n",
    "| 7.14 | Implementing Guardrails |\n",
    "| 7.15 | When to Refuse |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7-12",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7.12 RAG-Specific Failure Modes\n",
    "\n",
    "RAG reduces hallucination risk but introduces **new failure modes**:\n",
    "\n",
    "| Failure Mode | Description | Impact |\n",
    "|-------------|-------------|--------|\n",
    "| **Wrong but similar chunks** | Retrieval returns plausible but incorrect context | Grounded hallucination |\n",
    "| **Missing relevant chunks** | Best evidence not retrieved | Incomplete answer |\n",
    "| **Conflicting evidence** | Multiple chunks contradict each other | Confused response |\n",
    "| **Context overflow** | Too many chunks, model loses focus | Noise in answer |\n",
    "| **Stale data** | Knowledge base not updated | Outdated information |\n",
    "| **Citation hallucination** | Model cites sources that don't exist | False attribution |\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "> **RAG shifts the risk from \"model makes up facts\" to \"retrieval returns wrong evidence.\"**\n",
    ">\n",
    "> Both are problems. RAG is often more controllable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7-13",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7.13 The Near-Miss Problem\n",
    "\n",
    "The most dangerous failure in RAG: **near-misses**.\n",
    "\n",
    "### What is a Near-Miss?\n",
    "\n",
    "A chunk that is:\n",
    "- **Semantically similar** to the query (high retrieval score)\n",
    "- **Factually different** from what's needed\n",
    "\n",
    "### Example\n",
    "\n",
    "| Query | Retrieved Chunk | Problem |\n",
    "|-------|-----------------|----------|\n",
    "| \"What is Apple's revenue?\" | \"Apple reported Q2 revenue...\" | Wrong quarter |\n",
    "| \"What is the refund policy?\" | \"Our 2022 refund policy states...\" | Outdated policy |\n",
    "| \"What did the CEO say about AI?\" | \"The CTO commented on AI...\" | Wrong person |\n",
    "\n",
    "### Why Near-Misses are Dangerous\n",
    "\n",
    "1. **High confidence**: Model thinks it has good evidence\n",
    "2. **Plausible output**: Answer sounds correct\n",
    "3. **Hard to detect**: No obvious error signal\n",
    "4. **User trust**: Grounded answers seem authoritative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "near-miss-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate near-miss retrieval\n",
    "# Our knowledge base has sports content (the football doc)\n",
    "# that could be a near-miss for unrelated queries\n",
    "\n",
    "query_finance = \"What happened in the championship game?\"\n",
    "\n",
    "# This will retrieve the football doc even though our KB is mostly finance\n",
    "chunks = retriever.retrieve(query_finance, k=3)\n",
    "\n",
    "print(f\"Query: {query_finance}\")\n",
    "print(\"\\nRetrieved chunks:\")\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"\\n{i}. [Score: {chunk.score:.3f}]\")\n",
    "    print(f\"   {chunk.text[:80]}...\")\n",
    "\n",
    "print(\"\\n⚠️  Notice: The sports document (doc_008) is retrieved\")\n",
    "print(\"   because 'championship' matches, even though our KB\")\n",
    "print(\"   is primarily about monetary policy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7-14",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7.14 Implementing Guardrails\n",
    "\n",
    "Guardrails protect RAG systems from failure modes:\n",
    "\n",
    "| Guardrail | What It Does | When to Use |\n",
    "|-----------|-------------|-------------|\n",
    "| **Score threshold** | Reject low-confidence retrieval | Always |\n",
    "| **Chunk count validation** | Ensure minimum evidence | Critical queries |\n",
    "| **Source validation** | Verify chunks from trusted sources | Regulated domains |\n",
    "| **Response length check** | Detect overly brief/long answers | Quality control |\n",
    "| **Faithfulness check** | Verify answer uses context | High-stakes answers |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guardrails-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGPipelineWithGuardrails:\n",
    "    \"\"\"RAG pipeline with configurable guardrails.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        retriever: Retriever, \n",
    "        prompt_builder: PromptBuilder, \n",
    "        generator: Generator,\n",
    "        min_score: float = 0.3,\n",
    "        min_chunks: int = 1\n",
    "    ):\n",
    "        self.retriever = retriever\n",
    "        self.prompt_builder = prompt_builder\n",
    "        self.generator = generator\n",
    "        self.min_score = min_score\n",
    "        self.min_chunks = min_chunks\n",
    "    \n",
    "    def answer(self, question: str, k: int = 3) -> dict:\n",
    "        \"\"\"Answer with guardrails applied.\"\"\"\n",
    "        # Step 1: Retrieve\n",
    "        all_chunks = self.retriever.retrieve(question, k=k)\n",
    "        \n",
    "        # Guardrail 1: Filter by score threshold\n",
    "        valid_chunks = [\n",
    "            c for c in all_chunks \n",
    "            if c.score >= self.min_score\n",
    "        ]\n",
    "        \n",
    "        # Guardrail 2: Check minimum chunk count\n",
    "        if len(valid_chunks) < self.min_chunks:\n",
    "            return {\n",
    "                \"answer\": \"I don't have enough relevant information to answer this question confidently.\",\n",
    "                \"chunks\": all_chunks,\n",
    "                \"refused\": True,\n",
    "                \"reason\": f\"Only {len(valid_chunks)} chunks above threshold {self.min_score}\"\n",
    "            }\n",
    "        \n",
    "        # Step 2: Build prompt with valid chunks only\n",
    "        prompt = self.prompt_builder.build(valid_chunks, question)\n",
    "        \n",
    "        # Step 3: Generate\n",
    "        answer = self.generator.generate(prompt)\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"chunks\": valid_chunks,\n",
    "            \"refused\": False\n",
    "        }\n",
    "\n",
    "# Create pipeline with guardrails\n",
    "rag_guarded = RAGPipelineWithGuardrails(\n",
    "    retriever, \n",
    "    prompt_builder, \n",
    "    generator,\n",
    "    min_score=0.4,  # Require 40% similarity\n",
    "    min_chunks=2    # Require at least 2 relevant chunks\n",
    ")\n",
    "print(\"Guarded RAG pipeline created\")\n",
    "print(f\"  - Min score threshold: 0.4\")\n",
    "print(f\"  - Min chunks required: 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-guardrails",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test guardrails with a well-covered question\n",
    "print(\"Test 1: Question well-covered by knowledge base\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "result = rag_guarded.answer(\"Why did the central bank raise rates?\")\n",
    "print(f\"Refused: {result['refused']}\")\n",
    "print(f\"Chunks used: {len(result['chunks'])}\")\n",
    "print(f\"\\nAnswer: {result['answer'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-guardrails-refusal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test guardrails with an out-of-scope question\n",
    "print(\"Test 2: Question NOT covered by knowledge base\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "result = rag_guarded.answer(\"What is the best programming language?\")\n",
    "print(f\"Refused: {result['refused']}\")\n",
    "if result['refused']:\n",
    "    print(f\"Reason: {result['reason']}\")\n",
    "print(f\"\\nAnswer: {result['answer']}\")\n",
    "\n",
    "print(\"\\n✅ Guardrails prevent the system from hallucinating\")\n",
    "print(\"   when retrieval doesn't find relevant evidence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7-15",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7.15 When to Refuse\n",
    "\n",
    "**Refusal is a feature, not a failure.**\n",
    "\n",
    "### When RAG Should Refuse\n",
    "\n",
    "| Condition | Action |\n",
    "|-----------|--------|\n",
    "| No chunks above score threshold | Refuse |\n",
    "| Chunks are from wrong domain | Refuse |\n",
    "| Query asks for speculation | Refuse |\n",
    "| Conflicting evidence | Acknowledge uncertainty |\n",
    "\n",
    "### Refusal Patterns\n",
    "\n",
    "| Pattern | Example Response |\n",
    "|---------|------------------|\n",
    "| **No information** | \"I don't have information about that topic.\" |\n",
    "| **Low confidence** | \"Based on limited evidence, I cannot confidently answer.\" |\n",
    "| **Out of scope** | \"This question is outside my knowledge base.\" |\n",
    "\n",
    "### Enterprise Reality\n",
    "\n",
    "> **In regulated environments, a wrong answer is far more costly than no answer.**\n",
    ">\n",
    "> Banks, healthcare, legal: refusal is risk management."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "group5-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Group 5: Production RAG\n",
    "\n",
    "**Deploying evaluable, auditable RAG systems**\n",
    "\n",
    "| Section | Topic |\n",
    "|---------|-------|\n",
    "| 7.16 | Evaluating Retrieval Quality |\n",
    "| 7.17 | Evaluating Generation Faithfulness |\n",
    "| 7.18 | Caching and Performance |\n",
    "| 7.19 | Observability and Audit Trails |\n",
    "| 7.20 | RAG as a Platform Capability |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7-16",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7.16 Evaluating Retrieval Quality\n",
    "\n",
    "RAG quality starts with retrieval quality. Poor retrieval = poor answers.\n",
    "\n",
    "### Retrieval Metrics\n",
    "\n",
    "| Metric | What It Measures | How to Compute |\n",
    "|--------|-----------------|----------------|\n",
    "| **Precision@k** | Relevant chunks in top-k | Relevant / k |\n",
    "| **Recall@k** | Coverage of all relevant chunks | Retrieved relevant / Total relevant |\n",
    "| **MRR** | Position of first relevant chunk | 1 / rank of first relevant |\n",
    "| **NDCG** | Ranking quality weighted by position | Complex formula |\n",
    "\n",
    "### Practical Evaluation\n",
    "\n",
    "For most applications, simple checks work:\n",
    "\n",
    "1. **Does the top chunk answer the question?**\n",
    "2. **Are retrieved chunks from appropriate sources?**\n",
    "3. **Is retrieval score reasonable?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retrieval-evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrieval(query: str, expected_keywords: list, k: int = 3):\n",
    "    \"\"\"Simple retrieval evaluation.\"\"\"\n",
    "    chunks = retriever.retrieve(query, k=k)\n",
    "    \n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Expected keywords: {expected_keywords}\")\n",
    "    print(\"\\nRetrieved chunks:\")\n",
    "    \n",
    "    hits = 0\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        text_lower = chunk.text.lower()\n",
    "        matched = [kw for kw in expected_keywords if kw.lower() in text_lower]\n",
    "        \n",
    "        status = \"✅\" if matched else \"❌\"\n",
    "        hits += 1 if matched else 0\n",
    "        \n",
    "        print(f\"  {i}. [{chunk.score:.3f}] {status} Keywords: {matched}\")\n",
    "        print(f\"     {chunk.text[:60]}...\")\n",
    "    \n",
    "    precision = hits / k\n",
    "    print(f\"\\nPrecision@{k}: {precision:.1%}\")\n",
    "    return precision\n",
    "\n",
    "# Evaluate retrieval for a test query\n",
    "evaluate_retrieval(\n",
    "    \"Why did interest rates increase?\",\n",
    "    [\"interest\", \"rate\", \"inflation\", \"central bank\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7-17",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7.17 Evaluating Generation Faithfulness\n",
    "\n",
    "**Faithfulness**: Does the answer only use information from the provided context?\n",
    "\n",
    "### Faithfulness Checks\n",
    "\n",
    "| Check | Question |\n",
    "|-------|----------|\n",
    "| **Grounding** | Can every claim be traced to context? |\n",
    "| **No hallucination** | Does the answer avoid inventing facts? |\n",
    "| **Appropriate refusal** | Does it refuse when context is insufficient? |\n",
    "\n",
    "### Manual Evaluation Template\n",
    "\n",
    "For each answer, ask:\n",
    "\n",
    "1. Is this answer supported by the retrieved chunks? (Yes/No/Partial)\n",
    "2. Does the answer add information not in the chunks? (Yes/No)\n",
    "3. Is the answer's confidence appropriate? (Yes/No)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faithfulness-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_faithfulness_simple(answer: str, chunks: List[RetrievedChunk]) -> dict:\n",
    "    \"\"\"Simple faithfulness heuristics.\"\"\"\n",
    "    # Combine all chunk text\n",
    "    context_text = \" \".join(c.text.lower() for c in chunks)\n",
    "    answer_lower = answer.lower()\n",
    "    \n",
    "    # Check for common hallucination signals\n",
    "    hallucination_phrases = [\n",
    "        \"i think\", \"probably\", \"might be\", \"i believe\",\n",
    "        \"generally speaking\", \"in my opinion\", \"typically\"\n",
    "    ]\n",
    "    \n",
    "    found_phrases = [p for p in hallucination_phrases if p in answer_lower]\n",
    "    \n",
    "    # Check if answer is appropriately uncertain when needed\n",
    "    uncertainty_phrases = [\n",
    "        \"don't have\", \"cannot\", \"no information\", \n",
    "        \"not mentioned\", \"unclear\"\n",
    "    ]\n",
    "    shows_uncertainty = any(p in answer_lower for p in uncertainty_phrases)\n",
    "    \n",
    "    return {\n",
    "        \"potential_hallucination_signals\": found_phrases,\n",
    "        \"shows_uncertainty\": shows_uncertainty,\n",
    "        \"answer_length\": len(answer),\n",
    "        \"context_length\": len(context_text)\n",
    "    }\n",
    "\n",
    "# Test on a RAG response\n",
    "result = rag.answer(\"What is the current inflation rate?\")\n",
    "\n",
    "print(\"Question: What is the current inflation rate?\")\n",
    "print(f\"\\nAnswer: {result['answer']}\")\n",
    "print(\"\\nFaithfulness analysis:\")\n",
    "analysis = check_faithfulness_simple(result['answer'], result['chunks'])\n",
    "for key, value in analysis.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7-18",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7.18 Caching and Performance\n",
    "\n",
    "Production RAG systems need performance optimization.\n",
    "\n",
    "### What to Cache\n",
    "\n",
    "| Component | Cache Strategy | Invalidation |\n",
    "|-----------|---------------|---------------|\n",
    "| **Document embeddings** | Precompute, persist | On document change |\n",
    "| **Query embeddings** | LRU cache | Time-based |\n",
    "| **Retrieval results** | Query hash → chunks | On index update |\n",
    "| **LLM responses** | Prompt hash → answer | Careful—may go stale |\n",
    "\n",
    "### Latency Breakdown\n",
    "\n",
    "Typical RAG latency:\n",
    "\n",
    "| Step | Typical Time |\n",
    "|------|-------------|\n",
    "| Query embedding | 50-100ms |\n",
    "| Vector search | 10-50ms |\n",
    "| LLM generation | 500-3000ms |\n",
    "| **Total** | 600-3000ms |\n",
    "\n",
    "LLM generation dominates. Cache carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caching-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "import hashlib\n",
    "\n",
    "class CachedRetriever:\n",
    "    \"\"\"Retriever with query caching.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_retriever: Retriever, cache_size: int = 100):\n",
    "        self.base = base_retriever\n",
    "        self.cache = {}\n",
    "        self.cache_size = cache_size\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "    \n",
    "    def _cache_key(self, query: str, k: int) -> str:\n",
    "        return hashlib.md5(f\"{query}:{k}\".encode()).hexdigest()\n",
    "    \n",
    "    def retrieve(self, query: str, k: int = 3) -> List[RetrievedChunk]:\n",
    "        key = self._cache_key(query, k)\n",
    "        \n",
    "        if key in self.cache:\n",
    "            self.hits += 1\n",
    "            return self.cache[key]\n",
    "        \n",
    "        self.misses += 1\n",
    "        result = self.base.retrieve(query, k)\n",
    "        \n",
    "        # Simple cache with size limit\n",
    "        if len(self.cache) >= self.cache_size:\n",
    "            # Remove oldest entry (simple strategy)\n",
    "            oldest_key = next(iter(self.cache))\n",
    "            del self.cache[oldest_key]\n",
    "        \n",
    "        self.cache[key] = result\n",
    "        return result\n",
    "    \n",
    "    def stats(self):\n",
    "        total = self.hits + self.misses\n",
    "        hit_rate = self.hits / total if total > 0 else 0\n",
    "        return {\"hits\": self.hits, \"misses\": self.misses, \"hit_rate\": hit_rate}\n",
    "\n",
    "# Demo caching\n",
    "cached_retriever = CachedRetriever(retriever)\n",
    "\n",
    "# First query - cache miss\n",
    "cached_retriever.retrieve(\"interest rates\", k=3)\n",
    "print(f\"After first query: {cached_retriever.stats()}\")\n",
    "\n",
    "# Same query - cache hit\n",
    "cached_retriever.retrieve(\"interest rates\", k=3)\n",
    "print(f\"After same query:  {cached_retriever.stats()}\")\n",
    "\n",
    "# Different query - cache miss\n",
    "cached_retriever.retrieve(\"mortgage rates\", k=3)\n",
    "print(f\"After new query:   {cached_retriever.stats()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7-19",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7.19 Observability and Audit Trails\n",
    "\n",
    "Production RAG requires comprehensive logging for:\n",
    "\n",
    "| Purpose | What to Log |\n",
    "|---------|-------------|\n",
    "| **Debugging** | Query, chunks, prompt, response |\n",
    "| **Quality monitoring** | Retrieval scores, response latency |\n",
    "| **Compliance** | User ID, timestamp, sources cited |\n",
    "| **Improvement** | Failed queries, low-confidence responses |\n",
    "\n",
    "### Audit Trail Structure\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"request_id\": \"uuid\",\n",
    "  \"timestamp\": \"ISO-8601\",\n",
    "  \"user_id\": \"user-123\",\n",
    "  \"query\": \"original question\",\n",
    "  \"retrieval\": {\n",
    "    \"chunk_ids\": [\"doc_001\", \"doc_002\"],\n",
    "    \"scores\": [0.85, 0.72],\n",
    "    \"latency_ms\": 45\n",
    "  },\n",
    "  \"generation\": {\n",
    "    \"model\": \"llama3.1:8b\",\n",
    "    \"prompt_tokens\": 450,\n",
    "    \"response_tokens\": 120,\n",
    "    \"latency_ms\": 1200\n",
    "  },\n",
    "  \"response\": \"final answer\",\n",
    "  \"refused\": false\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "audit-logging",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "class AuditableRAGPipeline:\n",
    "    \"\"\"RAG pipeline with audit logging.\"\"\"\n",
    "    \n",
    "    def __init__(self, retriever, prompt_builder, generator):\n",
    "        self.retriever = retriever\n",
    "        self.prompt_builder = prompt_builder\n",
    "        self.generator = generator\n",
    "        self.audit_log = []\n",
    "    \n",
    "    def answer(self, question: str, user_id: str = \"anonymous\", k: int = 3) -> dict:\n",
    "        request_id = str(uuid.uuid4())\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Retrieval\n",
    "        retrieval_start = time.time()\n",
    "        chunks = self.retriever.retrieve(question, k=k)\n",
    "        retrieval_ms = (time.time() - retrieval_start) * 1000\n",
    "        \n",
    "        # Prompt building\n",
    "        prompt = self.prompt_builder.build(chunks, question)\n",
    "        \n",
    "        # Generation\n",
    "        generation_start = time.time()\n",
    "        answer = self.generator.generate(prompt)\n",
    "        generation_ms = (time.time() - generation_start) * 1000\n",
    "        \n",
    "        total_ms = (time.time() - start_time) * 1000\n",
    "        \n",
    "        # Build audit record\n",
    "        audit_record = {\n",
    "            \"request_id\": request_id,\n",
    "            \"timestamp\": datetime.utcnow().isoformat(),\n",
    "            \"user_id\": user_id,\n",
    "            \"query\": question,\n",
    "            \"retrieval\": {\n",
    "                \"chunk_sources\": [c.source for c in chunks],\n",
    "                \"scores\": [c.score for c in chunks],\n",
    "                \"latency_ms\": round(retrieval_ms, 1)\n",
    "            },\n",
    "            \"generation\": {\n",
    "                \"latency_ms\": round(generation_ms, 1)\n",
    "            },\n",
    "            \"total_latency_ms\": round(total_ms, 1),\n",
    "            \"response_length\": len(answer)\n",
    "        }\n",
    "        \n",
    "        self.audit_log.append(audit_record)\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"request_id\": request_id,\n",
    "            \"chunks\": chunks\n",
    "        }\n",
    "    \n",
    "    def get_audit_log(self):\n",
    "        return self.audit_log\n",
    "\n",
    "# Create auditable pipeline\n",
    "auditable_rag = AuditableRAGPipeline(retriever, prompt_builder, generator)\n",
    "\n",
    "# Make a query\n",
    "result = auditable_rag.answer(\n",
    "    \"What impact did rate hikes have on mortgages?\",\n",
    "    user_id=\"user-42\"\n",
    ")\n",
    "\n",
    "print(\"Answer:\", result[\"answer\"][:100], \"...\")\n",
    "print(f\"\\nRequest ID: {result['request_id']}\")\n",
    "print(\"\\nAudit Record:\")\n",
    "print(json.dumps(auditable_rag.audit_log[-1], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7-20",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7.20 RAG as a Platform Capability\n",
    "\n",
    "In enterprise settings, RAG becomes a **platform**—not a one-off feature.\n",
    "\n",
    "### Platform Characteristics\n",
    "\n",
    "| Aspect | Implementation |\n",
    "|--------|---------------|\n",
    "| **Multi-tenant** | Different knowledge bases per team/product |\n",
    "| **Swappable components** | Change LLM without rebuilding |\n",
    "| **Configurable guardrails** | Different thresholds per use case |\n",
    "| **Centralized logging** | Unified audit across all RAG apps |\n",
    "\n",
    "### Evolution Path\n",
    "\n",
    "```\n",
    "Prototype RAG          Production RAG         Platform RAG\n",
    "     │                      │                      │\n",
    "     │ Single corpus        │ Multiple corpora     │ Self-service corpora\n",
    "     │ One model            │ Model selection      │ Model marketplace\n",
    "     │ No guardrails        │ Fixed guardrails     │ Configurable policies\n",
    "     │ No logging           │ Basic logging        │ Full observability\n",
    "     ▼                      ▼                      ▼\n",
    "```\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "> **RAG at scale is about governance, not just generation.**\n",
    ">\n",
    "> Who can access which knowledge? What gets logged? How do we audit?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Module Summary\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "| Concept | Remember |\n",
    "|---------|----------|\n",
    "| **Why RAG** | LLMs have no runtime access to your data |\n",
    "| **RAG Architecture** | Component-based: Retriever → Prompt → Generator → Validator |\n",
    "| **Near-misses** | Most dangerous failure: semantically similar but factually different |\n",
    "| **Guardrails** | Score thresholds and refusal are features, not failures |\n",
    "| **Evaluation** | Measure retrieval quality and generation faithfulness separately |\n",
    "| **Production** | Logging, caching, audit trails are mandatory |\n",
    "\n",
    "## The RAG Mental Model\n",
    "\n",
    "> **RAG is how we turn LLMs from storytellers into assistants grounded in evidence.**\n",
    ">\n",
    "> It is an architectural discipline, not a prompt trick.\n",
    "\n",
    "## What's Next\n",
    "\n",
    "You now have all the components to build production AI systems:\n",
    "\n",
    "- Module 5: Embeddings and retrieval\n",
    "- Module 6: LLM API engineering\n",
    "- Module 7: RAG pipelines\n",
    "\n",
    "The assessment will test your ability to combine these into a working system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Practice Exercises\n",
    "\n",
    "## Exercise 1: Adjust Retrieval Parameters\n",
    "Modify the retriever to use k=5 instead of k=3. How does this affect answer quality?\n",
    "\n",
    "## Exercise 2: Custom Guardrails\n",
    "Create a guardrail that refuses to answer if retrieved chunks come from more than 2 different sources (potential conflicting evidence).\n",
    "\n",
    "## Exercise 3: Evaluate Your RAG\n",
    "Write 5 test questions and manually evaluate:\n",
    "1. Retrieval precision (are the right chunks retrieved?)\n",
    "2. Generation faithfulness (does the answer use only the context?)\n",
    "\n",
    "## Exercise 4: Add a New Document\n",
    "Add a new document to the knowledge base about cryptocurrency regulation. Test that queries about crypto now return relevant results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d92889da",
   "metadata": {},
   "source": [
    "# Module 4 Assessment â€” TEMPLATE WITH HIDDEN TESTS (Instructor/Grading)\n",
    "\n",
    "This template grades the student notebook deterministically (no LLMs).\n",
    "\n",
    "**Structure:**\n",
    "- 3 Written Tasks (55 points): Keyword groups + minimum length\n",
    "- 3 Coding Tasks (45 points): Unit tests with test cases\n",
    "\n",
    "Feedback written into `assessment_result.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e7143f",
   "metadata": {},
   "outputs": [],
   "source": [
    "__assessment_scores = {}\n",
    "__assessment_feedback = {}\n",
    "\n",
    "def record_score(task, points, max_points, feedback):\n",
    "    __assessment_scores[task] = (points, max_points)\n",
    "    __assessment_feedback[task] = feedback\n",
    "\n",
    "def validate_answer(\n",
    "    answer,\n",
    "    required_groups=None,\n",
    "    forbidden_strings=None,\n",
    "    forbidden_characters=None,\n",
    "    min_length=0,\n",
    "    max_length=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Validate an answer using string-level rules.\n",
    "    Returns (passed: bool, reasons: list[str])\n",
    "    \"\"\"\n",
    "    reasons = []\n",
    "    text = answer.strip()\n",
    "    t_lower = text.lower()\n",
    "\n",
    "    # Length checks\n",
    "    if len(text) < min_length:\n",
    "        reasons.append(f\"Too short (min {min_length} chars, got {len(text)})\")\n",
    "\n",
    "    if max_length is not None and len(text) > max_length:\n",
    "        reasons.append(f\"Too long (max {max_length} chars)\")\n",
    "\n",
    "    # Required keyword groups (AND logic - must have at least one from each group)\n",
    "    if required_groups:\n",
    "        for group in required_groups:\n",
    "            if not any(kw in t_lower for kw in group):\n",
    "                reasons.append(f\"Missing concept from: {group[:2]}...\")\n",
    "\n",
    "    # Forbidden substrings (AI detection)\n",
    "    if forbidden_strings:\n",
    "        matched = [s for s in forbidden_strings if s in t_lower]\n",
    "        if len(matched) >= 2:\n",
    "            reasons.append(f\"Appears AI-generated. Detected: {matched[:3]}\")\n",
    "\n",
    "    # Forbidden characters (markdown formatting from copy-paste)\n",
    "    if forbidden_characters:\n",
    "        found = [ch for ch in forbidden_characters if ch in text]\n",
    "        if found:\n",
    "            reasons.append(f\"Contains formatting characters (copy-paste?): {found}\")\n",
    "\n",
    "    passed = len(reasons) == 0\n",
    "    return passed, reasons\n",
    "\n",
    "# Common AI phrases that indicate copy-paste from ChatGPT/Claude\n",
    "AI_PHRASES = [\n",
    "    \"as an ai\",\n",
    "    \"as a large language model\",\n",
    "    \"i'm happy to help\",\n",
    "    \"i'd be happy to\",\n",
    "    \"let me explain\",\n",
    "    \"let me break this down\",\n",
    "    \"here's a comprehensive\",\n",
    "    \"it's important to note that\",\n",
    "    \"it's worth noting that\",\n",
    "    \"it is important to understand\",\n",
    "    \"in summary,\",\n",
    "    \"in conclusion,\",\n",
    "    \"to summarize,\",\n",
    "    \"first and foremost\",\n",
    "    \"delve into\",\n",
    "    \"crucial to understand\",\n",
    "    \"landscape of\",\n",
    "    \"realm of\",\n",
    "    \"paradigm\",\n",
    "    \"leverage the power\",\n",
    "    \"harness the capabilities\",\n",
    "    \"at its core,\",\n",
    "    \"fundamentally,\",\n",
    "    \"essentially,\",\n",
    "    \"in essence,\",\n",
    "    \"pivotal role\",\n",
    "    \"multifaceted\",\n",
    "    \"myriad of\",\n",
    "]\n",
    "\n",
    "# Markdown formatting characters that suggest copy-paste\n",
    "FORBIDDEN_CHARS = [\"##\", \"**\", \"```\", \"* \", \"- [ ]\", \"###\"]\n",
    "\n",
    "# Written task rules\n",
    "WRITTEN_RULES = {\n",
    "  \"Task 1\": {\n",
    "      \"var\": \"concept_mapping\",\n",
    "      \"min_len\": 350,\n",
    "      \"max_points\": 15,\n",
    "      \"groups\": [\n",
    "          [\"ai\", \"artificial intelligence\"],\n",
    "          [\"ml\", \"machine learning\"],\n",
    "          [\"dl\", \"deep learning\", \"neural\"],\n",
    "          [\"llm\", \"large language model\", \"generative\"]\n",
    "      ]\n",
    "  },\n",
    "  \"Task 3\": {\n",
    "      \"var\": \"learning_mechanics\",\n",
    "      \"min_len\": 450,\n",
    "      \"max_points\": 20,\n",
    "      \"groups\": [\n",
    "          [\"loss\", \"error\"],\n",
    "          [\"gradient\", \"descent\", \"slope\"],\n",
    "          [\"backprop\", \"back propagation\", \"propagate\", \"backward\"],\n",
    "          [\"learning rate\", \"step size\"],\n",
    "          [\"converge\", \"local\", \"minim\"]\n",
    "      ]\n",
    "  },\n",
    "  \"Task 6\": {\n",
    "      \"var\": \"llm_grounding_reflection\",\n",
    "      \"min_len\": 450,\n",
    "      \"max_points\": 20,\n",
    "      \"groups\": [\n",
    "          [\"hallucination\", \"fabricat\", \"made up\", \"confident\"],\n",
    "          [\"pattern\", \"next token\", \"probabilistic\", \"training data\"],\n",
    "          [\"grounding\", \"retrieval\", \"rag\", \"evidence\", \"context\"],\n",
    "          [\"enterprise\", \"audit\", \"compliance\", \"risk\", \"business\"]\n",
    "      ]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tests_header",
   "metadata": {},
   "source": [
    "---\n",
    "## Task Tests (1-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task1_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Concept Mapping (15 points) [Written]\n",
    "points = 0\n",
    "fb = []\n",
    "try:\n",
    "    r = WRITTEN_RULES[\"Task 1\"]\n",
    "    assert r[\"var\"] in globals(), f\"{r['var']} variable missing\"\n",
    "    text = globals()[r[\"var\"]]\n",
    "    \n",
    "    passed, reasons = validate_answer(\n",
    "        text,\n",
    "        required_groups=r[\"groups\"],\n",
    "        forbidden_strings=AI_PHRASES,\n",
    "        forbidden_characters=FORBIDDEN_CHARS,\n",
    "        min_length=r[\"min_len\"]\n",
    "    )\n",
    "    \n",
    "    if passed:\n",
    "        points = r[\"max_points\"]\n",
    "        fb.append(\"\\u2713 Passed\")\n",
    "    else:\n",
    "        for reason in reasons:\n",
    "            fb.append(f\"\\u2717 {reason}\")\n",
    "            \n",
    "except AssertionError as e:\n",
    "    fb.append(f\"\\u2717 {e}\")\n",
    "record_score(\"Task 1 - Concept Mapping\", points, WRITTEN_RULES[\"Task 1\"][\"max_points\"], fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task2_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Loss Calculation - calculate_mse (15 points) [Coding]\n",
    "points = 0\n",
    "fb = []\n",
    "max_points = 15\n",
    "\n",
    "try:\n",
    "    assert \"calculate_mse\" in globals(), \"calculate_mse function not defined\"\n",
    "    func = globals()[\"calculate_mse\"]\n",
    "    assert callable(func), \"calculate_mse is not a function\"\n",
    "    \n",
    "    # Test case 1: Basic MSE calculation\n",
    "    result = func([10, 20, 30], [12, 18, 33])\n",
    "    expected = (4 + 4 + 9) / 3  # 17/3 = 5.67\n",
    "    assert abs(result - expected) < 0.01, f\"Test 1 failed: expected ~{expected:.2f}, got {result}\"\n",
    "    fb.append(\"\\u2713 Test 1 passed (basic MSE)\")\n",
    "    points += 5\n",
    "    \n",
    "    # Test case 2: Perfect predictions (MSE = 0)\n",
    "    result = func([1, 2, 3], [1, 2, 3])\n",
    "    assert result == 0, f\"Test 2 failed: expected 0 for perfect predictions, got {result}\"\n",
    "    fb.append(\"\\u2713 Test 2 passed (zero loss)\")\n",
    "    points += 5\n",
    "    \n",
    "    # Test case 3: Single value\n",
    "    result = func([5], [8])\n",
    "    expected = 9  # (5-8)^2 = 9\n",
    "    assert result == expected, f\"Test 3 failed: expected {expected}, got {result}\"\n",
    "    fb.append(\"\\u2713 Test 3 passed (single value)\")\n",
    "    points += 5\n",
    "    \n",
    "except AssertionError as e:\n",
    "    fb.append(f\"\\u2717 {e}\")\n",
    "except Exception as e:\n",
    "    fb.append(f\"\\u2717 Runtime error: {e}\")\n",
    "\n",
    "record_score(\"Task 2 - MSE Calculation\", points, max_points, fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task3_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: How Learning Works (20 points) [Written]\n",
    "points = 0\n",
    "fb = []\n",
    "try:\n",
    "    r = WRITTEN_RULES[\"Task 3\"]\n",
    "    assert r[\"var\"] in globals(), f\"{r['var']} variable missing\"\n",
    "    text = globals()[r[\"var\"]]\n",
    "    \n",
    "    passed, reasons = validate_answer(\n",
    "        text,\n",
    "        required_groups=r[\"groups\"],\n",
    "        forbidden_strings=AI_PHRASES,\n",
    "        forbidden_characters=FORBIDDEN_CHARS,\n",
    "        min_length=r[\"min_len\"]\n",
    "    )\n",
    "    \n",
    "    if passed:\n",
    "        points = r[\"max_points\"]\n",
    "        fb.append(\"\\u2713 Passed\")\n",
    "    else:\n",
    "        for reason in reasons:\n",
    "            fb.append(f\"\\u2717 {reason}\")\n",
    "            \n",
    "except AssertionError as e:\n",
    "    fb.append(f\"\\u2717 {e}\")\n",
    "record_score(\"Task 3 - Learning Mechanics\", points, WRITTEN_RULES[\"Task 3\"][\"max_points\"], fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task4_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Gradient Descent Step (15 points) [Coding]\n",
    "points = 0\n",
    "fb = []\n",
    "max_points = 15\n",
    "\n",
    "try:\n",
    "    assert \"gradient_descent_step\" in globals(), \"gradient_descent_step function not defined\"\n",
    "    func = globals()[\"gradient_descent_step\"]\n",
    "    assert callable(func), \"gradient_descent_step is not a function\"\n",
    "    \n",
    "    # Test case 1: Perfect weight, should stay the same\n",
    "    result = func(2.0, [1, 2, 3], [2, 4, 6], 0.01)\n",
    "    assert abs(result - 2.0) < 0.001, f\"Test 1 failed: expected ~2.0 (no change for perfect weight), got {result}\"\n",
    "    fb.append(\"\\u2713 Test 1 passed (optimal weight unchanged)\")\n",
    "    points += 5\n",
    "    \n",
    "    # Test case 2: Weight too high, should decrease\n",
    "    result = func(3.0, [1, 2], [2, 4], 0.1)\n",
    "    expected = 2.5\n",
    "    assert abs(result - expected) < 0.001, f\"Test 2 failed: expected {expected}, got {result}\"\n",
    "    fb.append(\"\\u2713 Test 2 passed (weight decreases when too high)\")\n",
    "    points += 5\n",
    "    \n",
    "    # Test case 3: Weight too low, should increase\n",
    "    result = func(1.0, [1, 2], [2, 4], 0.1)\n",
    "    expected = 1.5\n",
    "    assert abs(result - expected) < 0.001, f\"Test 3 failed: expected {expected}, got {result}\"\n",
    "    fb.append(\"\\u2713 Test 3 passed (weight increases when too low)\")\n",
    "    points += 5\n",
    "    \n",
    "except AssertionError as e:\n",
    "    fb.append(f\"\\u2717 {e}\")\n",
    "except Exception as e:\n",
    "    fb.append(f\"\\u2717 Runtime error: {e}\")\n",
    "\n",
    "record_score(\"Task 4 - Gradient Descent\", points, max_points, fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task5_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5: Simple Neuron with ReLU (15 points) [Coding]\n",
    "points = 0\n",
    "fb = []\n",
    "max_points = 15\n",
    "\n",
    "try:\n",
    "    assert \"simple_neuron\" in globals(), \"simple_neuron function not defined\"\n",
    "    func = globals()[\"simple_neuron\"]\n",
    "    assert callable(func), \"simple_neuron is not a function\"\n",
    "    \n",
    "    # Test case 1: Positive output (ReLU passes through)\n",
    "    result = func([1, 2], [0.5, 0.5], 0.1)\n",
    "    expected = 1.6\n",
    "    assert abs(result - expected) < 0.001, f\"Test 1 failed: expected {expected}, got {result}\"\n",
    "    fb.append(\"\\u2713 Test 1 passed (positive output)\")\n",
    "    points += 5\n",
    "    \n",
    "    # Test case 2: Negative z, ReLU clips to 0\n",
    "    result = func([1, 2], [0.5, -0.5], 0.1)\n",
    "    expected = 0\n",
    "    assert result == expected, f\"Test 2 failed: expected {expected} (ReLU clips negative), got {result}\"\n",
    "    fb.append(\"\\u2713 Test 2 passed (ReLU clips negative)\")\n",
    "    points += 5\n",
    "    \n",
    "    # Test case 3: Multiple inputs\n",
    "    result = func([1, 2, 3], [0.1, 0.2, 0.3], -0.5)\n",
    "    expected = 0.9\n",
    "    assert abs(result - expected) < 0.001, f\"Test 3 failed: expected {expected}, got {result}\"\n",
    "    fb.append(\"\\u2713 Test 3 passed (multiple inputs)\")\n",
    "    points += 5\n",
    "    \n",
    "except AssertionError as e:\n",
    "    fb.append(f\"\\u2717 {e}\")\n",
    "except Exception as e:\n",
    "    fb.append(f\"\\u2717 Runtime error: {e}\")\n",
    "\n",
    "record_score(\"Task 5 - Simple Neuron\", points, max_points, fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task6_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6: LLM Behaviour + Grounding (20 points) [Written]\n",
    "points = 0\n",
    "fb = []\n",
    "try:\n",
    "    r = WRITTEN_RULES[\"Task 6\"]\n",
    "    assert r[\"var\"] in globals(), f\"{r['var']} variable missing\"\n",
    "    text = globals()[r[\"var\"]]\n",
    "    \n",
    "    passed, reasons = validate_answer(\n",
    "        text,\n",
    "        required_groups=r[\"groups\"],\n",
    "        forbidden_strings=AI_PHRASES,\n",
    "        forbidden_characters=FORBIDDEN_CHARS,\n",
    "        min_length=r[\"min_len\"]\n",
    "    )\n",
    "    \n",
    "    if passed:\n",
    "        points = r[\"max_points\"]\n",
    "        fb.append(\"\\u2713 Passed\")\n",
    "    else:\n",
    "        for reason in reasons:\n",
    "            fb.append(f\"\\u2717 {reason}\")\n",
    "            \n",
    "except AssertionError as e:\n",
    "    fb.append(f\"\\u2717 {e}\")\n",
    "record_score(\"Task 6 - LLM Grounding\", points, WRITTEN_RULES[\"Task 6\"][\"max_points\"], fb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "output_header",
   "metadata": {},
   "source": [
    "---\n",
    "## Generate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93764cb",
   "metadata": {},
   "outputs": [],
   "source": "import json, datetime, re\n\n# Sort scores by task number (extract number from \"Task N - ...\")\ndef task_sort_key(item):\n    match = re.search(r'Task (\\d+)', item[0])\n    return int(match.group(1)) if match else 99\n\nsorted_scores = dict(sorted(__assessment_scores.items(), key=task_sort_key))\nsorted_feedback = {k: __assessment_feedback[k] for k in sorted_scores.keys()}\n\n# Calculate totals\ntotal_points = sum(s[0] for s in sorted_scores.values())\nmax_possible = sum(s[1] for s in sorted_scores.values())\n\nresult = {\n  \"scores\": sorted_scores,\n  \"feedback\": sorted_feedback,\n  \"total\": f\"{total_points}/{max_possible}\",\n  \"percentage\": round(100 * total_points / max_possible, 1) if max_possible > 0 else 0,\n  \"timestamp\": datetime.datetime.now().isoformat()\n}\n\nwith open(\"assessment_result.json\", \"w\") as f:\n    json.dump(result, f, indent=2)\n\nprint(f\"\\n{'='*50}\")\nprint(f\"ASSESSMENT RESULTS: {total_points}/{max_possible} ({result['percentage']}%)\")\nprint(f\"{'='*50}\\n\")\n\nfor task, (pts, mx) in sorted_scores.items():\n    status = \"\\u2713\" if pts == mx else \"\\u2717\" if pts == 0 else \"~\"\n    print(f\"{status} {task}: {pts}/{mx}\")\n    for line in sorted_feedback[task]:\n        print(f\"    {line}\")\n\nresult"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
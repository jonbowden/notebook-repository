{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Module 5 Assessment — TEMPLATE WITH HIDDEN TESTS (Instructor/Grading)\n",
    "\n",
    "This template grades the student notebook deterministically (no LLMs).\n",
    "\n",
    "**Structure:**\n",
    "- 5 Coding Tasks (80 points): Unit tests with assertions\n",
    "- 1 Written Task (20 points): Keyword groups + minimum length\n",
    "\n",
    "Feedback written into `assessment_result.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": "# Setup - packages are pre-installed in grading environment\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\nimport faiss\n\n# Load the embedding model (cached after first download)\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\")\nprint(\"Setup complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corpus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus of documents (do not modify)\n",
    "corpus = [\n",
    "    \"Interest rates were increased by the central bank to control inflation.\",\n",
    "    \"The central bank raised borrowing costs to fight rising prices.\",\n",
    "    \"Quarterly earnings improved as net interest margin widened.\",\n",
    "    \"The Federal Reserve announced a 25 basis point rate hike.\",\n",
    "    \"Mortgage rates have reached their highest level in 20 years.\",\n",
    "    \"Football is a popular sport played across Europe.\",\n",
    "    \"The team won the championship after a dramatic penalty shootout.\",\n",
    "    \"Basketball players competed in the international tournament.\"\n",
    "]\n",
    "\n",
    "query = \"Why did the central bank increase rates?\"\n",
    "\n",
    "print(f\"Corpus loaded: {len(corpus)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scoring_setup",
   "metadata": {},
   "outputs": [],
   "source": "__assessment_scores = {}\n__assessment_feedback = {}\n\ndef record_score(task, points, max_points, feedback):\n    __assessment_scores[task] = (points, max_points)\n    __assessment_feedback[task] = feedback\n\ndef validate_answer(\n    answer,\n    required_groups=None,\n    forbidden_strings=None,\n    forbidden_characters=None,\n    min_length=0,\n    max_length=None,\n):\n    \"\"\"\n    Validate an answer using string-level rules.\n    Returns (passed: bool, reasons: list[str])\n    \"\"\"\n    reasons = []\n    text = answer.strip()\n    t_lower = text.lower()\n\n    # Length checks\n    if len(text) < min_length:\n        reasons.append(f\"Too short (min {min_length} chars, got {len(text)})\")\n\n    if max_length is not None and len(text) > max_length:\n        reasons.append(f\"Too long (max {max_length} chars)\")\n\n    # Required keyword groups (AND logic - must have at least one from each group)\n    if required_groups:\n        for group in required_groups:\n            if not any(kw in t_lower for kw in group):\n                reasons.append(f\"Missing concept from: {group[:2]}...\")\n\n    # Forbidden substrings (AI detection)\n    if forbidden_strings:\n        matched = [s for s in forbidden_strings if s in t_lower]\n        if len(matched) >= 2:\n            reasons.append(f\"Appears AI-generated. Detected: {matched[:3]}\")\n\n    # Forbidden characters (markdown formatting from copy-paste)\n    if forbidden_characters:\n        found = [ch for ch in forbidden_characters if ch in text]\n        if found:\n            reasons.append(f\"Contains formatting characters (copy-paste?): {found}\")\n    \n    # Check for suspiciously perfect structure (numbered lists with consistent formatting)\n    import re\n    numbered_pattern = re.findall(r'^\\d+\\.\\s', text, re.MULTILINE)\n    if len(numbered_pattern) >= 4:\n        reasons.append(\"Suspiciously structured (numbered list format typical of AI)\")\n    \n    # Check for excessive use of transitional phrases\n    transitions = [\"furthermore\", \"moreover\", \"additionally\", \"consequently\", \n                   \"therefore\", \"thus\", \"hence\", \"accordingly\", \"subsequently\"]\n    transition_count = sum(1 for t in transitions if t in t_lower)\n    if transition_count >= 3:\n        reasons.append(f\"Excessive formal transitions ({transition_count} found) - likely AI\")\n\n    passed = len(reasons) == 0\n    return passed, reasons\n\n# Common AI phrases that indicate copy-paste from ChatGPT/Claude\nAI_PHRASES = [\n    # Self-identification\n    \"as an ai\",\n    \"as a large language model\",\n    \"i'm happy to help\",\n    \"i'd be happy to\",\n    \"i cannot\",\n    \"i can't provide\",\n    \n    # Structural phrases\n    \"let me explain\",\n    \"let me break this down\",\n    \"let's dive into\",\n    \"let's explore\",\n    \"here's a comprehensive\",\n    \"here's an overview\",\n    \"here are the key\",\n    \n    # Emphasis phrases\n    \"it's important to note that\",\n    \"it's worth noting that\",\n    \"it is important to understand\",\n    \"it's crucial to\",\n    \"it's essential to\",\n    \"it bears mentioning\",\n    \n    # Summary phrases\n    \"in summary,\",\n    \"in conclusion,\",\n    \"to summarize,\",\n    \"to sum up,\",\n    \"overall,\",\n    \"in short,\",\n    \n    # Filler phrases\n    \"first and foremost\",\n    \"last but not least\",\n    \"needless to say\",\n    \"it goes without saying\",\n    \n    # Overused AI words\n    \"delve into\",\n    \"delve deeper\",\n    \"crucial to understand\",\n    \"landscape of\",\n    \"realm of\",\n    \"paradigm\",\n    \"leverage the power\",\n    \"harness the capabilities\",\n    \"utilize\",\n    \"facilitate\",\n    \"comprehensive understanding\",\n    \"nuanced\",\n    \"robust\",\n    \"seamless\",\n    \"streamline\",\n    \n    # Position phrases\n    \"at its core,\",\n    \"fundamentally,\",\n    \"essentially,\",\n    \"in essence,\",\n    \"inherently,\",\n    \"intrinsically,\",\n    \n    # Emphasis adjectives\n    \"pivotal role\",\n    \"multifaceted\",\n    \"myriad of\",\n    \"plethora of\",\n    \"wide array of\",\n    \"diverse range of\",\n    \n    # AI hedging\n    \"it's worth mentioning\",\n    \"one could argue\",\n    \"it can be said that\",\n    \"generally speaking\",\n    \n    # ChatGPT specific\n    \"i hope this helps\",\n    \"feel free to ask\",\n    \"happy to clarify\",\n    \"let me know if\",\n    \n    # Claude specific  \n    \"i appreciate\",\n    \"great question\",\n    \"that's a thoughtful\",\n]\n\n# Markdown formatting characters that suggest copy-paste\nFORBIDDEN_CHARS = [\"##\", \"**\", \"```\", \"* \", \"- [ ]\", \"###\", \">>>\", \"===\"]\n\n# Written task rules\nWRITTEN_RULES = {\n  \"Task 6\": {\n      \"var\": \"rag_explanation\",\n      \"min_len\": 400,\n      \"max_points\": 20,\n      \"groups\": [\n          [\"embed\", \"vector\", \"represent\"],\n          [\"similar\", \"close\", \"distance\", \"cosine\"],\n          [\"rag\", \"retrieval\", \"retrieve\", \"ground\"],\n          [\"hallucin\", \"fabricat\", \"made up\", \"invent\"]\n      ]\n  }\n}\n\nprint(\"Scoring infrastructure ready.\")"
  },
  {
   "cell_type": "markdown",
   "id": "tests_header",
   "metadata": {},
   "source": [
    "---\n",
    "## Task Tests (1-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task1_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Generate Embeddings (15 points) [Coding]\n",
    "points = 0\n",
    "fb = []\n",
    "max_points = 15\n",
    "\n",
    "try:\n",
    "    assert \"corpus_embeddings\" in globals(), \"corpus_embeddings variable not defined\"\n",
    "    embeddings = globals()[\"corpus_embeddings\"]\n",
    "    assert embeddings is not None, \"corpus_embeddings is None\"\n",
    "    \n",
    "    # Test 1: Check type is numpy array\n",
    "    assert hasattr(embeddings, 'shape'), \"corpus_embeddings should be a numpy array\"\n",
    "    fb.append(\"\\u2713 Test 1 passed (numpy array)\")\n",
    "    points += 5\n",
    "    \n",
    "    # Test 2: Check shape is (8, 384)\n",
    "    assert embeddings.shape == (8, 384), f\"Expected shape (8, 384), got {embeddings.shape}\"\n",
    "    fb.append(\"\\u2713 Test 2 passed (correct shape)\")\n",
    "    points += 5\n",
    "    \n",
    "    # Test 3: Check vectors are normalized (norm ≈ 1.0)\n",
    "    norms = np.linalg.norm(embeddings, axis=1)\n",
    "    assert np.allclose(norms, 1.0, atol=0.01), f\"Vectors not normalized. Norms: {norms[:3]}...\"\n",
    "    fb.append(\"\\u2713 Test 3 passed (normalized vectors)\")\n",
    "    points += 5\n",
    "    \n",
    "except AssertionError as e:\n",
    "    fb.append(f\"\\u2717 {e}\")\n",
    "except Exception as e:\n",
    "    fb.append(f\"\\u2717 Runtime error: {e}\")\n",
    "\n",
    "record_score(\"Task 1 - Generate Embeddings\", points, max_points, fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task2_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Calculate Similarity Scores (15 points) [Coding]\n",
    "points = 0\n",
    "fb = []\n",
    "max_points = 15\n",
    "\n",
    "try:\n",
    "    assert \"similarity_scores\" in globals(), \"similarity_scores variable not defined\"\n",
    "    scores = globals()[\"similarity_scores\"]\n",
    "    assert scores is not None, \"similarity_scores is None\"\n",
    "    \n",
    "    # Test 1: Check it's a numpy array\n",
    "    assert hasattr(scores, 'shape'), \"similarity_scores should be a numpy array\"\n",
    "    fb.append(\"\\u2713 Test 1 passed (numpy array)\")\n",
    "    points += 5\n",
    "    \n",
    "    # Test 2: Check shape is (8,) - 1D array of 8 scores\n",
    "    assert scores.shape == (8,) or scores.shape == (1, 8), f\"Expected shape (8,), got {scores.shape}\"\n",
    "    scores_flat = scores.flatten() if len(scores.shape) > 1 else scores\n",
    "    fb.append(\"\\u2713 Test 2 passed (correct shape)\")\n",
    "    points += 5\n",
    "    \n",
    "    # Test 3: Check values are in valid range [-1, 1] for cosine similarity\n",
    "    assert scores_flat.min() >= -1.01 and scores_flat.max() <= 1.01, f\"Scores out of range: [{scores_flat.min():.3f}, {scores_flat.max():.3f}]\"\n",
    "    fb.append(\"\\u2713 Test 3 passed (valid range)\")\n",
    "    points += 5\n",
    "    \n",
    "except AssertionError as e:\n",
    "    fb.append(f\"\\u2717 {e}\")\n",
    "except Exception as e:\n",
    "    fb.append(f\"\\u2717 Runtime error: {e}\")\n",
    "\n",
    "record_score(\"Task 2 - Similarity Scores\", points, max_points, fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task3_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Top-K Retrieval (15 points) [Coding]\n",
    "points = 0\n",
    "fb = []\n",
    "max_points = 15\n",
    "\n",
    "try:\n",
    "    assert \"top_3_indices\" in globals(), \"top_3_indices variable not defined\"\n",
    "    indices = globals()[\"top_3_indices\"]\n",
    "    assert indices is not None, \"top_3_indices is None\"\n",
    "    \n",
    "    # Convert to list/array for checking\n",
    "    indices_list = list(indices) if hasattr(indices, '__iter__') else [indices]\n",
    "    \n",
    "    # Test 1: Check length is 3\n",
    "    assert len(indices_list) == 3, f\"Expected 3 indices, got {len(indices_list)}\"\n",
    "    fb.append(\"\\u2713 Test 1 passed (3 indices)\")\n",
    "    points += 5\n",
    "    \n",
    "    # Test 2: Check indices are valid (0-7)\n",
    "    assert all(0 <= idx <= 7 for idx in indices_list), f\"Indices out of range: {indices_list}\"\n",
    "    fb.append(\"\\u2713 Test 2 passed (valid indices)\")\n",
    "    points += 5\n",
    "    \n",
    "    # Test 3: Check that finance-related docs are ranked higher than sports\n",
    "    # Docs 0-4 are finance, docs 5-7 are sports\n",
    "    # For query about central bank rates, finance docs should dominate top 3\n",
    "    finance_count = sum(1 for idx in indices_list if idx <= 4)\n",
    "    assert finance_count >= 2, f\"Expected mostly finance docs in top 3 for rate query, got {finance_count}\"\n",
    "    fb.append(\"\\u2713 Test 3 passed (relevant docs retrieved)\")\n",
    "    points += 5\n",
    "    \n",
    "except AssertionError as e:\n",
    "    fb.append(f\"\\u2717 {e}\")\n",
    "except Exception as e:\n",
    "    fb.append(f\"\\u2717 Runtime error: {e}\")\n",
    "\n",
    "record_score(\"Task 3 - Top-K Retrieval\", points, max_points, fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task4_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: FAISS Index and Search (20 points) [Coding]\n",
    "points = 0\n",
    "fb = []\n",
    "max_points = 20\n",
    "\n",
    "try:\n",
    "    # Test 1: Check faiss_index exists and is correct type\n",
    "    assert \"faiss_index\" in globals(), \"faiss_index variable not defined\"\n",
    "    idx = globals()[\"faiss_index\"]\n",
    "    assert idx is not None, \"faiss_index is None\"\n",
    "    assert hasattr(idx, 'ntotal'), \"faiss_index doesn't look like a FAISS index\"\n",
    "    assert idx.ntotal == 8, f\"Expected 8 vectors in index, got {idx.ntotal}\"\n",
    "    fb.append(\"\\u2713 Test 1 passed (FAISS index created with 8 vectors)\")\n",
    "    points += 5\n",
    "    \n",
    "    # Test 2: Check faiss_distances exists and has correct shape\n",
    "    assert \"faiss_distances\" in globals(), \"faiss_distances variable not defined\"\n",
    "    distances = globals()[\"faiss_distances\"]\n",
    "    assert distances is not None, \"faiss_distances is None\"\n",
    "    assert hasattr(distances, 'shape'), \"faiss_distances should be a numpy array\"\n",
    "    assert distances.shape == (1, 3), f\"Expected faiss_distances shape (1, 3), got {distances.shape}\"\n",
    "    fb.append(\"\\u2713 Test 2 passed (distances shape correct)\")\n",
    "    points += 5\n",
    "    \n",
    "    # Test 3: Check faiss_indices exists and has correct shape\n",
    "    assert \"faiss_indices\" in globals(), \"faiss_indices variable not defined\"\n",
    "    f_indices = globals()[\"faiss_indices\"]\n",
    "    assert f_indices is not None, \"faiss_indices is None\"\n",
    "    assert hasattr(f_indices, 'shape'), \"faiss_indices should be a numpy array\"\n",
    "    assert f_indices.shape == (1, 3), f\"Expected faiss_indices shape (1, 3), got {f_indices.shape}\"\n",
    "    fb.append(\"\\u2713 Test 3 passed (indices shape correct)\")\n",
    "    points += 5\n",
    "    \n",
    "    # Test 4: Check that FAISS returns relevant results (finance docs for rate query)\n",
    "    indices_list = list(f_indices[0])\n",
    "    finance_count = sum(1 for idx in indices_list if idx <= 4)\n",
    "    assert finance_count >= 2, f\"FAISS should return finance docs for rate query, got {indices_list}\"\n",
    "    fb.append(\"\\u2713 Test 4 passed (FAISS returns relevant results)\")\n",
    "    points += 5\n",
    "    \n",
    "except AssertionError as e:\n",
    "    fb.append(f\"\\u2717 {e}\")\n",
    "except Exception as e:\n",
    "    fb.append(f\"\\u2717 Runtime error: {e}\")\n",
    "\n",
    "record_score(\"Task 4 - FAISS Index\", points, max_points, fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task5_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5: Build RAG Prompt (15 points) [Coding]\n",
    "points = 0\n",
    "fb = []\n",
    "max_points = 15\n",
    "\n",
    "try:\n",
    "    assert \"rag_prompt\" in globals(), \"rag_prompt variable not defined\"\n",
    "    prompt = globals()[\"rag_prompt\"]\n",
    "    assert prompt is not None, \"rag_prompt is None\"\n",
    "    assert isinstance(prompt, str), f\"rag_prompt should be a string, got {type(prompt)}\"\n",
    "    \n",
    "    prompt_lower = prompt.lower()\n",
    "    \n",
    "    # Test 1: Check prompt contains the query\n",
    "    assert \"central bank\" in prompt_lower or \"increase rates\" in prompt_lower, \"Prompt should include the query\"\n",
    "    fb.append(\"\\u2713 Test 1 passed (contains query)\")\n",
    "    points += 5\n",
    "    \n",
    "    # Test 2: Check prompt has context instruction\n",
    "    has_context = any(word in prompt_lower for word in [\"context\", \"document\", \"based on\", \"following\"])\n",
    "    assert has_context, \"Prompt should reference context/documents\"\n",
    "    fb.append(\"\\u2713 Test 2 passed (has context instruction)\")\n",
    "    points += 5\n",
    "    \n",
    "    # Test 3: Check prompt includes actual corpus documents\n",
    "    has_docs = any(doc[:20].lower() in prompt_lower for doc in corpus[:5])\n",
    "    assert has_docs, \"Prompt should include retrieved documents from corpus\"\n",
    "    fb.append(\"\\u2713 Test 3 passed (includes retrieved documents)\")\n",
    "    points += 5\n",
    "    \n",
    "except AssertionError as e:\n",
    "    fb.append(f\"\\u2717 {e}\")\n",
    "except Exception as e:\n",
    "    fb.append(f\"\\u2717 Runtime error: {e}\")\n",
    "\n",
    "record_score(\"Task 5 - RAG Prompt\", points, max_points, fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task6_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6: RAG and Grounding Explanation (20 points) [Written]\n",
    "points = 0\n",
    "fb = []\n",
    "try:\n",
    "    r = WRITTEN_RULES[\"Task 6\"]\n",
    "    assert r[\"var\"] in globals(), f\"{r['var']} variable missing\"\n",
    "    text = globals()[r[\"var\"]]\n",
    "    \n",
    "    passed, reasons = validate_answer(\n",
    "        text,\n",
    "        required_groups=r[\"groups\"],\n",
    "        forbidden_strings=AI_PHRASES,\n",
    "        forbidden_characters=FORBIDDEN_CHARS,\n",
    "        min_length=r[\"min_len\"]\n",
    "    )\n",
    "    \n",
    "    if passed:\n",
    "        points = r[\"max_points\"]\n",
    "        fb.append(\"\\u2713 Passed\")\n",
    "    else:\n",
    "        for reason in reasons:\n",
    "            fb.append(f\"\\u2717 {reason}\")\n",
    "            \n",
    "except AssertionError as e:\n",
    "    fb.append(f\"\\u2717 {e}\")\n",
    "record_score(\"Task 6 - RAG Explanation\", points, WRITTEN_RULES[\"Task 6\"][\"max_points\"], fb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "output_header",
   "metadata": {},
   "source": [
    "---\n",
    "## Generate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, datetime, re\n",
    "\n",
    "# Sort scores by task number (extract number from \"Task N - ...\")\n",
    "def task_sort_key(item):\n",
    "    match = re.search(r'Task (\\d+)', item[0])\n",
    "    return int(match.group(1)) if match else 99\n",
    "\n",
    "sorted_scores = dict(sorted(__assessment_scores.items(), key=task_sort_key))\n",
    "sorted_feedback = {k: __assessment_feedback[k] for k in sorted_scores.keys()}\n",
    "\n",
    "# Calculate totals\n",
    "total_points = sum(s[0] for s in sorted_scores.values())\n",
    "max_possible = sum(s[1] for s in sorted_scores.values())\n",
    "\n",
    "result = {\n",
    "  \"scores\": sorted_scores,\n",
    "  \"feedback\": sorted_feedback,\n",
    "  \"total\": f\"{total_points}/{max_possible}\",\n",
    "  \"percentage\": round(100 * total_points / max_possible, 1) if max_possible > 0 else 0,\n",
    "  \"timestamp\": datetime.datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "with open(\"assessment_result.json\", \"w\") as f:\n",
    "    json.dump(result, f, indent=2)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"ASSESSMENT RESULTS: {total_points}/{max_possible} ({result['percentage']}%)\")\n",
    "print(f\"{'='*50}\\n\")\n",
    "\n",
    "for task, (pts, mx) in sorted_scores.items():\n",
    "    status = \"\\u2713\" if pts == mx else \"\\u2717\" if pts == 0 else \"~\"\n",
    "    print(f\"{status} {task}: {pts}/{mx}\")\n",
    "    for line in sorted_feedback[task]:\n",
    "        print(f\"    {line}\")\n",
    "\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
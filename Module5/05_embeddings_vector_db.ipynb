{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d32ceb18",
   "metadata": {},
   "source": "# Content\n\n**Module 5 — Embeddings & Vector Databases**\n\n**CodeVision Academy**\n\n## Overview\nIf Module 4 explains *how models learn*, Module 5 explains *how models remember*.\n\nThis module introduces **embeddings** and **vector databases**, the foundations of:\n- semantic search\n- retrieval systems\n- Retrieval-Augmented Generation (RAG)\n\nThe entire module runs in **Google Colab (CPU-only)** and requires no server access.\n\n---\n\n## One Big Idea to Remember\n\n> **Embeddings turn meaning into numbers, so computers can measure similarity.**\n\n---"
  },
  {
   "cell_type": "markdown",
   "id": "a75e8d33",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "By the end of this module, you will be able to:\n",
    "1. Explain what embeddings represent (and what they do not)\n",
    "2. Explain how neural networks produce embeddings\n",
    "3. Generate embeddings for text using a lightweight model\n",
    "4. Explain similarity geometrically\n",
    "5. Implement semantic search end-to-end\n",
    "6. Explain why vector databases exist\n",
    "7. Understand chunking, metadata, and retrieval quality\n",
    "8. Explain how embeddings + retrieval enable RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hf-token-setup",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Before You Start: Hugging Face Token Setup\n",
    "\n",
    "This notebook downloads models from **Hugging Face**. To avoid rate limits and warnings, you need your own free token.\n",
    "\n",
    "### Quick Setup (2-3 minutes)\n",
    "\n",
    "1. Go to [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
    "2. Click **New token** → Name it anything → Select **Read** access → **Create**\n",
    "3. **Copy the token** (you won't see it again)\n",
    "4. In Google Colab, click the **Key icon** in the left sidebar\n",
    "5. Add a secret named exactly `HF_TOKEN` with your token as the value\n",
    "6. Turn ON \"Notebook access\" → **Restart the runtime**\n",
    "\n",
    "**Important rules:**\n",
    "- Do NOT hard-code tokens in notebooks\n",
    "- Do NOT share your token\n",
    "- Do NOT upload tokens to GitHub\n",
    "\n",
    "Run the cell below to verify your setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hf-token-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify HF Token is set up correctly\n",
    "import os\n",
    "\n",
    "# Try to get token from Colab secrets first, then environment\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    hf_token = userdata.get('HF_TOKEN')\n",
    "    os.environ['HF_TOKEN'] = hf_token\n",
    "    print(\"HF_TOKEN loaded from Colab secrets\")\n",
    "except:\n",
    "    if 'HF_TOKEN' in os.environ:\n",
    "        print(\"HF_TOKEN found in environment\")\n",
    "    else:\n",
    "        print(\"WARNING: HF_TOKEN not found!\")\n",
    "        print(\"Please follow the setup instructions above.\")\n",
    "        print(\"You may see download warnings without it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f320cbdf",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install the required packages. This takes about 1-2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28356a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install sentence-transformers scikit-learn faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a943289",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Group 1 — What Are Embeddings?\n",
    "\n",
    "Before we write code, let's build intuition about what embeddings are and why they matter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2826a1c",
   "metadata": {},
   "source": [
    "## 5.1 What Is an Embedding?\n",
    "\n",
    "An **embedding** is a list of numbers (a vector) that represents the *meaning* of something.\n",
    "\n",
    "Think of it like coordinates on a map:\n",
    "- Paris and Lyon are closer together (both in France)\n",
    "- Paris and Tokyo are far apart (different continents)\n",
    "\n",
    "Embeddings work the same way for meaning:\n",
    "- \"dog\" and \"puppy\" → vectors close together\n",
    "- \"dog\" and \"economics\" → vectors far apart\n",
    "\n",
    "```\n",
    "Traditional approach:              Embedding approach:\n",
    "\n",
    "\"bank\" = \"bank\"                   \"bank\" = [0.12, -0.45, 0.78, ...]\n",
    "(just text, no meaning)            (captures context and meaning)\n",
    "```\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "| Without Embeddings | With Embeddings |\n",
    "|-------------------|------------------|\n",
    "| Search for exact words only | Search for similar *meaning* |\n",
    "| \"car\" won't find \"automobile\" | \"car\" finds \"automobile\", \"vehicle\" |\n",
    "| Keyword matching | Semantic understanding |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcc0246",
   "metadata": {},
   "source": [
    "## 5.2 What Embeddings Are NOT\n",
    "\n",
    "Embeddings are powerful, but they have important limitations:\n",
    "\n",
    "| Embeddings ARE | Embeddings are NOT |\n",
    "|----------------|--------------------|\n",
    "| Statistical patterns | Truth or facts |\n",
    "| Learned from training data | A knowledge database |\n",
    "| Good at similarity | Good at reasoning |\n",
    "| Context-dependent | Universal definitions |\n",
    "\n",
    "### Critical Insight for Enterprise\n",
    "\n",
    "Embeddings reflect the **biases in their training data**:\n",
    "- If training data associates \"nurse\" with \"female\", the embedding will too\n",
    "- Domain-specific language may not be well-represented\n",
    "- Recent events or proprietary terms won't be captured\n",
    "\n",
    "> **Key takeaway**: Embeddings are useful approximations, not ground truth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd75befa",
   "metadata": {},
   "source": [
    "## 5.3 How Neural Networks Create Embeddings\n",
    "\n",
    "Remember from Module 4: neural networks learn by adjusting weights to reduce error.\n",
    "\n",
    "Embedding models are trained on tasks like:\n",
    "- \"These two sentences mean the same thing\" (similarity)\n",
    "- \"This sentence follows that sentence\" (context)\n",
    "- \"This word fits in this blank\" (language modeling)\n",
    "\n",
    "Through training, the network learns to place similar meanings close together:\n",
    "\n",
    "```\n",
    "BEFORE TRAINING:                    AFTER TRAINING:\n",
    "(random positions)                  (meaningful positions)\n",
    "\n",
    "    dog •      • cat                    dog •  • puppy\n",
    "            • puppy                          • cat\n",
    "    car •                               \n",
    "        • economics                     car • • vehicle\n",
    "    • vehicle                           \n",
    "                                        economics •\n",
    "```\n",
    "\n",
    "The final layer of the network (before the output) contains the embedding—a compressed representation of meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412b9302",
   "metadata": {},
   "source": [
    "## 5.4 Dimensionality: Why Hundreds of Numbers?\n",
    "\n",
    "An embedding might have 384, 768, or even 1536 dimensions. Why so many?\n",
    "\n",
    "Each dimension captures a different *aspect* of meaning:\n",
    "\n",
    "| Dimension | Might capture... |\n",
    "|-----------|------------------|\n",
    "| #1 | Formality (casual ↔ formal) |\n",
    "| #2 | Sentiment (negative ↔ positive) |\n",
    "| #3 | Topic (finance ↔ sports) |\n",
    "| #47 | Tense (past ↔ future) |\n",
    "| ... | Hundreds more subtle features |\n",
    "\n",
    "**Note**: We don't actually know what each dimension means! The network learns these features automatically during training. This is called a **latent space**.\n",
    "\n",
    "### Practical Tradeoffs\n",
    "\n",
    "| Dimension Size | Pros | Cons |\n",
    "|---------------|------|------|\n",
    "| Small (384) | Fast, less memory | Less nuance |\n",
    "| Large (1536) | More detail | Slower, more storage |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf987d1",
   "metadata": {},
   "source": [
    "## 5.5 Enterprise Relevance\n",
    "\n",
    "Embeddings power many enterprise applications:\n",
    "\n",
    "| Use Case | How Embeddings Help |\n",
    "|----------|--------------------|\n",
    "| **Document Search** | Find relevant policies even with different wording |\n",
    "| **Customer Support** | Match queries to similar past tickets |\n",
    "| **Compliance** | Flag documents similar to known violations |\n",
    "| **Recommendation** | \"Customers who viewed X also viewed Y\" |\n",
    "| **Duplicate Detection** | Find near-duplicate records |\n",
    "| **RAG (Module 5 focus)** | Ground LLM answers in real documents |\n",
    "\n",
    "### Why This Matters for Banking\n",
    "\n",
    "In regulated industries, embeddings enable:\n",
    "- Searching regulations by *intent*, not just keywords\n",
    "- Detecting similar fraud patterns\n",
    "- Matching customer queries to approved responses\n",
    "- Auditable retrieval for compliance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c5055e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Group 2 — Generating Embeddings\n",
    "\n",
    "Now let's create embeddings with real code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33827e5",
   "metadata": {},
   "source": [
    "## 5.6 Choosing an Embedding Model\n",
    "\n",
    "There are many embedding models available. For this module, we use **all-MiniLM-L6-v2**:\n",
    "\n",
    "| Property | Value | Why it matters |\n",
    "|----------|-------|----------------|\n",
    "| Size | 80MB | Fits in Colab memory |\n",
    "| Dimensions | 384 | Good balance of quality/speed |\n",
    "| Speed | Fast | Works on CPU |\n",
    "| Quality | Good | Top performer for its size |\n",
    "\n",
    "### Other Popular Models\n",
    "\n",
    "| Model | Dimensions | Best for |\n",
    "|-------|------------|----------|\n",
    "| all-MiniLM-L6-v2 | 384 | General purpose, fast |\n",
    "| all-mpnet-base-v2 | 768 | Higher quality, slower |\n",
    "| OpenAI text-embedding-3-small | 1536 | API-based, high quality |\n",
    "| BGE, E5, GTE | Various | Multilingual, specialized |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9b22e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the embedding model\n",
    "# This downloads the model (~80MB) on first run\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "print(f\"Model loaded!\")\n",
    "print(f\"Embedding dimension: {model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54146e2",
   "metadata": {},
   "source": [
    "## 5.7 Building a Corpus\n",
    "\n",
    "A **corpus** is the collection of documents you want to search over.\n",
    "\n",
    "In a real system, this might be:\n",
    "- Company policies\n",
    "- Knowledge base articles\n",
    "- Customer support history\n",
    "- Regulatory documents\n",
    "\n",
    "For this demo, we'll use a small set of sentences about finance and sports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48139026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our corpus: documents we want to search\n",
    "corpus = [\n",
    "    \"Interest rates were increased by the central bank to control inflation.\",\n",
    "    \"The bank raised rates after inflation surprised to the upside.\",\n",
    "    \"Quarterly earnings improved as net interest margin widened.\",\n",
    "    \"The Federal Reserve announced a 25 basis point rate hike.\",\n",
    "    \"Mortgage rates have reached their highest level in 20 years.\",\n",
    "    \"Football is a popular sport in Europe.\",\n",
    "    \"A goal was scored in the final minute of the match.\",\n",
    "    \"The team won the championship after a penalty shootout.\",\n",
    "]\n",
    "\n",
    "print(f\"Corpus size: {len(corpus)} documents\")\n",
    "for i, doc in enumerate(corpus):\n",
    "    print(f\"  [{i}] {doc[:60]}...\" if len(doc) > 60 else f\"  [{i}] {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb052ed",
   "metadata": {},
   "source": [
    "## 5.8 Encoding the Corpus\n",
    "\n",
    "**Encoding** means converting text into embeddings (vectors).\n",
    "\n",
    "The `normalize_embeddings=True` option ensures all vectors have length 1, which:\n",
    "- Makes cosine similarity equal to dot product (faster!)\n",
    "- Ensures fair comparison regardless of text length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b55408f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert all documents to embeddings\n",
    "corpus_embeddings = model.encode(corpus, normalize_embeddings=True)\n",
    "\n",
    "print(f\"Corpus embeddings shape: {corpus_embeddings.shape}\")\n",
    "print(f\"  - {corpus_embeddings.shape[0]} documents\")\n",
    "print(f\"  - {corpus_embeddings.shape[1]} dimensions per embedding\")\n",
    "print()\n",
    "print(f\"First embedding (first 10 values): {corpus_embeddings[0][:10]}\")\n",
    "print(f\"Vector length (should be ~1.0): {np.linalg.norm(corpus_embeddings[0]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe7ef93",
   "metadata": {},
   "source": [
    "## 5.9 Encoding a Query\n",
    "\n",
    "When a user asks a question, we encode it the same way.\n",
    "\n",
    "The query embedding can then be compared to all corpus embeddings to find the most relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f522d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A user's question\n",
    "query = \"Why did the central bank raise interest rates?\"\n",
    "\n",
    "# Encode the query\n",
    "query_embedding = model.encode([query], normalize_embeddings=True)[0]\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "print(f\"Query embedding shape: {query_embedding.shape}\")\n",
    "print(f\"First 10 values: {query_embedding[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55579fd3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Group 3 — Similarity and Retrieval\n",
    "\n",
    "Now we have vectors for our corpus and query. How do we find the most similar documents?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52dc141",
   "metadata": {},
   "source": [
    "## 5.10 Measuring Similarity\n",
    "\n",
    "**Cosine similarity** measures the angle between two vectors:\n",
    "- **1.0** = identical direction (same meaning)\n",
    "- **0.0** = perpendicular (unrelated)\n",
    "- **-1.0** = opposite direction (opposite meaning)\n",
    "\n",
    "```\n",
    "           Similar (cos ≈ 0.9)\n",
    "              ↗\n",
    "            ↗\n",
    "Query →  ↗\n",
    "            ↘\n",
    "              ↘\n",
    "           Unrelated (cos ≈ 0.1)\n",
    "```\n",
    "\n",
    "### Why Cosine Similarity?\n",
    "\n",
    "| Metric | Pros | Cons |\n",
    "|--------|------|------|\n",
    "| Cosine similarity | Scale-independent, standard for text | Ignores magnitude |\n",
    "| Euclidean distance | Intuitive | Affected by vector length |\n",
    "| Dot product | Fast (with normalized vectors) | Requires normalization |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e28ee9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Calculate similarity between query and ALL corpus documents\n",
    "similarities = cosine_similarity([query_embedding], corpus_embeddings)[0]\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "print(\"\\nSimilarity scores:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for i, (doc, score) in enumerate(zip(corpus, similarities)):\n",
    "    # Visual indicator of relevance\n",
    "    bar = \"*\" * int(score * 20)\n",
    "    print(f\"[{score:.3f}] {bar:20s} {doc[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similarity-interpretation",
   "metadata": {},
   "source": [
    "### Interpreting the Results\n",
    "\n",
    "Notice how:\n",
    "- Finance-related sentences score high (0.5-0.8)\n",
    "- Sports sentences score low (0.1-0.2)\n",
    "- The model understands \"central bank\" and \"interest rates\" are related to \"Federal Reserve\" and \"rate hike\"\n",
    "\n",
    "This is **semantic search** — finding meaning, not just matching keywords!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5133e39",
   "metadata": {},
   "source": [
    "## 5.11 Ranking and Top-K Retrieval\n",
    "\n",
    "In practice, we don't return all documents. We return the **top K** most relevant.\n",
    "\n",
    "This is the core of semantic search:\n",
    "1. Encode the query\n",
    "2. Calculate similarity to all documents\n",
    "3. Sort by similarity\n",
    "4. Return top K results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286681e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, corpus, corpus_embeddings, model, k=3):\n",
    "    \"\"\"Perform semantic search and return top-k results.\"\"\"\n",
    "    # Encode query\n",
    "    query_emb = model.encode([query], normalize_embeddings=True)[0]\n",
    "    \n",
    "    # Calculate similarities\n",
    "    scores = cosine_similarity([query_emb], corpus_embeddings)[0]\n",
    "    \n",
    "    # Get top-k indices (highest scores first)\n",
    "    top_indices = np.argsort(scores)[::-1][:k]\n",
    "    \n",
    "    # Return results\n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append({\n",
    "            'rank': len(results) + 1,\n",
    "            'score': scores[idx],\n",
    "            'document': corpus[idx]\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Test it!\n",
    "query = \"Why did the central bank raise interest rates?\"\n",
    "results = semantic_search(query, corpus, corpus_embeddings, model, k=3)\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"Top 3 results:\")\n",
    "print(\"=\" * 70)\n",
    "for r in results:\n",
    "    print(f\"#{r['rank']} (score: {r['score']:.3f})\")\n",
    "    print(f\"   {r['document']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "try-different-queries",
   "metadata": {},
   "source": [
    "### Try Different Queries\n",
    "\n",
    "Experiment with the search to see how it handles different types of queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-queries",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try these different queries and see what comes back\n",
    "test_queries = [\n",
    "    \"What happened to mortgage costs?\",\n",
    "    \"Tell me about the football game\",\n",
    "    \"monetary policy decisions\",  # Different words, same concept!\n",
    "    \"Who won the sports competition?\"\n",
    "]\n",
    "\n",
    "for q in test_queries:\n",
    "    results = semantic_search(q, corpus, corpus_embeddings, model, k=2)\n",
    "    print(f\"Query: '{q}'\")\n",
    "    for r in results:\n",
    "        print(f\"  [{r['score']:.3f}] {r['document'][:50]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38b75e9",
   "metadata": {},
   "source": [
    "## 5.12 Failure Modes: When Embeddings Go Wrong\n",
    "\n",
    "Embeddings are powerful but not perfect. Understanding their limitations is crucial for enterprise use.\n",
    "\n",
    "### Common Failure Modes\n",
    "\n",
    "| Failure Mode | Example | Mitigation |\n",
    "|--------------|---------|------------|\n",
    "| **Domain mismatch** | General model doesn't understand legal jargon | Fine-tune on domain data |\n",
    "| **Ambiguity** | \"bank\" (financial vs river) | Add context, use metadata |\n",
    "| **Negation** | \"not interested in rates\" matches rate documents | Use reranking or hybrid search |\n",
    "| **Length mismatch** | Short query vs long document | Chunk documents appropriately |\n",
    "| **Recency** | Model doesn't know recent terms | Update model or use hybrid search |\n",
    "\n",
    "### Important Enterprise Considerations\n",
    "\n",
    "1. **Similarity ≠ Correctness**: A document can be similar but contain wrong information\n",
    "2. **No reasoning**: Embeddings don't understand logic or causation\n",
    "3. **Threshold sensitivity**: Choosing the right similarity cutoff is tricky\n",
    "4. **Adversarial inputs**: Carefully crafted queries can retrieve inappropriate content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "failure-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration: Negation doesn't work well\n",
    "print(\"Failure mode: Negation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "q1 = \"interest rate increases\"\n",
    "q2 = \"NOT about interest rates\"  # Should match different docs, but won't!\n",
    "\n",
    "for q in [q1, q2]:\n",
    "    results = semantic_search(q, corpus, corpus_embeddings, model, k=2)\n",
    "    print(f\"\\nQuery: '{q}'\")\n",
    "    for r in results:\n",
    "        print(f\"  [{r['score']:.3f}] {r['document'][:45]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Notice: Both queries return similar results!\")\n",
    "print(\"The model focuses on 'interest rates', ignoring 'NOT'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d032a0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Group 4 — Vector Databases and Scaling\n",
    "\n",
    "What happens when you have millions of documents?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11731f9f",
   "metadata": {},
   "source": [
    "## 5.13 Why Vector Databases Exist\n",
    "\n",
    "Our simple approach (compare query to ALL documents) doesn't scale:\n",
    "\n",
    "| Corpus Size | Comparisons per Query | Time (estimate) |\n",
    "|-------------|----------------------|------------------|\n",
    "| 1,000 | 1,000 | 1ms |\n",
    "| 1,000,000 | 1,000,000 | 1 second |\n",
    "| 1,000,000,000 | 1,000,000,000 | 17 minutes |\n",
    "\n",
    "**Vector databases** solve this with **approximate nearest neighbor (ANN)** algorithms:\n",
    "- Trade perfect accuracy for massive speed gains\n",
    "- Find 95% of correct results in 1% of the time\n",
    "\n",
    "### Popular Vector Databases\n",
    "\n",
    "| Database | Type | Best For |\n",
    "|----------|------|----------|\n",
    "| **FAISS** | Library | Local/embedded use |\n",
    "| **Pinecone** | Managed service | Production, serverless |\n",
    "| **Weaviate** | Open source | Self-hosted, GraphQL |\n",
    "| **ChromaDB** | Lightweight | Prototyping, local dev |\n",
    "| **pgvector** | PostgreSQL extension | Existing Postgres users |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d23a766",
   "metadata": {},
   "source": [
    "## 5.14 FAISS: Fast Similarity Search\n",
    "\n",
    "**FAISS** (Facebook AI Similarity Search) is a library for efficient similarity search.\n",
    "\n",
    "We'll use `IndexFlatIP` (Inner Product, exact search) for this demo. Production systems use approximate indexes like `IndexIVFFlat` or `IndexHNSW`.\n",
    "\n",
    "```\n",
    "Without Index:              With FAISS Index:\n",
    "\n",
    "Query → Compare ALL         Query → Check ~100 candidates\n",
    "        1,000,000 docs              (same quality!)\n",
    "        \n",
    "Slow, O(n)                  Fast, O(log n)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a99f826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "# Get embedding dimension\n",
    "d = corpus_embeddings.shape[1]  # 384 for our model\n",
    "\n",
    "# Create a FAISS index (Inner Product for normalized vectors = cosine similarity)\n",
    "index = faiss.IndexFlatIP(d)\n",
    "\n",
    "# Add our corpus embeddings to the index\n",
    "index.add(corpus_embeddings.astype('float32'))\n",
    "\n",
    "print(f\"FAISS index created!\")\n",
    "print(f\"  - Dimension: {d}\")\n",
    "print(f\"  - Vectors indexed: {index.ntotal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faiss-search",
   "metadata": {},
   "source": [
    "## 5.15 Searching with FAISS\n",
    "\n",
    "Now we can search using the index. The `search()` method returns:\n",
    "- **D**: Distances (similarities) to the top K matches\n",
    "- **I**: Indices of the top K matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83e78f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search with FAISS\n",
    "query = \"What is the Federal Reserve doing about inflation?\"\n",
    "query_emb = model.encode([query], normalize_embeddings=True).astype('float32')\n",
    "\n",
    "k = 3  # Return top 3 results\n",
    "D, I = index.search(query_emb, k)\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"FAISS Results:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for rank, (score, idx) in enumerate(zip(D[0], I[0]), 1):\n",
    "    print(f\"#{rank} (score: {score:.3f})\")\n",
    "    print(f\"   {corpus[idx]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49f76d7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Group 5 — From Retrieval to RAG\n",
    "\n",
    "Now we connect everything back to LLMs and enterprise applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rag-intro",
   "metadata": {},
   "source": [
    "## 5.16 What is RAG?\n",
    "\n",
    "**RAG (Retrieval-Augmented Generation)** combines:\n",
    "1. **Retrieval**: Find relevant documents using embeddings\n",
    "2. **Augmentation**: Add those documents to the LLM prompt\n",
    "3. **Generation**: LLM generates an answer using the context\n",
    "\n",
    "```\n",
    "WITHOUT RAG:\n",
    "User Question → LLM → Answer (might hallucinate)\n",
    "\n",
    "WITH RAG:\n",
    "User Question → Embedding → Vector Search → Retrieved Docs\n",
    "                                                  ↓\n",
    "                           LLM ← [Question + Docs] → Grounded Answer\n",
    "```\n",
    "\n",
    "### Why RAG Matters\n",
    "\n",
    "| Problem | How RAG Helps |\n",
    "|---------|---------------|\n",
    "| Hallucinations | LLM can only use provided facts |\n",
    "| Outdated knowledge | Retrieve from current documents |\n",
    "| Proprietary data | Search your own knowledge base |\n",
    "| Auditability | You can show *which* documents were used |\n",
    "| Cost | Retrieval is cheaper than fine-tuning |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rag-demo",
   "metadata": {},
   "source": [
    "## 5.17 Building a Simple RAG Pipeline\n",
    "\n",
    "Let's build a simple RAG system. We'll simulate the LLM part, but the retrieval is real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rag-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_retrieve(question, index, corpus, model, k=3):\n",
    "    \"\"\"Retrieve relevant documents for RAG.\"\"\"\n",
    "    # Encode the question\n",
    "    q_emb = model.encode([question], normalize_embeddings=True).astype('float32')\n",
    "    \n",
    "    # Search the index\n",
    "    D, I = index.search(q_emb, k)\n",
    "    \n",
    "    # Collect retrieved documents\n",
    "    retrieved = []\n",
    "    for score, idx in zip(D[0], I[0]):\n",
    "        retrieved.append({\n",
    "            'document': corpus[idx],\n",
    "            'score': float(score)\n",
    "        })\n",
    "    \n",
    "    return retrieved\n",
    "\n",
    "def build_rag_prompt(question, retrieved_docs):\n",
    "    \"\"\"Build a prompt for the LLM with retrieved context.\"\"\"\n",
    "    context = \"\\n\".join([f\"- {doc['document']}\" for doc in retrieved_docs])\n",
    "    \n",
    "    prompt = f\"\"\"Answer the question based ONLY on the following context.\n",
    "If the context doesn't contain the answer, say \"I don't have enough information.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    return prompt\n",
    "\n",
    "# Demo the RAG pipeline\n",
    "question = \"What actions has the central bank taken regarding interest rates?\"\n",
    "\n",
    "# Step 1: Retrieve\n",
    "retrieved = rag_retrieve(question, index, corpus, model, k=3)\n",
    "\n",
    "print(\"STEP 1: RETRIEVAL\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(\"Retrieved documents:\")\n",
    "for i, doc in enumerate(retrieved, 1):\n",
    "    print(f\"  {i}. [{doc['score']:.3f}] {doc['document']}\")\n",
    "\n",
    "# Step 2: Build prompt\n",
    "prompt = build_rag_prompt(question, retrieved)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 2: RAG PROMPT (would be sent to LLM)\")\n",
    "print(\"=\" * 60)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dhyi0t8kc5",
   "source": "## 5.18 Completing the RAG Pipeline with an LLM\n\nNow let's send the prompt to an actual LLM to generate a grounded answer.\n\n### LLM Gateway Configuration\n\nWe use the same LLM gateway from Module 3. You have two options:\n\n| Option | Model | Why |\n|--------|-------|-----|\n| **A: Local Ollama** | `phi3:mini` | Lightweight (2.2GB), runs on most laptops |\n| **B: JBChat Server** | `llama3.1:8b` | Higher quality answers, see production-grade RAG |\n\n---\n\n### Option A: Local Ollama\n\n**If running Jupyter locally:** Use `http://localhost:11434` directly.\n\n**If running in Google Colab:** You must expose Ollama via a tunnel (Colab cannot reach your localhost).\n\n**Pinggy Setup (required for Colab):**\n1. Open a terminal on your local machine\n2. Make sure Ollama is running: `ollama serve`\n3. Start the tunnel:\n   ```bash\n   ssh -p 443 -R0:localhost:11434 a.pinggy.io\n   ```\n4. Copy the HTTPS URL it gives you (e.g., `https://xyz-abc.a.pinggy.io`)\n5. Use that URL in the config below\n\n---\n\n### Option B: Server Gateway (JBChat)\n\nIf you cannot run Ollama locally, use the course server:\n- URL: `https://jbchat.jonbowden.com.ngrok.app`\n- Requires API key from instructor\n- Model: `llama3.1:8b` (better quality)\n\nConfigure your choice in the cell below:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "cuz8anqpxf",
   "source": "# ===== LLM GATEWAY CONFIGURATION =====\n# Same setup as Module 3\n\n# ------ OPTION A: Local Ollama ------\n# If running Jupyter LOCALLY, use localhost:\nLLM_BASE_URL = \"http://localhost:11434\"\n\n# If running in COLAB, use your Pinggy tunnel URL instead:\n# LLM_BASE_URL = \"https://your-pinggy-url.a.pinggy.io\"\n\nLLM_API_KEY = None  # No API key → uses Ollama /api/chat endpoint\nDEFAULT_MODEL = \"phi3:mini\"  # Lightweight, runs on most laptops\n\n# ------ OPTION B: Server Gateway (JBChat) ------\n# Uncomment these 3 lines to use the course server:\n# LLM_BASE_URL = \"https://jbchat.jonbowden.com.ngrok.app\"\n# LLM_API_KEY = \"<provided-by-instructor>\"\n# DEFAULT_MODEL = \"llama3.1:8b\"  # Higher quality model on server\n\nimport requests\n\ndef call_llm(\n    prompt: str,\n    model: str = None,\n    temperature: float = 0.0,\n    max_tokens: int = 256,\n    base_url: str = None,\n    api_key: str = None,\n    timeout: tuple = (10, 120)\n) -> str:\n    \"\"\"\n    Canonical LLM call - same as Module 3.\n    Auto-detects endpoint mode:\n      - If API key is set → JBChat gateway (/chat/direct)\n      - If no API key → Direct Ollama (/api/chat)\n    \"\"\"\n    # Use defaults if not specified\n    if base_url is None:\n        base_url = LLM_BASE_URL\n    if model is None:\n        model = DEFAULT_MODEL\n    if api_key is None:\n        api_key = LLM_API_KEY if (LLM_API_KEY and LLM_API_KEY != \"<provided-by-instructor>\") else None\n\n    use_jbchat = api_key is not None\n\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"ngrok-skip-browser-warning\": \"true\",\n        \"Bypass-Tunnel-Reminder\": \"true\",\n    }\n    \n    if api_key:\n        headers[\"X-API-Key\"] = api_key\n\n    if use_jbchat:\n        endpoint = f\"{base_url.rstrip('/')}/chat/direct\"\n        payload = {\n            \"model\": model,\n            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n            \"temperature\": temperature,\n            \"max_tokens\": max_tokens,\n            \"stream\": False\n        }\n    else:\n        endpoint = f\"{base_url.rstrip('/')}/api/chat\"\n        payload = {\n            \"model\": model,\n            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n            \"options\": {\"temperature\": temperature},\n            \"stream\": False\n        }\n\n    resp = requests.post(endpoint, headers=headers, json=payload, timeout=timeout)\n    resp.raise_for_status()\n    data = resp.json()\n    return data[\"message\"][\"content\"]\n\n# Smoke test\ntry:\n    mode = \"JBChat\" if (LLM_API_KEY and LLM_API_KEY != \"<provided-by-instructor>\") else \"Ollama\"\n    print(f\"Mode: {mode} | Model: {DEFAULT_MODEL} | URL: {LLM_BASE_URL}\")\n    out = call_llm(\"Say 'LLM connected' in exactly 3 words.\", temperature=0.0)\n    print(f\"Response: {out[:100]}\")\nexcept Exception as e:\n    print(f\"Connection error: {e}\")\n    print(\"\\nIf using Colab, make sure you've set up Pinggy tunnel (see instructions above)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4ikqwfqv208",
   "source": "### Step 3: Send to LLM and Get a Grounded Answer\n\nNow we complete the RAG pipeline by sending the prompt to OpenAI's GPT model:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "uwahfxrpqi",
   "source": "# Complete RAG pipeline with LLM generation\ndef rag_query(question, index, corpus, model, k=3):\n    \"\"\"Complete RAG pipeline: retrieve + generate.\"\"\"\n\n    # Step 1: Retrieve relevant documents\n    retrieved = rag_retrieve(question, index, corpus, model, k)\n\n    # Step 2: Build the prompt\n    prompt = build_rag_prompt(question, retrieved)\n\n    # Step 3: Send to LLM via call_llm()\n    answer = call_llm(prompt, temperature=0.0, max_tokens=200)\n\n    return {\n        'question': question,\n        'retrieved': retrieved,\n        'answer': answer\n    }\n\n# Run the complete RAG pipeline\nquestion = \"What actions has the central bank taken regarding interest rates?\"\nresult = rag_query(question, index, corpus, model, k=3)\n\nprint(\"COMPLETE RAG PIPELINE\")\nprint(\"=\" * 70)\nprint(f\"\\nQuestion: {result['question']}\\n\")\n\nprint(\"Retrieved Documents:\")\nfor i, doc in enumerate(result['retrieved'], 1):\n    print(f\"  {i}. [{doc['score']:.3f}] {doc['document']}\")\n\nprint(\"\\n\" + \"-\" * 70)\nprint(\"LLM ANSWER (grounded in retrieved documents):\")\nprint(\"-\" * 70)\nprint(result['answer'])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "y9dnrbmnaaj",
   "source": "### Try More Questions\n\nSee how RAG grounds the LLM's answers in the retrieved documents:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ee33qs28bl5",
   "source": "# Try different questions\ntest_questions = [\n    \"What happened to mortgage rates?\",\n    \"Who won the football match?\",\n    \"How did inflation affect bank earnings?\",\n]\n\nfor q in test_questions:\n    result = rag_query(q, index, corpus, model, k=2)\n    print(f\"Q: {q}\")\n    print(f\"A: {result['answer'][:200]}...\")\n    print(\"-\" * 50)\n    print()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "rag-best-practices",
   "metadata": {},
   "source": "## 5.19 RAG Best Practices\n\nBuilding effective RAG systems requires attention to several factors:\n\n### Chunking Strategy\n\nDocuments are usually too long to embed directly. You need to split them into **chunks**.\n\n| Strategy | Pros | Cons |\n|----------|------|------|\n| Fixed size (e.g., 500 tokens) | Simple, predictable | May split mid-sentence |\n| Sentence-based | Natural boundaries | Variable sizes |\n| Paragraph-based | Preserves context | May be too large |\n| Semantic chunking | Best quality | More complex |\n\n### Metadata Filtering\n\nAdd metadata to enable filtering before or after search:\n- Document type (policy, FAQ, procedure)\n- Date (for recency)\n- Department (for access control)\n- Confidence scores\n\n### Reranking\n\nFirst-stage retrieval prioritizes recall (finding all relevant docs). **Reranking** improves precision:\n1. Retrieve top 20-50 candidates\n2. Use a more expensive model to rerank\n3. Return top 3-5 to the LLM\n\n### Hybrid Search\n\nCombine embedding search with keyword search:\n- Embeddings: semantic understanding\n- Keywords: exact matches (product codes, names)\n- Weighted combination of both scores"
  },
  {
   "cell_type": "markdown",
   "id": "7eebfe42",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Module Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "| Concept | What It Means |\n",
    "|---------|---------------|\n",
    "| **Embedding** | Vector representation of meaning |\n",
    "| **Similarity** | Cosine of angle between vectors |\n",
    "| **Corpus** | Collection of documents to search |\n",
    "| **Vector Database** | Efficient storage and search for embeddings |\n",
    "| **RAG** | Retrieval + LLM for grounded answers |\n",
    "\n",
    "### The RAG Pipeline\n",
    "\n",
    "```\n",
    "1. PREPARE (once):\n",
    "   Documents → Chunk → Embed → Store in Vector DB\n",
    "\n",
    "2. QUERY (each request):\n",
    "   Question → Embed → Search → Retrieve → Build Prompt → LLM → Answer\n",
    "```\n",
    "\n",
    "### Enterprise Implications\n",
    "\n",
    "- Embeddings enable semantic search beyond keywords\n",
    "- RAG grounds LLM answers in your actual documents\n",
    "- Vector databases scale to millions of documents\n",
    "- Retrieval is auditable — you can explain *why* an answer was given\n",
    "- This is how enterprise AI assistants work responsibly\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Quiz** — Test your understanding\n",
    "2. **Assessment** — Apply these concepts\n",
    "3. **Resources** — Further reading and tools"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
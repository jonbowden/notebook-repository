{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Module 5 Assessment — Embeddings & Vector Databases\n",
    "\n",
    "This assessment tests both your **practical skills** (coding tasks) and **conceptual understanding** (written task).\n",
    "\n",
    "## Assessment Structure\n",
    "- **5 Coding Tasks** (80 points): Implement embedding and retrieval operations\n",
    "- **1 Written Task** (20 points): Explain RAG and grounding\n",
    "\n",
    "## Instructions\n",
    "- **Coding tasks**: Complete the code cells with the exact variable names shown\n",
    "- **Written task**: Fill in the string variable with full sentences\n",
    "- Do **not** rename variables\n",
    "- Ensure the notebook runs top-to-bottom without errors\n",
    "- You may use the module content for reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup\n",
    "\n",
    "Run this cell first to install required packages and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install sentence-transformers scikit-learn faiss-cpu\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# Load the embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corpus-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Corpus for All Tasks\n",
    "\n",
    "Use this corpus for all coding tasks. Do not modify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corpus-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus of documents (do not modify)\n",
    "corpus = [\n",
    "    \"Interest rates were increased by the central bank to control inflation.\",\n",
    "    \"The central bank raised borrowing costs to fight rising prices.\",\n",
    "    \"Quarterly earnings improved as net interest margin widened.\",\n",
    "    \"The Federal Reserve announced a 25 basis point rate hike.\",\n",
    "    \"Mortgage rates have reached their highest level in 20 years.\",\n",
    "    \"Football is a popular sport played across Europe.\",\n",
    "    \"The team won the championship after a dramatic penalty shootout.\",\n",
    "    \"Basketball players competed in the international tournament.\"\n",
    "]\n",
    "\n",
    "print(f\"Corpus loaded: {len(corpus)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task1-md",
   "metadata": {},
   "source": "---\n## Task 1 — Generate Embeddings (15 points) [Coding]\n\n**Generate normalized embeddings for the entire corpus.**\n\nThe embedding model is already loaded in the Setup cell as `model`. Use it to encode the corpus.\n\nStore the result in `corpus_embeddings`. It should be a numpy array with shape `(8, 384)`.\n\n**Hint:** \n```python\ncorpus_embeddings = model.encode(corpus, normalize_embeddings=True)\n```\n\nThe `normalize_embeddings=True` parameter ensures each vector has length 1.0 (required for cosine similarity)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task1-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Generate normalized embeddings for the corpus\n",
    "# Store the result in corpus_embeddings\n",
    "\n",
    "corpus_embeddings = None  # YOUR CODE HERE\n",
    "\n",
    "# Verification (do not modify)\n",
    "if corpus_embeddings is not None:\n",
    "    print(f\"Shape: {corpus_embeddings.shape}\")\n",
    "    print(f\"First vector norm: {np.linalg.norm(corpus_embeddings[0]):.4f} (should be ~1.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task2-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2 — Calculate Similarity Scores (15 points) [Coding]\n",
    "\n",
    "**Calculate cosine similarity between the query and all corpus documents.**\n",
    "\n",
    "Query: `\"Why did the central bank increase rates?\"`\n",
    "\n",
    "Store the similarity scores in `similarity_scores`. It should be a 1D array of length 8.\n",
    "\n",
    "**Hint:** \n",
    "1. First encode the query with `normalize_embeddings=True`\n",
    "2. Use `cosine_similarity()` from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task2-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Calculate similarity scores\n",
    "query = \"Why did the central bank increase rates?\"\n",
    "\n",
    "similarity_scores = None  # YOUR CODE HERE\n",
    "\n",
    "# Verification (do not modify)\n",
    "if similarity_scores is not None:\n",
    "    print(f\"Scores shape: {similarity_scores.shape}\")\n",
    "    print(f\"Score range: {similarity_scores.min():.3f} to {similarity_scores.max():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task3-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 3 — Top-K Retrieval (15 points) [Coding]\n",
    "\n",
    "**Retrieve the indices of the top 3 most similar documents.**\n",
    "\n",
    "Store the result in `top_3_indices`. It should be an array of 3 indices, sorted by similarity (highest first).\n",
    "\n",
    "**Hint:** Use `np.argsort()` and remember to reverse the order (highest similarity first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task3-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Get indices of top 3 most similar documents\n",
    "# Use the similarity_scores from Task 2\n",
    "\n",
    "top_3_indices = None  # YOUR CODE HERE\n",
    "\n",
    "# Verification (do not modify)\n",
    "if top_3_indices is not None:\n",
    "    print(f\"Top 3 indices: {top_3_indices}\")\n",
    "    print(\"\\nTop 3 documents:\")\n",
    "    for i, idx in enumerate(top_3_indices):\n",
    "        print(f\"  {i+1}. [{similarity_scores[idx]:.3f}] {corpus[idx][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task4-md",
   "metadata": {},
   "source": "---\n## Task 4 — FAISS Index and Search (20 points) [Coding]\n\n**Build a FAISS index and search for the top 3 documents.**\n\nSteps:\n1. Create a FAISS `IndexFlatIP` index with dimension 384\n2. Add the corpus embeddings to the index (convert to float32 first)\n3. Search for the query and get top 3 results\n\nStore the search results in:\n- `faiss_distances`: The similarity scores from FAISS (shape: (1, 3))\n- `faiss_indices`: The document indices from FAISS (shape: (1, 3))\n\n**Hints:**\n```python\n# Step 1: Create index\nfaiss_index = faiss.IndexFlatIP(384)\n\n# Step 2: Add embeddings (must be float32)\nfaiss_index.add(corpus_embeddings.astype('float32'))\n\n# Step 3: Search (query must be 2D array with shape (1, 384))\nfaiss_distances, faiss_indices = faiss_index.search(query_embedding, k=3)\n```\n\n**Important:** FAISS requires the query to be a 2D array. The query embedding is already prepared for you below."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task4-code",
   "metadata": {},
   "outputs": [],
   "source": "# Task 4: Build FAISS index and search\n\n# Step 1: Create FAISS index (dimension 384 for all-MiniLM-L6-v2)\nfaiss_index = None  # YOUR CODE HERE\n\n# Step 2: Add embeddings to index (must be float32)\n# YOUR CODE HERE\n\n# Step 3: Search for top 3\n# Query embedding is prepared for you (2D array required by FAISS)\nquery_embedding = model.encode(\n    query,\n    convert_to_numpy=True,\n    normalize_embeddings=True\n).astype('float32').reshape(1, -1)  # reshape to (1, 384)\n\nfaiss_distances, faiss_indices = None, None  # YOUR CODE HERE\n\n# Verification (do not modify)\nif faiss_indices is not None:\n    print(f\"FAISS indices shape: {faiss_indices.shape}\")\n    print(f\"Top 3 indices from FAISS: {faiss_indices[0]}\")"
  },
  {
   "cell_type": "markdown",
   "id": "task5-md",
   "metadata": {},
   "source": "---\n## Task 5 — Build RAG Prompt (15 points) [Coding]\n\n**Build a RAG prompt that could be sent to an LLM for a grounded answer.**\n\nSteps:\n1. Create a prompt that instructs the LLM to answer based ONLY on the provided context\n2. Include the top 3 retrieved documents as context (use `top_3_indices` from Task 3)\n3. Include the original query\n\nStore the result in:\n- `rag_prompt`: The prompt string you build\n\n**Prompt format:**\n```\nAnswer the question based ONLY on the following context.\nIf the context doesn't contain the answer, say \"I don't have enough information.\"\n\nContext:\n- [document 1]\n- [document 2]\n- [document 3]\n\nQuestion: [query]\n\nAnswer:\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task5-code",
   "metadata": {},
   "outputs": [],
   "source": "# Task 5: Build RAG prompt\n# Use top_3_indices from Task 3 to build a prompt for the LLM\n\nrag_prompt = None  # YOUR CODE HERE\n\n# Verification (do not modify)\nif rag_prompt is not None:\n    print(\"=\" * 60)\n    print(\"RAG PROMPT:\")\n    print(\"=\" * 60)\n    print(rag_prompt)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bonus-md"
   },
   "source": [
    "---\n",
    "## Bonus — See RAG in Action (Not Graded)\n",
    "\n",
    "Run this cell to see a complete RAG pipeline working. This demonstrates how your code from Tasks 1-5 comes together.\n",
    "\n",
    "**Note:** This cell is for enrichment only and is **not graded**. LLM responses vary each time."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bonus-code"
   },
   "source": "# BONUS: Complete RAG Pipeline Demo (Not Graded)\n# This shows how all the pieces fit together\n\nimport requests\n\n# --- LLM Configuration (same as Task 5) ---\n# ------ OPTION A: Pinggy Tunnel (for Colab) ------\n# LLM_BASE_URL = \"https://your-pinggy-url.a.pinggy.io\"\n# LLM_API_KEY = None\n\n# ------ OPTION B: JBChat Server ------\nLLM_BASE_URL = \"https://jbchat.jonbowden.com.ngrok.app\"\nLLM_API_KEY = \"<provided-by-instructor>\"  # Get from instructor\nDEFAULT_MODEL = \"llama3.1:8b\"\n\ndef call_llm(prompt, model=DEFAULT_MODEL):\n    \"\"\"Send prompt to LLM. Auto-detects Ollama vs JBChat.\"\"\"\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"ngrok-skip-browser-warning\": \"true\",\n    }\n\n    use_jbchat = LLM_API_KEY and LLM_API_KEY != \"<provided-by-instructor>\"\n\n    if use_jbchat:\n        headers[\"X-API-Key\"] = LLM_API_KEY\n        endpoint = f\"{LLM_BASE_URL}/chat/direct\"\n        payload = {\n            \"model\": model,\n            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n            \"temperature\": 0.0,\n            \"stream\": False\n        }\n    else:\n        endpoint = f\"{LLM_BASE_URL}/api/chat\"\n        payload = {\n            \"model\": model,\n            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n            \"stream\": False\n        }\n\n    try:\n        response = requests.post(endpoint, headers=headers, json=payload, timeout=60)\n        response.raise_for_status()\n        return response.json()[\"message\"][\"content\"]\n    except Exception as e:\n        return f\"LLM Error: {e}\"\n\n# --- Run the RAG Demo ---\nif top_3_indices is not None:\n    demo_prompt = \"\"\"Answer the question based ONLY on the following context.\nIf the context doesn't contain the answer, say \"I don't have enough information.\"\n\nContext:\n\"\"\"\n    for idx in top_3_indices:\n        demo_prompt += f\"- {corpus[idx]}\\n\"\n    \n    demo_prompt += f\"\\nQuestion: {query}\\n\\nAnswer:\"\n    \n    print(\"=\" * 60)\n    print(\"SENDING TO LLM...\")\n    print(\"=\" * 60)\n    \n    demo_answer = call_llm(demo_prompt)\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\"LLM RESPONSE:\")\n    print(\"=\" * 60)\n    print(demo_answer)\n    print(\"\\n\" + \"=\" * 60)\n    print(\"This is RAG: Retrieved documents + LLM = Grounded answer!\")\n    print(\"=\" * 60)\nelse:\n    print(\"Complete Tasks 1-3 first to retrieve documents.\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "task6-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 6 — RAG and Grounding Explanation (20 points) [Written]\n",
    "\n",
    "**Prompt:** Explain why the top result is the most similar and how retrieval enables RAG.\n",
    "\n",
    "Include:\n",
    "- Why embeddings place semantically similar text close together\n",
    "- Why the top result matches the query (shared concepts, not just keywords)\n",
    "- How RAG uses retrieval to ground LLM answers in real documents\n",
    "- Why grounding reduces hallucinations in enterprise settings\n",
    "\n",
    "Write **6–10 sentences**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task6-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6: Written explanation\n",
    "\n",
    "rag_explanation = \"\"\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "submission-md",
   "metadata": {},
   "source": "---\n## Submission\n\nBefore submitting:\n1. **Restart kernel** and **Run All Cells** to ensure everything works\n2. Verify all coding tasks produce the expected outputs\n3. Verify your written explanation is complete and in your own words\n4. Save the notebook\n\n### How to Download from Colab\n1. Go to **File → Download → Download .ipynb**\n2. The file will download to your computer\n3. **Do not rename the file** — keep it as `Module5_Assessment.ipynb`\n\n### Submit\nUpload your completed notebook via the [Module 5 Assessment Form](https://docs.google.com/forms/d/e/1FAIpQLSd8bQIDEUnvHYXPoga7Rn9FIfrzij6s9NrL4y28wocyuSIQ6g/viewform).\n\n### Submission Checklist\n- [ ] All coding variables are filled with working code\n- [ ] Written explanation is thoughtful and **in your own words**\n- [ ] Notebook runs top-to-bottom without errors\n- [ ] Downloaded as .ipynb (not edited in a text editor)\n- [ ] File not renamed"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
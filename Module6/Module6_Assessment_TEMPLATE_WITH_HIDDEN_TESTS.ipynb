{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Module 6 Assessment â€” TEMPLATE WITH HIDDEN TESTS (Instructor/Grading)\n",
    "\n",
    "This template grades the student notebook deterministically (no LLMs).\n",
    "\n",
    "**Structure:**\n",
    "- 5 Coding Tasks (80 points): Unit tests with assertions\n",
    "- 1 Written Task (20 points): Keyword groups + minimum length\n",
    "\n",
    "Feedback written into `assessment_result.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup - standard library only\n",
    "import json\n",
    "import time\n",
    "from typing import Optional, Dict, Any, List, Callable, TypeVar\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scoring_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "__assessment_scores = {}\n",
    "__assessment_feedback = {}\n",
    "\n",
    "def record_score(task, points, max_points, feedback):\n",
    "    __assessment_scores[task] = (points, max_points)\n",
    "    __assessment_feedback[task] = feedback\n",
    "\n",
    "def validate_answer(\n",
    "    answer,\n",
    "    required_groups=None,\n",
    "    forbidden_strings=None,\n",
    "    forbidden_characters=None,\n",
    "    min_length=0,\n",
    "    max_length=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Validate an answer using string-level rules.\n",
    "    Returns (passed: bool, reasons: list[str])\n",
    "    \"\"\"\n",
    "    reasons = []\n",
    "    text = answer.strip()\n",
    "    t_lower = text.lower()\n",
    "\n",
    "    # Length checks\n",
    "    if len(text) < min_length:\n",
    "        reasons.append(f\"Too short (min {min_length} chars, got {len(text)})\")\n",
    "\n",
    "    if max_length is not None and len(text) > max_length:\n",
    "        reasons.append(f\"Too long (max {max_length} chars)\")\n",
    "\n",
    "    # Required keyword groups (AND logic - must have at least one from each group)\n",
    "    if required_groups:\n",
    "        for group in required_groups:\n",
    "            if not any(kw in t_lower for kw in group):\n",
    "                reasons.append(f\"Missing concept from: {group[:2]}...\")\n",
    "\n",
    "    # Forbidden substrings (AI detection)\n",
    "    if forbidden_strings:\n",
    "        matched = [s for s in forbidden_strings if s in t_lower]\n",
    "        if len(matched) >= 2:\n",
    "            reasons.append(f\"Appears AI-generated. Detected: {matched[:3]}\")\n",
    "\n",
    "    # Forbidden characters (markdown formatting from copy-paste)\n",
    "    if forbidden_characters:\n",
    "        found = [ch for ch in forbidden_characters if ch in text]\n",
    "        if found:\n",
    "            reasons.append(f\"Contains formatting characters (copy-paste?): {found}\")\n",
    "    \n",
    "    # Check for suspiciously perfect structure (numbered lists with consistent formatting)\n",
    "    import re\n",
    "    numbered_pattern = re.findall(r'^\\d+\\.\\s', text, re.MULTILINE)\n",
    "    if len(numbered_pattern) >= 4:\n",
    "        reasons.append(\"Suspiciously structured (numbered list format typical of AI)\")\n",
    "    \n",
    "    # Check for excessive use of transitional phrases\n",
    "    transitions = [\"furthermore\", \"moreover\", \"additionally\", \"consequently\", \n",
    "                   \"therefore\", \"thus\", \"hence\", \"accordingly\", \"subsequently\"]\n",
    "    transition_count = sum(1 for t in transitions if t in t_lower)\n",
    "    if transition_count >= 3:\n",
    "        reasons.append(f\"Excessive formal transitions ({transition_count} found) - likely AI\")\n",
    "\n",
    "    passed = len(reasons) == 0\n",
    "    return passed, reasons\n",
    "\n",
    "# Common AI phrases that indicate copy-paste from ChatGPT/Claude\n",
    "AI_PHRASES = [\n",
    "    # Self-identification\n",
    "    \"as an ai\",\n",
    "    \"as a large language model\",\n",
    "    \"i'm happy to help\",\n",
    "    \"i'd be happy to\",\n",
    "    \"i cannot\",\n",
    "    \"i can't provide\",\n",
    "    \n",
    "    # Structural phrases\n",
    "    \"let me explain\",\n",
    "    \"let me break this down\",\n",
    "    \"let's dive into\",\n",
    "    \"let's explore\",\n",
    "    \"here's a comprehensive\",\n",
    "    \"here's an overview\",\n",
    "    \"here are the key\",\n",
    "    \n",
    "    # Emphasis phrases\n",
    "    \"it's important to note that\",\n",
    "    \"it's worth noting that\",\n",
    "    \"it is important to understand\",\n",
    "    \"it's crucial to\",\n",
    "    \"it's essential to\",\n",
    "    \"it bears mentioning\",\n",
    "    \n",
    "    # Summary phrases\n",
    "    \"in summary,\",\n",
    "    \"in conclusion,\",\n",
    "    \"to summarize,\",\n",
    "    \"to sum up,\",\n",
    "    \"overall,\",\n",
    "    \"in short,\",\n",
    "    \n",
    "    # Filler phrases\n",
    "    \"first and foremost\",\n",
    "    \"last but not least\",\n",
    "    \"needless to say\",\n",
    "    \"it goes without saying\",\n",
    "    \n",
    "    # Overused AI words\n",
    "    \"delve into\",\n",
    "    \"delve deeper\",\n",
    "    \"crucial to understand\",\n",
    "    \"landscape of\",\n",
    "    \"realm of\",\n",
    "    \"paradigm\",\n",
    "    \"leverage the power\",\n",
    "    \"harness the capabilities\",\n",
    "    \"utilize\",\n",
    "    \"facilitate\",\n",
    "    \"comprehensive understanding\",\n",
    "    \"nuanced\",\n",
    "    \"robust\",\n",
    "    \"seamless\",\n",
    "    \"streamline\",\n",
    "    \n",
    "    # Position phrases\n",
    "    \"at its core,\",\n",
    "    \"fundamentally,\",\n",
    "    \"essentially,\",\n",
    "    \"in essence,\",\n",
    "    \"inherently,\",\n",
    "    \"intrinsically,\",\n",
    "    \n",
    "    # Emphasis adjectives\n",
    "    \"pivotal role\",\n",
    "    \"multifaceted\",\n",
    "    \"myriad of\",\n",
    "    \"plethora of\",\n",
    "    \"wide array of\",\n",
    "    \"diverse range of\",\n",
    "    \n",
    "    # AI hedging\n",
    "    \"it's worth mentioning\",\n",
    "    \"one could argue\",\n",
    "    \"it can be said that\",\n",
    "    \"generally speaking\",\n",
    "    \n",
    "    # ChatGPT specific\n",
    "    \"i hope this helps\",\n",
    "    \"feel free to ask\",\n",
    "    \"happy to clarify\",\n",
    "    \"let me know if\",\n",
    "    \n",
    "    # Claude specific  \n",
    "    \"i appreciate\",\n",
    "    \"great question\",\n",
    "    \"that's a thoughtful\",\n",
    "]\n",
    "\n",
    "# Markdown formatting characters that suggest copy-paste\n",
    "FORBIDDEN_CHARS = [\"##\", \"**\", \"```\", \"* \", \"- [ ]\", \"###\", \">>>\", \"===\"]\n",
    "\n",
    "# Written task rules\n",
    "WRITTEN_RULES = {\n",
    "  \"Task 6\": {\n",
    "      \"var\": \"production_explanation\",\n",
    "      \"min_len\": 400,\n",
    "      \"max_points\": 20,\n",
    "      \"groups\": [\n",
    "          [\"service\", \"api\", \"remote\", \"external\"],\n",
    "          [\"retry\", \"backoff\", \"fail\", \"error\"],\n",
    "          [\"valid\", \"json\", \"parse\", \"check\"],\n",
    "          [\"test\", \"mock\", \"unit\", \"cost\"]\n",
    "      ]\n",
    "  }\n",
    "}\n",
    "\n",
    "print(\"Scoring infrastructure ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tests_header",
   "metadata": {},
   "source": [
    "---\n",
    "## Task Tests (1-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task1_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: LLM Client Class (20 points) [Coding]\n",
    "points = 0\n",
    "fb = []\n",
    "max_points = 20\n",
    "\n",
    "try:\n",
    "    assert \"LLMClient\" in globals(), \"LLMClient class not defined\"\n",
    "    LLMClient_class = globals()[\"LLMClient\"]\n",
    "    \n",
    "    # Test 1: Can instantiate with all parameters\n",
    "    client = LLMClient_class(\"http://localhost:11434/\", api_key=\"test-key\", model=\"test-model\")\n",
    "    fb.append(\"\\u2713 Test 1 passed (instantiation works)\")\n",
    "    points += 5\n",
    "    \n",
    "    # Test 2: base_url stored correctly with trailing slash removed\n",
    "    assert hasattr(client, 'base_url'), \"Client missing base_url attribute\"\n",
    "    assert client.base_url == \"http://localhost:11434\", f\"base_url not cleaned: {client.base_url}\"\n",
    "    fb.append(\"\\u2713 Test 2 passed (base_url cleaned)\")\n",
    "    points += 5\n",
    "    \n",
    "    # Test 3: api_key stored correctly\n",
    "    assert hasattr(client, 'api_key'), \"Client missing api_key attribute\"\n",
    "    assert client.api_key == \"test-key\", f\"api_key not stored: {client.api_key}\"\n",
    "    fb.append(\"\\u2713 Test 3 passed (api_key stored)\")\n",
    "    points += 5\n",
    "    \n",
    "    # Test 4: model stored correctly\n",
    "    assert hasattr(client, 'model'), \"Client missing model attribute\"\n",
    "    assert client.model == \"test-model\", f\"model not stored: {client.model}\"\n",
    "    fb.append(\"\\u2713 Test 4 passed (model stored)\")\n",
    "    points += 5\n",
    "    \n",
    "except AssertionError as e:\n",
    "    fb.append(f\"\\u2717 {e}\")\n",
    "except Exception as e:\n",
    "    fb.append(f\"\\u2717 Runtime error: {e}\")\n",
    "\n",
    "record_score(\"Task 1 - LLM Client Class\", points, max_points, fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task2_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: JSON Response Parser (15 points) [Coding]\n",
    "points = 0\n",
    "fb = []\n",
    "max_points = 15\n",
    "\n",
    "try:\n",
    "    assert \"parse_json_response\" in globals(), \"parse_json_response function not defined\"\n",
    "    parse_fn = globals()[\"parse_json_response\"]\n",
    "    \n",
    "    # Test 1: Parse plain JSON\n",
    "    result = parse_fn('{\"status\": \"ok\"}')\n",
    "    assert result == {\"status\": \"ok\"}, f\"Plain JSON parse failed: {result}\"\n",
    "    fb.append(\"\\u2713 Test 1 passed (plain JSON)\")\n",
    "    points += 5\n",
    "    \n",
    "    # Test 2: Parse JSON with markdown code block\n",
    "    result = parse_fn('```json\\n{\"status\": \"ok\"}\\n```')\n",
    "    assert result == {\"status\": \"ok\"}, f\"Markdown JSON parse failed: {result}\"\n",
    "    fb.append(\"\\u2713 Test 2 passed (markdown code block)\")\n",
    "    points += 5\n",
    "    \n",
    "    # Test 3: Raise ValueError on invalid JSON\n",
    "    try:\n",
    "        parse_fn(\"not valid json\")\n",
    "        fb.append(\"\\u2717 Test 3 failed (should raise ValueError)\")\n",
    "    except ValueError as e:\n",
    "        assert \"invalid json\" in str(e).lower(), f\"Wrong error message: {e}\"\n",
    "        fb.append(\"\\u2713 Test 3 passed (raises ValueError)\")\n",
    "        points += 5\n",
    "    \n",
    "except AssertionError as e:\n",
    "    fb.append(f\"\\u2717 {e}\")\n",
    "except Exception as e:\n",
    "    fb.append(f\"\\u2717 Runtime error: {e}\")\n",
    "\n",
    "record_score(\"Task 2 - JSON Parser\", points, max_points, fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task3_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Retry with Exponential Backoff (20 points) [Coding]\n",
    "points = 0\n",
    "fb = []\n",
    "max_points = 20\n",
    "\n",
    "try:\n",
    "    assert \"retry_with_backoff\" in globals(), \"retry_with_backoff function not defined\"\n",
    "    retry_fn = globals()[\"retry_with_backoff\"]\n",
    "    \n",
    "    # Test 1: Successful call returns result\n",
    "    result = retry_fn(lambda: \"success\", max_retries=3, base_delay=0.001)\n",
    "    assert result == \"success\", f\"Successful call should return result: {result}\"\n",
    "    fb.append(\"\\u2713 Test 1 passed (returns result on success)\")\n",
    "    points += 5\n",
    "    \n",
    "    # Test 2: Retries on failure then succeeds\n",
    "    attempt_count = [0]\n",
    "    def fail_twice():\n",
    "        attempt_count[0] += 1\n",
    "        if attempt_count[0] < 3:\n",
    "            raise RuntimeError(\"fail\")\n",
    "        return \"worked\"\n",
    "    \n",
    "    result = retry_fn(fail_twice, max_retries=3, base_delay=0.001)\n",
    "    assert result == \"worked\", f\"Should succeed after retries: {result}\"\n",
    "    assert attempt_count[0] == 3, f\"Should have tried 3 times: {attempt_count[0]}\"\n",
    "    fb.append(\"\\u2713 Test 2 passed (retries then succeeds)\")\n",
    "    points += 5\n",
    "    \n",
    "    # Test 3: Raises exception after max retries\n",
    "    def always_fail():\n",
    "        raise RuntimeError(\"permanent failure\")\n",
    "    \n",
    "    try:\n",
    "        retry_fn(always_fail, max_retries=2, base_delay=0.001)\n",
    "        fb.append(\"\\u2717 Test 3 failed (should raise after max retries)\")\n",
    "    except RuntimeError as e:\n",
    "        assert \"permanent failure\" in str(e)\n",
    "        fb.append(\"\\u2713 Test 3 passed (raises after max retries)\")\n",
    "        points += 5\n",
    "    \n",
    "    # Test 4: Check exponential backoff timing (approximate)\n",
    "    timing_attempts = [0]\n",
    "    timing_start = time.time()\n",
    "    \n",
    "    def timed_fail():\n",
    "        timing_attempts[0] += 1\n",
    "        if timing_attempts[0] < 3:\n",
    "            raise RuntimeError(\"fail\")\n",
    "        return \"done\"\n",
    "    \n",
    "    retry_fn(timed_fail, max_retries=3, base_delay=0.05)  # 50ms base\n",
    "    elapsed = time.time() - timing_start\n",
    "    # Should take at least 0.05 + 0.1 = 0.15 seconds (first retry + second retry)\n",
    "    # Allow some tolerance\n",
    "    if elapsed >= 0.1:\n",
    "        fb.append(\"\\u2713 Test 4 passed (exponential delay)\")\n",
    "        points += 5\n",
    "    else:\n",
    "        fb.append(f\"\\u2717 Test 4 failed (elapsed {elapsed:.3f}s, expected >= 0.1s)\")\n",
    "    \n",
    "except AssertionError as e:\n",
    "    fb.append(f\"\\u2717 {e}\")\n",
    "except Exception as e:\n",
    "    fb.append(f\"\\u2717 Runtime error: {e}\")\n",
    "\n",
    "record_score(\"Task 3 - Retry with Backoff\", points, max_points, fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task4_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Schema Validation (15 points) [Coding]\n",
    "points = 0\n",
    "fb = []\n",
    "max_points = 15\n",
    "\n",
    "try:\n",
    "    assert \"validate_schema\" in globals(), \"validate_schema function not defined\"\n",
    "    validate_fn = globals()[\"validate_schema\"]\n",
    "    \n",
    "    # Test 1: Valid data passes\n",
    "    result = validate_fn({\"name\": \"Alice\", \"age\": 30}, [\"name\", \"age\"])\n",
    "    assert result == {\"name\": \"Alice\", \"age\": 30}, f\"Valid data should be returned: {result}\"\n",
    "    fb.append(\"\\u2713 Test 1 passed (valid data passes)\")\n",
    "    points += 5\n",
    "    \n",
    "    # Test 2: Missing single field raises ValueError\n",
    "    try:\n",
    "        validate_fn({\"name\": \"Alice\"}, [\"name\", \"age\"])\n",
    "        fb.append(\"\\u2717 Test 2 failed (should raise ValueError)\")\n",
    "    except ValueError as e:\n",
    "        assert \"missing\" in str(e).lower() or \"age\" in str(e), f\"Error should mention missing fields: {e}\"\n",
    "        fb.append(\"\\u2713 Test 2 passed (raises on missing field)\")\n",
    "        points += 5\n",
    "    \n",
    "    # Test 3: Missing multiple fields lists them\n",
    "    try:\n",
    "        validate_fn({}, [\"field1\", \"field2\", \"field3\"])\n",
    "        fb.append(\"\\u2717 Test 3 failed (should raise ValueError)\")\n",
    "    except ValueError as e:\n",
    "        error_str = str(e).lower()\n",
    "        # Should mention missing fields\n",
    "        if \"missing\" in error_str or \"field\" in error_str:\n",
    "            fb.append(\"\\u2713 Test 3 passed (lists missing fields)\")\n",
    "            points += 5\n",
    "        else:\n",
    "            fb.append(f\"\\u2717 Test 3 failed (should list missing fields): {e}\")\n",
    "    \n",
    "except AssertionError as e:\n",
    "    fb.append(f\"\\u2717 {e}\")\n",
    "except Exception as e:\n",
    "    fb.append(f\"\\u2717 Runtime error: {e}\")\n",
    "\n",
    "record_score(\"Task 4 - Schema Validation\", points, max_points, fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task5_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5: Build JSON Prompt (10 points) [Coding]\n",
    "points = 0\n",
    "fb = []\n",
    "max_points = 10\n",
    "\n",
    "try:\n",
    "    assert \"build_json_prompt\" in globals(), \"build_json_prompt function not defined\"\n",
    "    build_fn = globals()[\"build_json_prompt\"]\n",
    "    \n",
    "    test_schema = {\"sentiment\": \"positive|negative|neutral\"}\n",
    "    prompt = build_fn(\"Analyze sentiment\", test_schema)\n",
    "    \n",
    "    assert prompt is not None, \"Function returned None\"\n",
    "    assert isinstance(prompt, str), f\"Should return string, got {type(prompt)}\"\n",
    "    \n",
    "    prompt_lower = prompt.lower()\n",
    "    \n",
    "    # Test 1: Contains \"only\" instruction\n",
    "    if \"only\" in prompt_lower:\n",
    "        fb.append(\"\\u2713 Test 1 passed (contains ONLY instruction)\")\n",
    "        points += 3\n",
    "    else:\n",
    "        fb.append(\"\\u2717 Test 1 failed (should contain 'ONLY')\")\n",
    "    \n",
    "    # Test 2: Contains \"json\" keyword\n",
    "    if \"json\" in prompt_lower:\n",
    "        fb.append(\"\\u2713 Test 2 passed (mentions JSON)\")\n",
    "        points += 3\n",
    "    else:\n",
    "        fb.append(\"\\u2717 Test 2 failed (should mention JSON)\")\n",
    "    \n",
    "    # Test 3: Contains the schema fields\n",
    "    if \"sentiment\" in prompt_lower:\n",
    "        fb.append(\"\\u2713 Test 3 passed (includes schema)\")\n",
    "        points += 4\n",
    "    else:\n",
    "        fb.append(\"\\u2717 Test 3 failed (should include schema)\")\n",
    "    \n",
    "except AssertionError as e:\n",
    "    fb.append(f\"\\u2717 {e}\")\n",
    "except Exception as e:\n",
    "    fb.append(f\"\\u2717 Runtime error: {e}\")\n",
    "\n",
    "record_score(\"Task 5 - Build JSON Prompt\", points, max_points, fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task6_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6: Production LLM Integration (20 points) [Written]\n",
    "points = 0\n",
    "fb = []\n",
    "try:\n",
    "    r = WRITTEN_RULES[\"Task 6\"]\n",
    "    assert r[\"var\"] in globals(), f\"{r['var']} variable missing\"\n",
    "    text = globals()[r[\"var\"]]\n",
    "    \n",
    "    passed, reasons = validate_answer(\n",
    "        text,\n",
    "        required_groups=r[\"groups\"],\n",
    "        forbidden_strings=AI_PHRASES,\n",
    "        forbidden_characters=FORBIDDEN_CHARS,\n",
    "        min_length=r[\"min_len\"]\n",
    "    )\n",
    "    \n",
    "    if passed:\n",
    "        points = r[\"max_points\"]\n",
    "        fb.append(\"\\u2713 Passed\")\n",
    "    else:\n",
    "        for reason in reasons:\n",
    "            fb.append(f\"\\u2717 {reason}\")\n",
    "            \n",
    "except AssertionError as e:\n",
    "    fb.append(f\"\\u2717 {e}\")\n",
    "record_score(\"Task 6 - Production Explanation\", points, WRITTEN_RULES[\"Task 6\"][\"max_points\"], fb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "output_header",
   "metadata": {},
   "source": [
    "---\n",
    "## Generate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, datetime, re\n",
    "\n",
    "# Sort scores by task number (extract number from \"Task N - ...\")\n",
    "def task_sort_key(item):\n",
    "    match = re.search(r'Task (\\d+)', item[0])\n",
    "    return int(match.group(1)) if match else 99\n",
    "\n",
    "sorted_scores = dict(sorted(__assessment_scores.items(), key=task_sort_key))\n",
    "sorted_feedback = {k: __assessment_feedback[k] for k in sorted_scores.keys()}\n",
    "\n",
    "# Calculate totals\n",
    "total_points = sum(s[0] for s in sorted_scores.values())\n",
    "max_possible = sum(s[1] for s in sorted_scores.values())\n",
    "\n",
    "result = {\n",
    "  \"scores\": sorted_scores,\n",
    "  \"feedback\": sorted_feedback,\n",
    "  \"total\": f\"{total_points}/{max_possible}\",\n",
    "  \"percentage\": round(100 * total_points / max_possible, 1) if max_possible > 0 else 0,\n",
    "  \"timestamp\": datetime.datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "with open(\"assessment_result.json\", \"w\") as f:\n",
    "    json.dump(result, f, indent=2)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"ASSESSMENT RESULTS: {total_points}/{max_possible} ({result['percentage']}%)\")\n",
    "print(f\"{'='*50}\\n\")\n",
    "\n",
    "for task, (pts, mx) in sorted_scores.items():\n",
    "    status = \"\\u2713\" if pts == mx else \"\\u2717\" if pts == 0 else \"~\"\n",
    "    print(f\"{status} {task}: {pts}/{mx}\")\n",
    "    for line in sorted_feedback[task]:\n",
    "        print(f\"    {line}\")\n",
    "\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

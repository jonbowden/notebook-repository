{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Module 6 Assessment — LLM APIs (Python)\n",
    "\n",
    "This assessment tests both your **practical skills** (coding tasks) and **conceptual understanding** (written task).\n",
    "\n",
    "## Assessment Structure\n",
    "- **5 Coding Tasks** (80 points): Build LLM client components\n",
    "- **1 Written Task** (20 points): Explain production LLM integration\n",
    "\n",
    "## Instructions\n",
    "- **Coding tasks**: Complete the code cells with the exact variable names shown\n",
    "- **Written task**: Fill in the string variable with full sentences\n",
    "- Do **not** rename variables\n",
    "- Ensure the notebook runs top-to-bottom without errors\n",
    "- You may use the module content for reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup\n",
    "\n",
    "Run this cell first to set up the environment. No external packages required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from typing import Optional, Dict, Any, List, Callable, TypeVar\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task1-md",
   "metadata": {},
   "source": "---\n## Task 1 — LLM Client Class (20 points) [Coding]\n\n**Build an LLM client class that supports both server-side and local execution.**\n\nCreate a class called `LLMClient` that:\n\n1. Accepts parameters:\n   - `base_url` (str)\n   - `api_key` (optional, default `None`)\n   - `model` (default `\"phi3:mini\"`)\n\n2. Stores these as instance attributes, with `base_url` having trailing slashes removed\n\n3. Sets `self.endpoint` based on whether an API key is provided:\n   - If `api_key` is provided → `/chat/direct`\n   - If `api_key` is `None` → `/api/chat`\n\n4. Creates `self.headers` dictionary with:\n   - Always: `\"Content-Type\": \"application/json\"`\n   - If `api_key` provided: also include `\"X-API-Key\": api_key`\n\n**Hint:**\n```python\nclass LLMClient:\n    def __init__(self, base_url: str, api_key: str = None, model: str = \"phi3:mini\"):\n        self.base_url = base_url.rstrip('/')\n        self.api_key = api_key\n        self.model = model\n        self.endpoint = \"/chat/direct\" if api_key else \"/api/chat\"\n        self.headers = {\"Content-Type\": \"application/json\"}\n        if api_key:\n            self.headers[\"X-API-Key\"] = api_key\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task1-code",
   "metadata": {},
   "outputs": [],
   "source": "# Task 1: Create the LLMClient class\n# YOUR CODE HERE\n\nclass LLMClient:\n    pass  # Replace with your implementation\n\n\n# Create client instance (choose ONE option)\n\n# Option A — Server-side gateway (recommended for Colab)\nclient = LLMClient(\n    base_url=\"https://jbchat.jonbowden.com.ngrok.app\",\n    api_key=\"INSTRUCTOR_SUPPLIED_KEY\",\n    model=\"phi3:mini\"\n)\n\n# Option B — Local LLM (only works outside Colab)\n# client = LLMClient(\n#     base_url=\"http://localhost:11434\",\n#     api_key=None,\n#     model=\"phi3:mini\"\n# )\n\n\n# Verification (do not modify)\nprint(\"Client Configuration:\")\nprint(f\"  Base URL : {client.base_url}\")\nprint(f\"  Endpoint : {client.endpoint}\")\nprint(f\"  Model    : {client.model}\")\nprint(f\"  API Key  : {'SET' if client.api_key else 'NOT SET'}\")\nprint(f\"  Headers  : {client.headers}\")"
  },
  {
   "cell_type": "markdown",
   "id": "task2-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2 — JSON Response Parser (15 points) [Coding]\n",
    "\n",
    "**Create a function to safely parse JSON from LLM responses.**\n",
    "\n",
    "LLMs often return JSON wrapped in markdown code blocks or with extra whitespace. Create a function called `parse_json_response` that:\n",
    "\n",
    "1. Strips leading/trailing whitespace\n",
    "2. Removes markdown code blocks if present (` ```json ` and ` ``` `)\n",
    "3. Parses the cleaned text as JSON\n",
    "4. Returns the parsed dictionary\n",
    "5. Raises `ValueError` with message `\"Invalid JSON\"` if parsing fails\n",
    "\n",
    "**Hint:**\n",
    "```python\n",
    "def parse_json_response(text: str) -> dict:\n",
    "    text = text.strip()\n",
    "    if text.startswith(\"```json\"):\n",
    "        text = text[7:]  # Remove ```json\n",
    "    # ... handle other cases\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        raise ValueError(\"Invalid JSON\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task2-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Create the parse_json_response function\n",
    "# YOUR CODE HERE\n",
    "\n",
    "def parse_json_response(text: str) -> dict:\n",
    "    pass  # Replace with your implementation\n",
    "\n",
    "# Verification (do not modify)\n",
    "test_cases = [\n",
    "    '{\"status\": \"ok\"}',\n",
    "    '```json\\n{\"status\": \"ok\"}\\n```',\n",
    "    '  {\"status\": \"ok\"}  ',\n",
    "]\n",
    "\n",
    "for tc in test_cases:\n",
    "    try:\n",
    "        result = parse_json_response(tc)\n",
    "        print(f\"Parsed: {result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task3-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 3 — Retry with Exponential Backoff (20 points) [Coding]\n",
    "\n",
    "**Implement a retry function with exponential backoff.**\n",
    "\n",
    "Create a function called `retry_with_backoff` that:\n",
    "\n",
    "1. Takes a function `fn` (no arguments), `max_retries` (default 3), and `base_delay` (default 1.0)\n",
    "2. Tries to execute `fn()`\n",
    "3. If it raises an exception, waits `base_delay * (2 ** attempt)` seconds before retrying\n",
    "4. After `max_retries` failures, re-raises the last exception\n",
    "5. Returns the result of `fn()` on success\n",
    "\n",
    "**Important:** The delay calculation should be:\n",
    "- After 1st failure: `base_delay * (2 ** 0)` = 1.0 seconds\n",
    "- After 2nd failure: `base_delay * (2 ** 1)` = 2.0 seconds\n",
    "- After 3rd failure: `base_delay * (2 ** 2)` = 4.0 seconds\n",
    "\n",
    "**Hint:**\n",
    "```python\n",
    "T = TypeVar('T')\n",
    "\n",
    "def retry_with_backoff(fn: Callable[[], T], max_retries: int = 3, base_delay: float = 1.0) -> T:\n",
    "    last_exception = None\n",
    "    for attempt in range(max_retries + 1):\n",
    "        try:\n",
    "            return fn()\n",
    "        except Exception as e:\n",
    "            last_exception = e\n",
    "            if attempt < max_retries:\n",
    "                time.sleep(base_delay * (2 ** attempt))\n",
    "    raise last_exception\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task3-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Create the retry_with_backoff function\n",
    "# YOUR CODE HERE\n",
    "\n",
    "T = TypeVar('T')\n",
    "\n",
    "def retry_with_backoff(fn: Callable[[], T], max_retries: int = 3, base_delay: float = 1.0) -> T:\n",
    "    pass  # Replace with your implementation\n",
    "\n",
    "# Verification (do not modify)\n",
    "# Test 1: Successful call\n",
    "result = retry_with_backoff(lambda: \"success\", max_retries=3, base_delay=0.01)\n",
    "print(f\"Test 1 - Successful call: {result}\")\n",
    "\n",
    "# Test 2: Failing call that eventually succeeds\n",
    "fail_counter = [0]\n",
    "def failing_then_success():\n",
    "    fail_counter[0] += 1\n",
    "    if fail_counter[0] < 3:\n",
    "        raise RuntimeError(\"Not yet!\")\n",
    "    return \"finally worked\"\n",
    "\n",
    "result = retry_with_backoff(failing_then_success, max_retries=3, base_delay=0.01)\n",
    "print(f\"Test 2 - Retry success: {result} (attempts: {fail_counter[0]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task4-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 4 — Schema Validation (15 points) [Coding]\n",
    "\n",
    "**Create a function to validate JSON responses against a schema.**\n",
    "\n",
    "Create a function called `validate_schema` that:\n",
    "\n",
    "1. Takes `data` (a dictionary), `required_fields` (a list of field names)\n",
    "2. Checks if all required fields exist in the data\n",
    "3. If any fields are missing, raises `ValueError` with message `\"Missing fields: [field1, field2, ...]\"` (list the missing fields)\n",
    "4. Returns the data if validation passes\n",
    "\n",
    "**Hint:**\n",
    "```python\n",
    "def validate_schema(data: dict, required_fields: list) -> dict:\n",
    "    missing = [f for f in required_fields if f not in data]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing fields: {missing}\")\n",
    "    return data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task4-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Create the validate_schema function\n",
    "# YOUR CODE HERE\n",
    "\n",
    "def validate_schema(data: dict, required_fields: list) -> dict:\n",
    "    pass  # Replace with your implementation\n",
    "\n",
    "# Verification (do not modify)\n",
    "# Test 1: Valid data\n",
    "try:\n",
    "    result = validate_schema({\"name\": \"Alice\", \"age\": 30}, [\"name\", \"age\"])\n",
    "    print(f\"Test 1 - Valid: {result}\")\n",
    "except ValueError as e:\n",
    "    print(f\"Test 1 - Error: {e}\")\n",
    "\n",
    "# Test 2: Missing field\n",
    "try:\n",
    "    result = validate_schema({\"name\": \"Alice\"}, [\"name\", \"age\"])\n",
    "    print(f\"Test 2 - Should have failed!\")\n",
    "except ValueError as e:\n",
    "    print(f\"Test 2 - Expected error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task5-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 5 — Build JSON Prompt (10 points) [Coding]\n",
    "\n",
    "**Create a function to build prompts that request JSON output.**\n",
    "\n",
    "Create a function called `build_json_prompt` that:\n",
    "\n",
    "1. Takes `task` (string describing what to do) and `schema` (dictionary showing expected output format)\n",
    "2. Returns a prompt string that instructs the LLM to return ONLY valid JSON\n",
    "3. The prompt must include the schema as a JSON string\n",
    "4. The prompt must instruct the LLM to return \"ONLY valid JSON\" with \"no other text\"\n",
    "\n",
    "**Example output:**\n",
    "```\n",
    "Task: Analyze the sentiment\n",
    "\n",
    "Return ONLY valid JSON matching this schema:\n",
    "{\"sentiment\": \"positive|negative|neutral\"}\n",
    "\n",
    "Rules:\n",
    "- Return ONLY the JSON object\n",
    "- No other text\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task5-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5: Create the build_json_prompt function\n",
    "# YOUR CODE HERE\n",
    "\n",
    "def build_json_prompt(task: str, schema: dict) -> str:\n",
    "    pass  # Replace with your implementation\n",
    "\n",
    "# Verification (do not modify)\n",
    "test_schema = {\"sentiment\": \"positive|negative|neutral\", \"confidence\": \"float\"}\n",
    "prompt = build_json_prompt(\"Analyze the sentiment of the text\", test_schema)\n",
    "\n",
    "if prompt:\n",
    "    print(\"Generated prompt:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(prompt)\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"\\nContains 'ONLY': {'only' in prompt.lower()}\")\n",
    "    print(f\"Contains schema: {'sentiment' in prompt}\")\n",
    "else:\n",
    "    print(\"No prompt generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bonus-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Bonus — Test Your Client (Not Graded)\n",
    "\n",
    "If you have access to an LLM server, you can test your components together. This is for enrichment only.\n",
    "\n",
    "**Note:** This cell is **not graded** and may not work depending on your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bonus-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS: Integration test (Not Graded)\n",
    "# Uncomment and modify if you want to test against a real LLM\n",
    "\n",
    "\"\"\"\n",
    "import requests\n",
    "\n",
    "# Configuration\n",
    "LLM_BASE_URL = \"http://localhost:11434\"  # Or your Pinggy tunnel URL\n",
    "LLM_MODEL = \"phi3:mini\"\n",
    "\n",
    "# Test the full pipeline\n",
    "prompt = build_json_prompt(\n",
    "    task=\"Classify the sentiment of: 'I love this product!'\",\n",
    "    schema={\"sentiment\": \"positive|negative|neutral\"}\n",
    ")\n",
    "\n",
    "def call_llm():\n",
    "    response = requests.post(\n",
    "        f\"{LLM_BASE_URL}/api/chat\",\n",
    "        json={\n",
    "            \"model\": LLM_MODEL,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"stream\": False\n",
    "        },\n",
    "        timeout=(5, 60)\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response.json()[\"message\"][\"content\"]\n",
    "\n",
    "# Use retry with backoff\n",
    "raw_response = retry_with_backoff(call_llm, max_retries=2, base_delay=1.0)\n",
    "print(f\"Raw response: {raw_response}\")\n",
    "\n",
    "# Parse and validate\n",
    "parsed = parse_json_response(raw_response)\n",
    "validated = validate_schema(parsed, [\"sentiment\"])\n",
    "print(f\"Validated result: {validated}\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"Bonus cell - uncomment code above to test with a real LLM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task6-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 6 — Production LLM Integration (20 points) [Written]\n",
    "\n",
    "**Prompt:** Explain why production LLM systems require defensive programming.\n",
    "\n",
    "Include:\n",
    "- Why LLM APIs should be treated as unreliable external services (not simple functions)\n",
    "- Why retry logic with exponential backoff is important\n",
    "- Why you must validate LLM responses before using them\n",
    "- Why unit tests should never call live LLM APIs\n",
    "\n",
    "Write **6–10 sentences** in your own words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task6-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6: Written explanation\n",
    "\n",
    "production_explanation = \"\"\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "submission-md",
   "metadata": {},
   "source": "---\n## Submission\n\nBefore submitting:\n1. **Restart kernel** and **Run All Cells** to ensure everything works\n2. Verify all coding tasks produce the expected outputs\n3. Verify your written explanation is complete and in your own words\n4. Save the notebook\n\n### How to Download from Colab\n1. Go to **File → Download → Download .ipynb**\n2. The file will download to your computer\n3. **Do not rename the file** — keep it as `Module6_Assessment.ipynb`\n\n### Submit\nUpload your completed notebook via the [Module 6 Assessment Form](https://docs.google.com/forms/d/e/1FAIpQLScGwetqP0pNS8ZJ8g8beV8TnCPhd3D8iLK5jA0EhO4K0sRAkg/viewform).\n\n### Submission Checklist\n- [ ] All coding variables are filled with working code\n- [ ] Written explanation is thoughtful and **in your own words**\n- [ ] Notebook runs top-to-bottom without errors\n- [ ] Downloaded as .ipynb (not edited in a text editor)\n- [ ] File not renamed"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84504bbe",
   "metadata": {},
   "source": "# Content\n\n**Module 6 — LLM APIs (Python)**\n\n**CodeVision Academy**\n\n## Overview\n\nIf Module 5 taught you *how to find relevant information*, Module 6 teaches you *how to reliably call the AI that uses it*.\n\nThis module marks the transition from **using AI** to **engineering AI systems**.\n\nUp to now, you've called LLMs casually—paste a prompt, get a response. That works for demos. It does not work for production systems that must:\n- Handle failures gracefully\n- Validate outputs before using them\n- Control costs and latency\n- Pass audits and compliance reviews\n\n---\n\n## One Big Idea to Remember\n\n> **An LLM is not a function call. It is a remote, rate-limited, probabilistic service. Failure is normal. Correctness is engineered.**\n\n---"
  },
  {
   "cell_type": "markdown",
   "id": "learning-objectives",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this module, you will be able to:\n",
    "\n",
    "1. Explain why LLMs must be treated as external services, not functions\n",
    "2. Build a reusable Python client class for LLM APIs\n",
    "3. Implement proper error handling with timeouts and retries\n",
    "4. Enforce structured JSON outputs and validate responses\n",
    "5. Write tests for LLM-integrated code without calling live models\n",
    "6. Implement logging for cost tracking and auditability\n",
    "7. Apply defensive programming patterns for non-deterministic systems\n",
    "8. Prepare code cleanly for RAG integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-intro",
   "metadata": {},
   "source": "---\n\n## Before You Start: LLM Gateway Configuration\n\nThis module requires access to an LLM API.\n\n### Recommended: JBChat Server (works in Colab)\n\n- URL: `https://jbchat.jonbowden.com.ngrok.app`\n- API key: Get from your instructor\n- Model: `llama3.1:8b`\n\n### Alternative: Local Ollama (advanced)\n\n**Important:** `localhost` does NOT work in Google Colab. In Colab, localhost refers to the Colab VM, not your local machine.\n\nIf you want to use your local Ollama from Colab, you must:\n1. Run Ollama locally: `ollama serve`\n2. Create a tunnel using Pinggy:\n   ```bash\n   ssh -p 443 -R0:localhost:11434 a.pinggy.io\n   ```\n3. Use the HTTPS URL from Pinggy (e.g., `https://xyz-abc.a.pinggy.io`)\n\nConfigure your choice in the cell below:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-config",
   "metadata": {},
   "outputs": [],
   "source": "# ===== LLM GATEWAY CONFIGURATION =====\n\n# ------ OPTION A: Local Ollama ------\n# NOTE: localhost does NOT work in Colab - it refers to the Colab VM, not your machine\n# You must use a tunnel (Pinggy) to expose your local Ollama to Colab\n# LLM_BASE_URL = \"https://your-pinggy-url.a.pinggy.io\"\n\n# ------ OPTION B: Server Gateway (JBChat) - Recommended for Colab ------\nLLM_BASE_URL = \"https://jbchat.jonbowden.com.ngrok.app\"\nLLM_API_KEY = \"instructor-supplied\"  # Get this from your instructor\nDEFAULT_MODEL = \"llama3.1:8b\"\n\nprint(f\"Configured: {LLM_BASE_URL}\")\nprint(f\"Model: {DEFAULT_MODEL}\")\nprint(f\"API Key: {'Set' if LLM_API_KEY else 'Not set'}\")"
  },
  {
   "cell_type": "markdown",
   "id": "group1-intro",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Group 1 — The Service Mindset\n",
    "\n",
    "Before we write code, we need to understand why LLM integration is fundamentally different from calling a local function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ce20e0",
   "metadata": {},
   "source": [
    "## 6.1 From Model Calls to Service Contracts\n",
    "\n",
    "When you call a local function, you expect:\n",
    "- Instant response\n",
    "- Deterministic output\n",
    "- No network failures\n",
    "- No rate limits\n",
    "\n",
    "When you call an LLM API, you face:\n",
    "\n",
    "| Challenge | Reality |\n",
    "|-----------|---------|\n",
    "| **Latency** | 1-30+ seconds per call |\n",
    "| **Availability** | Services go down, networks fail |\n",
    "| **Rate limits** | Too many calls = blocked |\n",
    "| **Non-determinism** | Same input can yield different outputs |\n",
    "| **Cost** | Every token costs money |\n",
    "| **Output format** | No guarantee of structure |\n",
    "\n",
    "### The Mindset Shift\n",
    "\n",
    "```\n",
    "WRONG MENTAL MODEL:           RIGHT MENTAL MODEL:\n",
    "\n",
    "result = llm(prompt)          try:\n",
    "use(result)                       result = llm_with_retry(prompt)\n",
    "                                  validated = parse_and_validate(result)\n",
    "                                  use(validated)\n",
    "                              except LLMError:\n",
    "                                  handle_gracefully()\n",
    "```\n",
    "\n",
    "### Enterprise Implications\n",
    "\n",
    "In production systems, you must design for:\n",
    "- **Graceful degradation** — What happens when the LLM is down?\n",
    "- **Timeout budgets** — How long can users wait?\n",
    "- **Fallback strategies** — Can you use cached responses?\n",
    "- **Cost controls** — How do you prevent runaway API bills?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5758ef8",
   "metadata": {},
   "source": [
    "## 6.2 Anatomy of an LLM API Request\n",
    "\n",
    "Every LLM API call is fundamentally **JSON over HTTP**. Understanding the structure helps you debug issues and optimize performance.\n",
    "\n",
    "### Request Components\n",
    "\n",
    "| Component | Purpose | Example |\n",
    "|-----------|---------|--------|\n",
    "| **Endpoint** | Where to send the request | `/api/chat`, `/v1/completions` |\n",
    "| **Headers** | Authentication, content type | `Authorization: Bearer xxx` |\n",
    "| **Model** | Which model to use | `phi3:mini`, `gpt-4` |\n",
    "| **Messages** | The conversation/prompt | `[{\"role\": \"user\", \"content\": \"...\"}]` |\n",
    "| **Temperature** | Randomness (0=deterministic) | `0.0` to `1.0` |\n",
    "| **Max tokens** | Output length limit | `256`, `1024` |\n",
    "\n",
    "### Standard Payload Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d958c94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A typical LLM API payload\n",
    "payload = {\n",
    "    \"model\": \"phi3:mini\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain inflation in one sentence.\"}\n",
    "    ],\n",
    "    \"temperature\": 0.0,  # Deterministic\n",
    "    \"max_tokens\": 100    # Limit response length\n",
    "}\n",
    "\n",
    "import json\n",
    "print(\"Request payload:\")\n",
    "print(json.dumps(payload, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "response-structure",
   "metadata": {},
   "source": [
    "### Response Structure\n",
    "\n",
    "The response also follows a standard structure:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"model\": \"phi3:mini\",\n",
    "  \"message\": {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": \"Inflation is the rate at which prices rise over time.\"\n",
    "  },\n",
    "  \"done\": true,\n",
    "  \"total_duration\": 1234567890\n",
    "}\n",
    "```\n",
    "\n",
    "Different providers have slightly different response formats, but the core pattern is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01092176",
   "metadata": {},
   "source": [
    "## 6.3 Configuration Discipline\n",
    "\n",
    "**Never hardcode configuration.** This is a fundamental principle for maintainable systems.\n",
    "\n",
    "### Why Configuration Matters\n",
    "\n",
    "| Hardcoded | Configurable |\n",
    "|-----------|-------------|\n",
    "| Change requires code edit | Change via environment |\n",
    "| Secrets in source code | Secrets in secure storage |\n",
    "| Same settings everywhere | Dev/staging/prod can differ |\n",
    "| Hard to test | Easy to mock |\n",
    "\n",
    "### The Configuration Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ee5dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Configuration from environment (with fallbacks)\n",
    "class LLMConfig:\n",
    "    \"\"\"Centralized LLM configuration.\"\"\"\n",
    "    \n",
    "    BASE_URL = os.getenv(\"LLM_BASE_URL\", LLM_BASE_URL)\n",
    "    API_KEY = os.getenv(\"LLM_API_KEY\", LLM_API_KEY)\n",
    "    MODEL = os.getenv(\"LLM_MODEL\", DEFAULT_MODEL)\n",
    "    \n",
    "    # Operational defaults\n",
    "    DEFAULT_TEMPERATURE = 0.0\n",
    "    DEFAULT_MAX_TOKENS = 256\n",
    "    DEFAULT_TIMEOUT = (5, 60)  # (connect, read) in seconds\n",
    "    MAX_RETRIES = 3\n",
    "\n",
    "print(f\"Config loaded:\")\n",
    "print(f\"  BASE_URL: {LLMConfig.BASE_URL}\")\n",
    "print(f\"  MODEL: {LLMConfig.MODEL}\")\n",
    "print(f\"  API_KEY: {'***' if LLMConfig.API_KEY else 'None (Ollama mode)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-best-practices",
   "metadata": {},
   "source": [
    "### Configuration Best Practices\n",
    "\n",
    "1. **Use environment variables** for anything that varies by environment\n",
    "2. **Provide sensible defaults** for development\n",
    "3. **Never commit secrets** to version control\n",
    "4. **Validate configuration** at startup, not at first use\n",
    "5. **Document required variables** in README or setup scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "group2-intro",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Group 2 — Building a Robust Client\n",
    "\n",
    "Now we build a reusable client class that encapsulates all the complexity of LLM communication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b06db2",
   "metadata": {},
   "source": [
    "## 6.4 Client Class Rationale\n",
    "\n",
    "Why wrap LLM calls in a class instead of simple functions?\n",
    "\n",
    "| Approach | Pros | Cons |\n",
    "|----------|------|------|\n",
    "| **Raw requests** | Simple, direct | Repeated code, no encapsulation |\n",
    "| **Functions** | Reusable | State management is awkward |\n",
    "| **Client class** | Encapsulated, testable, extensible | Slightly more setup |\n",
    "\n",
    "### What a Good Client Provides\n",
    "\n",
    "- **Encapsulation** — Hide HTTP details from business logic\n",
    "- **Configuration** — Centralized settings management\n",
    "- **Retry logic** — Automatic handling of transient failures\n",
    "- **Logging** — Consistent audit trail\n",
    "- **Testability** — Easy to mock for unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247da5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "from typing import Optional, Dict, Any, List\n",
    "\n",
    "class LLMClient:\n",
    "    \"\"\"\n",
    "    A robust client for LLM API interactions.\n",
    "    \n",
    "    Handles:\n",
    "    - Configuration management\n",
    "    - Request construction\n",
    "    - Error handling\n",
    "    - Retry logic\n",
    "    - Response parsing\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        base_url: str,\n",
    "        api_key: Optional[str] = None,\n",
    "        model: str = \"phi3:mini\",\n",
    "        timeout: tuple = (5, 60)\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the LLM client.\n",
    "        \n",
    "        Args:\n",
    "            base_url: API endpoint base URL\n",
    "            api_key: Optional API key (None for Ollama)\n",
    "            model: Model identifier\n",
    "            timeout: (connect_timeout, read_timeout) in seconds\n",
    "        \"\"\"\n",
    "        self.base_url = base_url.rstrip('/')\n",
    "        self.api_key = api_key\n",
    "        self.model = model\n",
    "        self.timeout = timeout\n",
    "        \n",
    "        # Detect mode based on API key\n",
    "        self._use_jbchat = api_key is not None\n",
    "    \n",
    "    def __repr__(self):\n",
    "        mode = \"JBChat\" if self._use_jbchat else \"Ollama\"\n",
    "        return f\"LLMClient(mode={mode}, model={self.model})\"\n",
    "\n",
    "# Create a client instance\n",
    "client = LLMClient(\n",
    "    base_url=LLMConfig.BASE_URL,\n",
    "    api_key=LLMConfig.API_KEY,\n",
    "    model=LLMConfig.MODEL\n",
    ")\n",
    "print(f\"Client created: {client}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39f4d09",
   "metadata": {},
   "source": [
    "## 6.5 Request Construction\n",
    "\n",
    "Building requests correctly is crucial. Different APIs have different formats, so we encapsulate this complexity.\n",
    "\n",
    "### Key Considerations\n",
    "\n",
    "| Aspect | Why It Matters |\n",
    "|--------|---------------|\n",
    "| **Headers** | Authentication, content negotiation |\n",
    "| **Endpoint** | Different APIs use different paths |\n",
    "| **Payload format** | Ollama vs OpenAI vs others differ |\n",
    "| **Timeout tuning** | Connect fast, allow long reads |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79073ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add request construction methods to our client\n",
    "\n",
    "class LLMClient(LLMClient):  # Extending previous definition\n",
    "    \n",
    "    def _build_headers(self) -> Dict[str, str]:\n",
    "        \"\"\"Build HTTP headers for the request.\"\"\"\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            # Bypass tunnel browser warnings\n",
    "            \"ngrok-skip-browser-warning\": \"true\",\n",
    "            \"Bypass-Tunnel-Reminder\": \"true\",\n",
    "        }\n",
    "        \n",
    "        if self.api_key:\n",
    "            headers[\"X-API-Key\"] = self.api_key\n",
    "        \n",
    "        return headers\n",
    "    \n",
    "    def _build_payload(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        temperature: float = 0.0,\n",
    "        max_tokens: int = 256,\n",
    "        system_prompt: Optional[str] = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Build the request payload.\"\"\"\n",
    "        \n",
    "        # Build messages list\n",
    "        messages = []\n",
    "        if system_prompt:\n",
    "            messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        \n",
    "        if self._use_jbchat:\n",
    "            # JBChat/OpenAI format\n",
    "            return {\n",
    "                \"model\": self.model,\n",
    "                \"messages\": messages,\n",
    "                \"temperature\": temperature,\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"stream\": False\n",
    "            }\n",
    "        else:\n",
    "            # Ollama format\n",
    "            return {\n",
    "                \"model\": self.model,\n",
    "                \"messages\": messages,\n",
    "                \"options\": {\"temperature\": temperature},\n",
    "                \"stream\": False\n",
    "            }\n",
    "    \n",
    "    def _get_endpoint(self) -> str:\n",
    "        \"\"\"Get the correct API endpoint.\"\"\"\n",
    "        if self._use_jbchat:\n",
    "            return f\"{self.base_url}/chat/direct\"\n",
    "        else:\n",
    "            return f\"{self.base_url}/api/chat\"\n",
    "\n",
    "# Recreate client with new methods\n",
    "client = LLMClient(\n",
    "    base_url=LLMConfig.BASE_URL,\n",
    "    api_key=LLMConfig.API_KEY,\n",
    "    model=LLMConfig.MODEL\n",
    ")\n",
    "\n",
    "# Show what a request looks like\n",
    "print(\"Endpoint:\", client._get_endpoint())\n",
    "print(\"\\nHeaders:\", json.dumps(client._build_headers(), indent=2))\n",
    "print(\"\\nPayload:\", json.dumps(client._build_payload(\"Hello\"), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe28df9",
   "metadata": {},
   "source": [
    "## 6.6 Making Safe API Calls\n",
    "\n",
    "The actual API call must handle many potential failures:\n",
    "\n",
    "| Failure Type | Cause | Handling |\n",
    "|--------------|-------|----------|\n",
    "| **Connection timeout** | Network issues, server down | Retry with backoff |\n",
    "| **Read timeout** | Slow response, overloaded server | Increase timeout or retry |\n",
    "| **HTTP 429** | Rate limited | Back off, then retry |\n",
    "| **HTTP 500** | Server error | Retry with backoff |\n",
    "| **HTTP 401/403** | Auth failure | Don't retry, fix config |\n",
    "| **Invalid JSON** | Malformed response | Log and raise |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeeebe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMClient(LLMClient):  # Extending again\n",
    "    \n",
    "    def chat(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        temperature: float = 0.0,\n",
    "        max_tokens: int = 256,\n",
    "        system_prompt: Optional[str] = None\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Send a chat request and return the response content.\n",
    "        \n",
    "        Args:\n",
    "            prompt: User message\n",
    "            temperature: Randomness (0.0 = deterministic)\n",
    "            max_tokens: Maximum response length\n",
    "            system_prompt: Optional system message\n",
    "            \n",
    "        Returns:\n",
    "            The assistant's response text\n",
    "            \n",
    "        Raises:\n",
    "            requests.exceptions.RequestException: On network/HTTP errors\n",
    "            ValueError: On invalid response format\n",
    "        \"\"\"\n",
    "        response = requests.post(\n",
    "            self._get_endpoint(),\n",
    "            headers=self._build_headers(),\n",
    "            json=self._build_payload(prompt, temperature, max_tokens, system_prompt),\n",
    "            timeout=self.timeout\n",
    "        )\n",
    "        \n",
    "        # Raise exception for HTTP errors (4xx, 5xx)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse response\n",
    "        data = response.json()\n",
    "        \n",
    "        # Extract content (handle different response formats)\n",
    "        if \"message\" in data and \"content\" in data[\"message\"]:\n",
    "            return data[\"message\"][\"content\"]\n",
    "        elif \"choices\" in data:  # OpenAI format\n",
    "            return data[\"choices\"][0][\"message\"][\"content\"]\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected response format: {data}\")\n",
    "\n",
    "# Recreate and test\n",
    "client = LLMClient(\n",
    "    base_url=LLMConfig.BASE_URL,\n",
    "    api_key=LLMConfig.API_KEY,\n",
    "    model=LLMConfig.MODEL\n",
    ")\n",
    "\n",
    "# Test the client\n",
    "try:\n",
    "    response = client.chat(\"Say 'API connected' in exactly two words.\")\n",
    "    print(f\"Response: {response}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"\\nMake sure your LLM server is running!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92eb7937",
   "metadata": {},
   "source": [
    "## 6.7 Failure as the Default Assumption\n",
    "\n",
    "In distributed systems, the question is not *if* things will fail, but *when* and *how often*.\n",
    "\n",
    "### Types of Failures\n",
    "\n",
    "| Type | Example | Frequency |\n",
    "|------|---------|----------|\n",
    "| **Transient** | Network hiccup, brief overload | Common (retry helps) |\n",
    "| **Persistent** | Server down, config error | Less common (retry won't help) |\n",
    "| **Partial** | Slow response, truncated output | Common (timeout/validate) |\n",
    "| **Silent** | Wrong answer, hallucination | Common (validation needed) |\n",
    "\n",
    "### The Defensive Mindset\n",
    "\n",
    "```python\n",
    "# WRONG: Assume success\n",
    "result = client.chat(prompt)\n",
    "use(result)\n",
    "\n",
    "# RIGHT: Assume failure, verify success\n",
    "try:\n",
    "    result = client.chat(prompt)\n",
    "    validated = validate(result)\n",
    "    use(validated)\n",
    "except TransientError:\n",
    "    retry()\n",
    "except PermanentError:\n",
    "    fallback()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "failure-demo",
   "metadata": {},
   "outputs": [],
   "source": "# Demonstrating common LLM API failure modes\n# NOTE: localhost refers to the notebook VM, not your machine\n\nimport requests\n\nprint(\"Common LLM API Failures:\")\nprint(\"=\" * 50)\n\n# 1. ConnectionError (Ollama unreachable - expected in Colab)\nprint(\"\\n1. ConnectionError (server not reachable):\")\ntry:\n    requests.post(\"http://localhost:11434/api/chat\", timeout=1)\nexcept Exception as e:\n    print(f\"   Exception type: {type(e).__name__}\")\n    print(f\"   This is expected - localhost doesn't work in Colab\")\n\n# 2. Timeout (slow inference / network)\nprint(\"\\n2. Timeout (server too slow):\")\ntry:\n    requests.get(\n        \"https://jbchat.jonbowden.com.ngrok.app/200?sleep=5000\",\n        timeout=0.1\n    )\nexcept Exception as e:\n    print(f\"   Exception type: {type(e).__name__}\")\n    print(f\"   This happens with slow models or overloaded servers\")\n\n# 3. HTTP 401 (real API auth failure)\nprint(\"\\n3. HTTP 401 (authentication failure):\")\ntry:\n    r = requests.post(\n        \"https://api.openai.com/v1/chat/completions\",\n        json={\"model\": \"gpt-4o-mini\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}]},\n        timeout=10\n    )\n    print(f\"   Status code: {r.status_code}\")\n    print(f\"   This is a REAL error from OpenAI - no API key provided\")\nexcept Exception as e:\n    print(f\"   Exception: {e}\")\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"Key insight: HTTP errors (401, 404, 429, 500) are SAFE to demonstrate\")\nprint(\"Transport failures may crash notebook kernels - use with caution\")"
  },
  {
   "cell_type": "markdown",
   "id": "c12eecea",
   "metadata": {},
   "source": [
    "## 6.8 Retry with Exponential Backoff\n",
    "\n",
    "**Exponential backoff** is the standard pattern for handling transient failures:\n",
    "\n",
    "1. Try the operation\n",
    "2. If it fails, wait a short time and retry\n",
    "3. If it fails again, wait longer (exponentially)\n",
    "4. After N retries, give up\n",
    "\n",
    "### Why Exponential?\n",
    "\n",
    "| Attempt | Wait Time | Cumulative |\n",
    "|---------|-----------|------------|\n",
    "| 1 | 0s | 0s |\n",
    "| 2 | 1s | 1s |\n",
    "| 3 | 2s | 3s |\n",
    "| 4 | 4s | 7s |\n",
    "| 5 | 8s | 15s |\n",
    "\n",
    "This gives the server time to recover while not waiting forever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c0ab00",
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nExponential backoff retry with visible output.\n\nWhat you'll see:\n- Attempt lines always print (so you know fn() is being called)\n- Failures print + backoff delay\n- Demo 1: fails twice then succeeds (shows retries + success)\n- Demo 2: always times out (shows retries + final exception)\n\"\"\"\n\nimport time\nfrom typing import Callable, TypeVar, Tuple\nimport requests\n\nT = TypeVar(\"T\")\n\n\ndef retry_with_backoff(\n    fn: Callable[[], T],\n    max_retries: int = 3,\n    base_delay: float = 1.0,\n    max_delay: float = 30.0,\n    retryable_exceptions: Tuple[type, ...] = (requests.exceptions.RequestException,),\n) -> T:\n    \"\"\"\n    Execute a function with exponential backoff retry.\n\n    Args:\n        fn: Function to execute (no arguments)\n        max_retries: Maximum number of retry attempts (retries after the first attempt)\n        base_delay: Initial delay in seconds\n        max_delay: Maximum delay between retries\n        retryable_exceptions: Exceptions that trigger retry\n\n    Returns:\n        Result of fn() on success\n\n    Raises:\n        The last exception if all retries fail\n    \"\"\"\n    last_exception: Exception | None = None\n    start = time.monotonic()\n\n    for attempt in range(max_retries + 1):\n        try:\n            elapsed = time.monotonic() - start\n            print(f\"[{elapsed:6.2f}s] Attempt {attempt + 1}/{max_retries + 1}\")\n            return fn()\n\n        except retryable_exceptions as e:\n            last_exception = e\n\n            # No sleep after last attempt\n            if attempt == max_retries:\n                break\n\n            delay = min(base_delay * (2 ** attempt), max_delay)\n            elapsed = time.monotonic() - start\n            print(f\"[{elapsed:6.2f}s]   Failed with {type(e).__name__}: {e}\")\n            print(f\"[{elapsed:6.2f}s]   Backing off for {delay:.1f}s...\\n\")\n            time.sleep(delay)\n\n    # If we get here, all attempts failed\n    assert last_exception is not None\n    raise last_exception\n\n\n# -------------------------\n# DEMOS (visible output)\n# -------------------------\n\nclass FailsTwiceThenSucceeds:\n    \"\"\"A callable that fails twice (Timeout) then returns success.\"\"\"\n    def __init__(self):\n        self.calls = 0\n\n    def __call__(self) -> str:\n        self.calls += 1\n        if self.calls < 3:\n            raise requests.exceptions.Timeout(\"temporary timeout\")\n        return \"OK (eventual success)\"\n\n\ndef always_timeout() -> str:\n    \"\"\"A callable that always times out.\"\"\"\n    raise requests.exceptions.Timeout(\"forced timeout (will never succeed)\")\n\n\nprint(\"=== Demo 1: transient failure (fails twice, then succeeds) ===\")\nresult = retry_with_backoff(\n    fn=FailsTwiceThenSucceeds(),\n    max_retries=3,\n    base_delay=1.0,\n    max_delay=10.0,\n    retryable_exceptions=(requests.exceptions.Timeout,),\n)\nprint(\"Result:\", result)\n\nprint(\"\\n=== Demo 2: permanent failure (always times out) ===\")\ntry:\n    retry_with_backoff(\n        fn=always_timeout,\n        max_retries=3,\n        base_delay=1.0,\n        max_delay=10.0,\n        retryable_exceptions=(requests.exceptions.Timeout,),\n    )\nexcept Exception as e:\n    print(\"Final failure (as expected):\", type(e).__name__, \"-\", e)"
  },
  {
   "cell_type": "markdown",
   "id": "group3-intro",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Group 3 — Structured Output and Validation\n",
    "\n",
    "Getting a response is only half the battle. The response must be **usable**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1668b00",
   "metadata": {},
   "source": [
    "## 6.9 The Necessity of Structured Output\n",
    "\n",
    "LLMs naturally produce free-form text. That's great for chatbots. It's terrible for software systems.\n",
    "\n",
    "### The Problem\n",
    "\n",
    "```python\n",
    "# You asked for a summary\n",
    "response = \"Here's a summary of the document. The main points are...\"\n",
    "\n",
    "# How do you extract the actual summary programmatically?\n",
    "# What if the format changes?\n",
    "# What if there's extra text?\n",
    "```\n",
    "\n",
    "### The Solution: JSON\n",
    "\n",
    "```python\n",
    "# Ask for JSON\n",
    "response = '{\"summary\": \"The main points are...\", \"confidence\": 0.85}'\n",
    "\n",
    "# Now you can parse and use it reliably\n",
    "data = json.loads(response)\n",
    "summary = data[\"summary\"]\n",
    "```\n",
    "\n",
    "### Why JSON?\n",
    "\n",
    "| Format | Pros | Cons |\n",
    "|--------|------|------|\n",
    "| **Free text** | Natural, flexible | Hard to parse, unreliable |\n",
    "| **JSON** | Structured, parseable, typed | LLM may not comply |\n",
    "| **XML** | Structured, handles nesting | Verbose, harder for LLMs |\n",
    "| **YAML** | Readable, structured | Whitespace-sensitive |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0746703c",
   "metadata": {},
   "source": [
    "## 6.10 JSON Enforcement in Prompts\n",
    "\n",
    "The key to getting JSON output is **explicit instruction** and **schema specification**.\n",
    "\n",
    "### Prompt Patterns for JSON\n",
    "\n",
    "| Pattern | Reliability |\n",
    "|---------|-------------|\n",
    "| \"Return JSON\" | Low |\n",
    "| \"Return ONLY valid JSON: {schema}\" | Medium |\n",
    "| \"Return ONLY valid JSON. No other text. Schema: {schema}\" | High |\n",
    "| System prompt + user prompt + schema | Highest |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a010b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template for JSON-enforced prompts\n",
    "\n",
    "def build_json_prompt(task: str, schema: dict, data: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Build a prompt that requests JSON output.\n",
    "    \n",
    "    Args:\n",
    "        task: What to do\n",
    "        schema: Expected JSON structure\n",
    "        data: Optional data to process\n",
    "    \"\"\"\n",
    "    schema_str = json.dumps(schema, indent=2)\n",
    "    \n",
    "    prompt = f\"\"\"Task: {task}\n",
    "\n",
    "Return ONLY valid JSON matching this schema:\n",
    "{schema_str}\n",
    "\n",
    "Rules:\n",
    "- Return ONLY the JSON object\n",
    "- No markdown, no explanations, no extra text\n",
    "- All fields are required\n",
    "\"\"\"\n",
    "    \n",
    "    if data:\n",
    "        prompt += f\"\\nData to process:\\n{data}\\n\"\n",
    "    \n",
    "    prompt += \"\\nJSON:\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Example: Sentiment analysis with structured output\n",
    "schema = {\n",
    "    \"sentiment\": \"positive | negative | neutral\",\n",
    "    \"confidence\": \"float between 0 and 1\",\n",
    "    \"key_phrases\": [\"list\", \"of\", \"phrases\"]\n",
    "}\n",
    "\n",
    "prompt = build_json_prompt(\n",
    "    task=\"Analyze the sentiment of the following text\",\n",
    "    schema=schema,\n",
    "    data=\"The product exceeded my expectations. Great value!\"\n",
    ")\n",
    "\n",
    "print(\"JSON-enforced prompt:\")\n",
    "print(\"=\" * 50)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "json-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test JSON output with the LLM\n",
    "try:\n",
    "    response = client.chat(prompt, temperature=0.0)\n",
    "    print(\"LLM Response:\")\n",
    "    print(response)\n",
    "    \n",
    "    # Try to parse it\n",
    "    parsed = json.loads(response)\n",
    "    print(\"\\nParsed successfully!\")\n",
    "    print(f\"Sentiment: {parsed.get('sentiment')}\")\n",
    "    print(f\"Confidence: {parsed.get('confidence')}\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"JSON parsing failed: {e}\")\n",
    "    print(\"The LLM did not return valid JSON.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0696f5a",
   "metadata": {},
   "source": [
    "## 6.11 Validation Before Use\n",
    "\n",
    "Even when the LLM returns valid JSON, you must validate it before using it in your application.\n",
    "\n",
    "### Validation Layers\n",
    "\n",
    "| Layer | Checks | Example |\n",
    "|-------|--------|--------|\n",
    "| **Syntax** | Is it valid JSON? | `json.loads()` |\n",
    "| **Schema** | Are required fields present? | Check keys exist |\n",
    "| **Types** | Are values the right type? | `isinstance()` |\n",
    "| **Values** | Are values in valid ranges? | Business logic |\n",
    "| **Semantic** | Does it make sense? | Domain validation |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5942f1af",
   "metadata": {},
   "outputs": [],
   "source": "import json\nfrom typing import Any\n\n\nclass ValidationError(Exception):\n    \"\"\"Raised when LLM output fails validation.\"\"\"\n    pass\n\n\ndef parse_json_response(text: str) -> dict:\n    \"\"\"\n    Parse JSON from LLM response.\n\n    Handles:\n    - Markdown code blocks (```json ... ```)\n    - Extra text before/after the JSON\n    - Leading/trailing whitespace\n    \"\"\"\n    text = text.strip()\n\n    # Extract JSON from markdown code block\n    if \"```json\" in text:\n        start = text.find(\"```json\") + 7\n        end = text.find(\"```\", start)\n        if end != -1:\n            text = text[start:end]\n    elif \"```\" in text:\n        start = text.find(\"```\") + 3\n        end = text.find(\"```\", start)\n        if end != -1:\n            text = text[start:end]\n\n    text = text.strip()\n\n    # Try to find JSON object boundaries if still failing\n    if not text.startswith(\"{\"):\n        start = text.find(\"{\")\n        if start != -1:\n            # Find matching closing brace\n            depth = 0\n            for i, ch in enumerate(text[start:], start):\n                if ch == \"{\":\n                    depth += 1\n                elif ch == \"}\":\n                    depth -= 1\n                    if depth == 0:\n                        text = text[start:i+1]\n                        break\n\n    try:\n        return json.loads(text)\n    except json.JSONDecodeError as e:\n        raise ValidationError(f\"Invalid JSON: {e}\")\n\n\ndef _type_name(t: Any) -> str:\n    \"\"\"Return a readable name for a type or tuple of types.\"\"\"\n    if isinstance(t, tuple):\n        return \" or \".join(x.__name__ for x in t)\n    return t.__name__\n\n\ndef validate_schema(\n    data: dict,\n    required_fields: list,\n    field_types: dict[str, Any] | None = None,\n) -> dict:\n    \"\"\"\n    Validate that data matches expected schema.\n    \"\"\"\n    # Check required fields\n    missing = [f for f in required_fields if f not in data]\n    if missing:\n        raise ValidationError(f\"Missing required fields: {missing}\")\n\n    # Check field types\n    if field_types:\n        for field, expected_type in field_types.items():\n            if field in data and not isinstance(data[field], expected_type):\n                actual = type(data[field]).__name__\n                expected = _type_name(expected_type)\n                raise ValidationError(\n                    f\"Field '{field}' has wrong type: expected {expected}, got {actual}\"\n                )\n\n    return data\n\n\n# -------------------------\n# Example validation demo\n# -------------------------\n\ntest_responses = [\n    '{\"sentiment\": \"positive\", \"confidence\": 0.9}',          # Valid\n    '```json\\n{\"sentiment\": \"positive\", \"confidence\": 0.9}\\n```',  # Markdown wrapped\n    '{\"sentiment\": \"positive\"}',                              # Missing confidence\n    '{\"sentiment\": \"positive\", \"confidence\": \"high\"}',       # Wrong type\n    'Here is the JSON: {\"sentiment\": \"positive\"}',            # Extra text before\n]\n\nprint(\"Validation Examples:\")\nprint(\"=\" * 50)\n\nfor response in test_responses:\n    print(f\"\\nInput: {response[:50]}...\")\n    try:\n        data = parse_json_response(response)\n        validated = validate_schema(\n            data,\n            required_fields=[\"sentiment\", \"confidence\"],\n            field_types={\"confidence\": (int, float)}\n        )\n        print(f\"  Valid: {validated}\")\n    except ValidationError as e:\n        print(f\"  Invalid: {e}\")"
  },
  {
   "cell_type": "markdown",
   "id": "group4-intro",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Group 4 — Testing and Determinism\n",
    "\n",
    "How do you test code that calls a non-deterministic external service?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfe5b5e",
   "metadata": {},
   "source": [
    "## 6.12 Testing Without Live Models\n",
    "\n",
    "**Never call live LLMs in unit tests.** This is a fundamental principle.\n",
    "\n",
    "### Why Not?\n",
    "\n",
    "| Problem | Consequence |\n",
    "|---------|-------------|\n",
    "| **Cost** | Every test run costs money |\n",
    "| **Speed** | Tests take seconds instead of milliseconds |\n",
    "| **Flakiness** | Non-deterministic = random failures |\n",
    "| **Availability** | Tests fail when API is down |\n",
    "| **Rate limits** | CI/CD can hit rate limits |\n",
    "\n",
    "### The Solution: Mocking\n",
    "\n",
    "Replace the LLM call with a predictable fake:\n",
    "\n",
    "```python\n",
    "# Production code\n",
    "result = client.chat(prompt)  # Calls real LLM\n",
    "\n",
    "# Test code\n",
    "client.chat = lambda p: '{\"result\": \"mocked\"}'  # Returns fixed value\n",
    "result = client.chat(prompt)  # Uses mock\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d4d9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the JSON parser (no LLM needed!)\n",
    "\n",
    "def test_parse_json_response():\n",
    "    \"\"\"Test JSON parsing without any LLM calls.\"\"\"\n",
    "    \n",
    "    # Test 1: Valid JSON\n",
    "    result = parse_json_response('{\"key\": \"value\"}')\n",
    "    assert result == {\"key\": \"value\"}, \"Basic JSON parsing failed\"\n",
    "    \n",
    "    # Test 2: With markdown code block\n",
    "    result = parse_json_response('```json\\n{\"key\": \"value\"}\\n```')\n",
    "    assert result == {\"key\": \"value\"}, \"Markdown cleanup failed\"\n",
    "    \n",
    "    # Test 3: With whitespace\n",
    "    result = parse_json_response('  \\n{\"key\": \"value\"}\\n  ')\n",
    "    assert result == {\"key\": \"value\"}, \"Whitespace handling failed\"\n",
    "    \n",
    "    # Test 4: Invalid JSON should raise\n",
    "    try:\n",
    "        parse_json_response('not json')\n",
    "        assert False, \"Should have raised ValidationError\"\n",
    "    except ValidationError:\n",
    "        pass  # Expected\n",
    "    \n",
    "    print(\"✓ All JSON parser tests passed!\")\n",
    "\n",
    "def test_validate_schema():\n",
    "    \"\"\"Test schema validation without any LLM calls.\"\"\"\n",
    "    \n",
    "    # Test 1: Valid data\n",
    "    data = {\"name\": \"Alice\", \"age\": 30}\n",
    "    result = validate_schema(data, required_fields=[\"name\", \"age\"])\n",
    "    assert result == data\n",
    "    \n",
    "    # Test 2: Missing field\n",
    "    try:\n",
    "        validate_schema({\"name\": \"Alice\"}, required_fields=[\"name\", \"age\"])\n",
    "        assert False, \"Should have raised ValidationError\"\n",
    "    except ValidationError as e:\n",
    "        assert \"age\" in str(e)\n",
    "    \n",
    "    # Test 3: Wrong type\n",
    "    try:\n",
    "        validate_schema(\n",
    "            {\"name\": \"Alice\", \"age\": \"thirty\"},\n",
    "            required_fields=[\"name\", \"age\"],\n",
    "            field_types={\"age\": int}\n",
    "        )\n",
    "        assert False, \"Should have raised ValidationError\"\n",
    "    except ValidationError as e:\n",
    "        assert \"wrong type\" in str(e)\n",
    "    \n",
    "    print(\"✓ All schema validation tests passed!\")\n",
    "\n",
    "# Run the tests\n",
    "test_parse_json_response()\n",
    "test_validate_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mock-client",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing with a mock client\n",
    "\n",
    "class MockLLMClient:\n",
    "    \"\"\"A mock client for testing LLM-integrated code.\"\"\"\n",
    "    \n",
    "    def __init__(self, responses: dict = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            responses: Dict mapping prompt substrings to responses\n",
    "        \"\"\"\n",
    "        self.responses = responses or {}\n",
    "        self.calls = []  # Track calls for verification\n",
    "    \n",
    "    def chat(self, prompt: str, **kwargs) -> str:\n",
    "        \"\"\"Return a mocked response based on the prompt.\"\"\"\n",
    "        self.calls.append({\"prompt\": prompt, **kwargs})\n",
    "        \n",
    "        # Find matching response\n",
    "        for key, response in self.responses.items():\n",
    "            if key.lower() in prompt.lower():\n",
    "                return response\n",
    "        \n",
    "        # Default response\n",
    "        return '{\"status\": \"mocked\"}'\n",
    "\n",
    "# Example: Testing a function that uses LLM\n",
    "def analyze_sentiment(client, text: str) -> str:\n",
    "    \"\"\"Analyze sentiment using LLM.\"\"\"\n",
    "    prompt = f\"Analyze sentiment: {text}\\nReturn JSON: {{\\\"sentiment\\\": \\\"...\\\"}}\"\n",
    "    response = client.chat(prompt)\n",
    "    data = parse_json_response(response)\n",
    "    return data.get(\"sentiment\", \"unknown\")\n",
    "\n",
    "# Test with mock\n",
    "def test_analyze_sentiment():\n",
    "    mock_client = MockLLMClient(responses={\n",
    "        \"great\": '{\"sentiment\": \"positive\"}',\n",
    "        \"terrible\": '{\"sentiment\": \"negative\"}',\n",
    "        \"okay\": '{\"sentiment\": \"neutral\"}',\n",
    "    })\n",
    "    \n",
    "    assert analyze_sentiment(mock_client, \"This is great!\") == \"positive\"\n",
    "    assert analyze_sentiment(mock_client, \"This is terrible!\") == \"negative\"\n",
    "    assert analyze_sentiment(mock_client, \"It's okay.\") == \"neutral\"\n",
    "    \n",
    "    # Verify the client was called correctly\n",
    "    assert len(mock_client.calls) == 3\n",
    "    \n",
    "    print(\"✓ Sentiment analysis tests passed (using mock)!\")\n",
    "\n",
    "test_analyze_sentiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444915f0",
   "metadata": {},
   "source": [
    "## 6.13 Achieving Determinism\n",
    "\n",
    "Even with mocks, you need strategies for making LLM behavior more predictable.\n",
    "\n",
    "### Techniques for Determinism\n",
    "\n",
    "| Technique | How | Effectiveness |\n",
    "|-----------|-----|---------------|\n",
    "| **Temperature 0** | Set `temperature=0.0` | High (but not perfect) |\n",
    "| **Seed parameter** | Some APIs support `seed=42` | Medium (provider-dependent) |\n",
    "| **Constrained output** | JSON mode, function calling | High |\n",
    "| **Caching** | Cache responses by prompt hash | Perfect (for repeated calls) |\n",
    "| **Mocking** | Replace with fake in tests | Perfect (for tests) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caching-demo",
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nLLM response caching with observable behaviour.\nNotebook-safe version using FakeLLMClient to demonstrate the pattern.\n\"\"\"\n\nimport time\nimport hashlib\nfrom typing import Dict\n\n\nclass FakeLLMClient:\n    \"\"\"Simulates an expensive LLM call with visible latency.\"\"\"\n    \n    def __init__(self):\n        self.calls = 0\n\n    def chat(self, prompt: str, temperature: float = 0.0, max_tokens: int = 256) -> str:\n        self.calls += 1\n        print(\"  [FakeLLM] generating response...\")\n        time.sleep(0.3)  # short but visible delay\n        return f\"LLM response #{self.calls} to: {prompt}\"\n\n\nclass CachingLLMClient:\n    \"\"\"LLM client with response caching for determinism.\"\"\"\n    \n    def __init__(self, client):\n        self.client = client\n        self.cache: Dict[str, str] = {}\n        self.cache_hits = 0\n        self.cache_misses = 0\n    \n    def _cache_key(self, prompt: str, temperature: float, max_tokens: int) -> str:\n        key_data = f\"{prompt}:{temperature}:{max_tokens}\"\n        return hashlib.sha256(key_data.encode()).hexdigest()[:16]\n    \n    def chat(self, prompt: str, temperature: float = 0.0, max_tokens: int = 256) -> str:\n        key = self._cache_key(prompt, temperature, max_tokens)\n        \n        if key in self.cache:\n            self.cache_hits += 1\n            return self.cache[key]\n        \n        self.cache_misses += 1\n        response = self.client.chat(prompt, temperature, max_tokens)\n        self.cache[key] = response\n        return response\n    \n    def stats(self) -> dict:\n        total = self.cache_hits + self.cache_misses\n        hit_rate = self.cache_hits / total if total else 0\n        return {\n            \"hits\": self.cache_hits,\n            \"misses\": self.cache_misses,\n            \"hit_rate\": f\"{hit_rate:.1%}\",\n            \"cached_responses\": len(self.cache),\n            \"underlying_llm_calls\": self.client.calls,\n        }\n\n\n# Demonstration\nllm = FakeLLMClient()\ncached_llm = CachingLLMClient(llm)\n\nprompt = \"Explain caching in one sentence.\"\n\nprint(\"=== First call (cache miss, slow) ===\")\nstart = time.monotonic()\nprint(\"Response:\", cached_llm.chat(prompt))\nprint(f\"Elapsed: {time.monotonic() - start:.2f}s\")\n\nprint(\"\\n=== Second call (cache hit, fast) ===\")\nstart = time.monotonic()\nprint(\"Response:\", cached_llm.chat(prompt))\nprint(f\"Elapsed: {time.monotonic() - start:.2f}s\")\n\nprint(\"\\n=== Third call (different prompt, miss again) ===\")\nstart = time.monotonic()\nprint(\"Response:\", cached_llm.chat(prompt + \" Again.\"))\nprint(f\"Elapsed: {time.monotonic() - start:.2f}s\")\n\nprint(\"\\n=== Cache statistics ===\")\nprint(cached_llm.stats())"
  },
  {
   "cell_type": "markdown",
   "id": "group5-intro",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Group 5 — Production Concerns\n",
    "\n",
    "Beyond correctness, production systems need logging, cost control, and auditability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee8c61e",
   "metadata": {},
   "source": [
    "## 6.14 Cost and Audit Logging\n",
    "\n",
    "Every LLM call should be logged for:\n",
    "\n",
    "| Purpose | What to Log |\n",
    "|---------|------------|\n",
    "| **Cost tracking** | Tokens used, model, timestamp |\n",
    "| **Debugging** | Prompt, response, errors |\n",
    "| **Compliance** | User ID, request ID, inputs/outputs |\n",
    "| **Performance** | Latency, retry count |\n",
    "| **Security** | Suspicious patterns, PII detection |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2d28ab",
   "metadata": {},
   "outputs": [],
   "source": "import time\nimport uuid\nfrom datetime import datetime\nfrom dataclasses import dataclass\nfrom typing import Optional, List\n\n\n@dataclass\nclass LLMCallLog:\n    \"\"\"Structured log entry for an LLM call.\"\"\"\n    request_id: str\n    timestamp: str\n    model: str\n    prompt_preview: str\n    response_preview: str\n    latency_ms: float\n    success: bool\n    error: Optional[str] = None\n    \n    def to_dict(self) -> dict:\n        return {\n            \"request_id\": self.request_id,\n            \"timestamp\": self.timestamp,\n            \"model\": self.model,\n            \"prompt_preview\": self.prompt_preview,\n            \"response_preview\": self.response_preview,\n            \"latency_ms\": self.latency_ms,\n            \"success\": self.success,\n            \"error\": self.error\n        }\n\n\nclass FakeLLMClient:\n    \"\"\"Simulates an LLM with latency and one forced failure.\"\"\"\n    \n    def __init__(self):\n        self.model = \"fake-llm-v1\"\n        self.calls = 0\n    \n    def chat(self, prompt: str, **kwargs) -> str:\n        self.calls += 1\n        time.sleep(0.25)  # visible latency\n        \n        if \"fail\" in prompt.lower():\n            raise RuntimeError(\"Simulated LLM failure\")\n        \n        return f\"Response #{self.calls} to: {prompt}\"\n\n\nclass LoggingLLMClient:\n    \"\"\"LLM client with structured logging.\"\"\"\n    \n    def __init__(self, client, preview_length: int = 100):\n        self.client = client\n        self.preview_length = preview_length\n        self.logs: List[LLMCallLog] = []\n    \n    def chat(self, prompt: str, **kwargs) -> str:\n        request_id = str(uuid.uuid4())[:8]\n        start_time = datetime.now()\n        \n        try:\n            response = self.client.chat(prompt, **kwargs)\n            latency = (datetime.now() - start_time).total_seconds() * 1000\n            \n            log = LLMCallLog(\n                request_id=request_id,\n                timestamp=start_time.isoformat(),\n                model=self.client.model,\n                prompt_preview=prompt[:self.preview_length],\n                response_preview=response[:self.preview_length],\n                latency_ms=round(latency, 2),\n                success=True\n            )\n            self.logs.append(log)\n            return response\n            \n        except Exception as e:\n            latency = (datetime.now() - start_time).total_seconds() * 1000\n            \n            log = LLMCallLog(\n                request_id=request_id,\n                timestamp=start_time.isoformat(),\n                model=self.client.model,\n                prompt_preview=prompt[:self.preview_length],\n                response_preview=\"\",\n                latency_ms=round(latency, 2),\n                success=False,\n                error=str(e)\n            )\n            self.logs.append(log)\n            raise\n    \n    def get_logs(self) -> list[dict]:\n        return [log.to_dict() for log in self.logs]\n    \n    def summary(self) -> dict:\n        if not self.logs:\n            return {\"total_calls\": 0}\n        \n        successful = [l for l in self.logs if l.success]\n        failed = [l for l in self.logs if not l.success]\n        latencies = [l.latency_ms for l in successful]\n        \n        return {\n            \"total_calls\": len(self.logs),\n            \"successful\": len(successful),\n            \"failed\": len(failed),\n            \"avg_latency_ms\": round(sum(latencies) / len(latencies), 2) if latencies else 0,\n            \"max_latency_ms\": round(max(latencies), 2) if latencies else 0\n        }\n\n\n# Demonstration\nllm = FakeLLMClient()\nlogging_client = LoggingLLMClient(llm)\n\nprint(\"=== Successful call ===\")\nprint(logging_client.chat(\"Explain logging in one sentence.\"))\n\nprint(\"\\n=== Another successful call ===\")\nprint(logging_client.chat(\"Why is structured logging useful?\"))\n\nprint(\"\\n=== Failing call ===\")\ntry:\n    logging_client.chat(\"This should FAIL\")\nexcept Exception as e:\n    print(\"Caught error:\", e)\n\nprint(\"\\n=== Log entries ===\")\nfor entry in logging_client.get_logs():\n    print(entry)\n\nprint(\"\\n=== Summary ===\")\nprint(logging_client.summary())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logging-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate logging (using mock to avoid real API calls)\n",
    "\n",
    "# Create a mock for demonstration\n",
    "mock = MockLLMClient(responses={\n",
    "    \"hello\": '{\"greeting\": \"Hello!\"}',\n",
    "    \"weather\": '{\"forecast\": \"Sunny\"}',\n",
    "})\n",
    "mock.model = \"mock-model\"  # Add model attribute\n",
    "\n",
    "# Wrap with logging\n",
    "logged_client = LoggingLLMClient(mock)\n",
    "\n",
    "# Make some calls\n",
    "logged_client.chat(\"Hello, how are you?\")\n",
    "logged_client.chat(\"What's the weather like?\")\n",
    "logged_client.chat(\"Hello again!\")\n",
    "\n",
    "# View logs\n",
    "print(\"Call Logs:\")\n",
    "print(\"=\" * 60)\n",
    "for log in logged_client.get_logs():\n",
    "    print(f\"[{log['request_id']}] {log['prompt_preview'][:30]}... -> {log['latency_ms']}ms\")\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "print(logged_client.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1975fd79",
   "metadata": {},
   "source": [
    "## 6.15 Preparing for RAG\n",
    "\n",
    "Everything in this module prepares you for building RAG (Retrieval-Augmented Generation) systems.\n",
    "\n",
    "### The RAG Pattern (Recap from Module 5)\n",
    "\n",
    "```\n",
    "User Question\n",
    "     ↓\n",
    "Embed Question (Module 5)\n",
    "     ↓\n",
    "Vector Search (Module 5)\n",
    "     ↓\n",
    "Build Prompt with Context (Module 6)\n",
    "     ↓\n",
    "Call LLM with Retry (Module 6)\n",
    "     ↓\n",
    "Validate Response (Module 6)\n",
    "     ↓\n",
    "Return to User\n",
    "```\n",
    "\n",
    "### What You Now Know\n",
    "\n",
    "| Module 5 Skills | Module 6 Skills |\n",
    "|-----------------|----------------|\n",
    "| Generate embeddings | Build robust API clients |\n",
    "| Semantic search | Handle failures gracefully |\n",
    "| Vector databases | Enforce structured output |\n",
    "| Chunking strategies | Validate before using |\n",
    "| Retrieval evaluation | Test without live models |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Combining Modules 5 and 6, you can now:\n",
    "1. Index documents with embeddings\n",
    "2. Retrieve relevant context for any question\n",
    "3. Build prompts that ground LLM answers in facts\n",
    "4. Call LLMs reliably with proper error handling\n",
    "5. Validate and log everything for compliance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rag-ready",
   "metadata": {},
   "outputs": [],
   "source": "# A complete RAG-ready prompt builder\n# This pattern will be used in Module 7 for full RAG pipelines\n\ndef build_rag_prompt(question: str, context_docs: list[str]) -> str:\n    \"\"\"\n    Build a RAG prompt with retrieved context.\n    \n    Args:\n        question: User's question\n        context_docs: Retrieved relevant documents\n    \"\"\"\n    context = \"\\n\\n\".join([f\"[{i+1}] {doc}\" for i, doc in enumerate(context_docs)])\n    \n    return f\"\"\"Answer the question based ONLY on the provided context.\nIf the context doesn't contain enough information, say \"I don't have enough information.\"\n\nContext:\n{context}\n\nQuestion: {question}\n\nProvide your answer in JSON format:\n{{\n    \"answer\": \"your answer here\",\n    \"sources_used\": [1, 2],\n    \"confidence\": \"high|medium|low\"\n}}\n\nJSON:\"\"\"\n\n# Example with sample context\ncontext = [\n    \"The company's Q3 revenue was $4.2 billion, up 15% YoY.\",\n    \"Operating margin improved to 23% from 21% last year.\",\n    \"The CEO announced plans to expand into Asian markets.\"\n]\n\nprompt = build_rag_prompt(\n    question=\"What was the company's revenue in Q3?\",\n    context_docs=context\n)\n\nprint(\"RAG Prompt Example:\")\nprint(\"=\" * 60)\nprint(prompt)\n\n# To use with the actual LLM (requires instructor-supplied API key):\n# response = client.chat(prompt, temperature=0.0)\n# print(\"\\nLLM Response:\", response)"
  },
  {
   "cell_type": "markdown",
   "id": "f2c07971",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Module Summary\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "| Concept | What It Means |\n",
    "|---------|---------------|\n",
    "| **Service mindset** | LLMs are external services, not functions |\n",
    "| **Defensive programming** | Assume failure, verify success |\n",
    "| **Exponential backoff** | Wait longer between each retry |\n",
    "| **Structured output** | Request JSON, validate before use |\n",
    "| **Mocking** | Test without live LLM calls |\n",
    "| **Audit logging** | Track every call for cost/compliance |\n",
    "\n",
    "## The Production LLM Client Checklist\n",
    "\n",
    "- [ ] Configuration externalized (no hardcoded secrets)\n",
    "- [ ] Proper timeouts set (connect and read)\n",
    "- [ ] Retry logic with exponential backoff\n",
    "- [ ] Structured output requested (JSON)\n",
    "- [ ] Response validation before use\n",
    "- [ ] Error handling for all failure modes\n",
    "- [ ] Logging for debugging and audit\n",
    "- [ ] Tests that don't call live APIs\n",
    "\n",
    "## Enterprise Implications\n",
    "\n",
    "| Concern | Solution |\n",
    "|---------|----------|\n",
    "| Cost control | Logging, caching, token limits |\n",
    "| Reliability | Retries, fallbacks, timeouts |\n",
    "| Compliance | Audit logs, input/output validation |\n",
    "| Security | No hardcoded secrets, PII detection |\n",
    "| Testability | Mocking, dependency injection |\n",
    "\n",
    "---\n",
    "\n",
    "## What's Next\n",
    "\n",
    "1. **Quiz** — Test your understanding of LLM API patterns\n",
    "2. **Assessment** — Build and test your own LLM client\n",
    "3. **Module 7** — Putting it all together with a complete RAG system"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
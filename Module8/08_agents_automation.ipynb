{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "cells": [
  {
   "id": "a1b2c3d4",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 8 — Agents & Automation\n",
    "\n",
    "**Controlled Decision-Making with LLMs**\n",
    "\n",
    "---\n",
    "\n",
    "## What This Module Covers\n",
    "\n",
    "| Group | Topic | Key Skill |\n",
    "|-------|-------|-----------|\n",
    "| 1 | Agent Fundamentals | Understand what makes a system an agent vs a pipeline |\n",
    "| 2 | Building a Triage Agent | Implement a complete bounded agent with real tools |\n",
    "| 3 | Control & Guardrails | Apply bounded loops and policy enforcement |\n",
    "| 4 | Failure Modes & Auditability | Handle failures and build audit trails |\n",
    "| 5 | Automation vs Autonomy | Know when agents are appropriate and when they are not |\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this module, you will be able to:\n",
    "\n",
    "1. **Explain** the difference between a fixed pipeline and an LLM-driven agent\n",
    "2. **Implement** the propose-validate-execute pattern for safe agent design\n",
    "3. **Define** explicit allowed-action sets and enforce them in code\n",
    "4. **Build** a triage agent that classifies, retrieves, or refuses based on input\n",
    "5. **Apply** guardrails including bounded loops, input validation, and audit logging\n",
    "6. **Evaluate** when agents are appropriate vs when a simple pipeline is safer\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "This module builds directly on:\n",
    "\n",
    "| Module | Concepts Used Here |\n",
    "|--------|-------------------|\n",
    "| Module 3 | LLM behavior, hallucination risk, structured output |\n",
    "| Module 6 | LLM API clients, JSON parsing, retry logic |\n",
    "| Module 7 | RAG retrieval pattern, grounded answers, refusal as a feature |\n",
    "\n",
    "**Module 8 is where retrieval meets decision-making.**"
   ]
  },
  {
   "id": "e5f6a7b8",
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Setup\n\nNo external dependencies are needed for this module. All LLM calls are mocked using keyword-based logic defined in **`mock_toolkit.py`**, so the notebook runs deterministically in any environment."
  },
  {
   "id": "c9d0e1f2",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport os\n\n# Install mock_toolkit package if not available locally (e.g. running in Colab)\ntry:\n    import mock_toolkit\nexcept ImportError:\n    !pip install -q git+https://github.com/jonbowden/notebook-repository.git#subdirectory=packages/codevision-mock-toolkit\n\nfrom mock_toolkit import (\n    MockLLMClient, ALLOWED_ACTIONS, REFUSAL_TEXT, KNOWLEDGE_BASE,\n    classify_only, retrieve_and_answer, refuse, decide_action,\n)\n\nllm_client = MockLLMClient()\n\nprint(\"Setup complete.\")\nprint(\"All LLM calls in this module use MockLLMClient from mock_toolkit.\")\nprint(\"No API key or network connection is required.\")"
  },
  {
   "id": "a3b4c5d6",
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Group 1: Agent Fundamentals\n\n**What makes a system an agent vs a pipeline**\n\n| Section | Topic |\n|---------|-------|\n| 8.1 | Pipelines vs Agents |\n| 8.2 | The Agent Pattern |"
  },
  {
   "id": "e7f8a9b0",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8.1 Pipelines vs Agents\n",
    "\n",
    "Most LLM systems are **pipelines**: a fixed sequence of steps executed in order. The code determines what happens and when. The LLM only fills in content at predetermined slots.\n",
    "\n",
    "An **agent** is different. The LLM itself chooses which action to take next. The code provides a menu of allowed actions; the LLM picks from that menu based on context.\n",
    "\n",
    "### Comparison\n",
    "\n",
    "| Property | Pipeline | Agent |\n",
    "|----------|----------|-------|\n",
    "| Control flow | Fixed steps, hard-coded order | LLM chooses the next action |\n",
    "| Determinism | Same input always runs same steps | Same input may choose different actions |\n",
    "| Decision-making | None — code decides everything | LLM decides which tool to use |\n",
    "| Predictability | High | Lower (bounded by allowed actions) |\n",
    "| Use case | ETL, formatting, summarisation | Triage, routing, multi-step reasoning |\n",
    "\n",
    "### The Agent Loop\n",
    "\n",
    "```\n",
    "+-------------------+\n",
    "|   User Input      |\n",
    "+--------+----------+\n",
    "         |\n",
    "         v\n",
    "+--------+----------+\n",
    "|  LLM Proposes     |  <--- LLM reads input and picks an action\n",
    "|  Action           |\n",
    "+--------+----------+\n",
    "         |\n",
    "         v\n",
    "+--------+----------+\n",
    "|  Code Validates   |  <--- Code checks action is in ALLOWED_ACTIONS\n",
    "|  Action           |\n",
    "+--------+----------+\n",
    "         |\n",
    "         v\n",
    "+--------+----------+\n",
    "|  System Executes  |  <--- Code runs the chosen tool\n",
    "|  Tool             |\n",
    "+--------+----------+\n",
    "         |\n",
    "         v\n",
    "+-------------------+\n",
    "|   Result / Stop   |\n",
    "+-------------------+\n",
    "```\n",
    "\n",
    "> **Key Insight:** The LLM never executes code directly. It proposes an action label. The code decides whether to honour that proposal."
   ]
  },
  {
   "id": "c1d2e3f4",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Demonstrate the difference between a pipeline and an agent\n# These use simplified tool functions (full versions built in 8.3)\n\ndef demo_classify(text):\n    \"\"\"Classify topic using keyword matching.\"\"\"\n    text_lower = text.lower()\n    if any(kw in text_lower for kw in [\"rate\", \"inflation\", \"monetary\"]):\n        return \"Interest Rates\"\n    if any(kw in text_lower for kw in [\"loan\", \"mortgage\", \"lending\"]):\n        return \"Lending & Credit\"\n    return \"Out of Domain\"\n\ndef demo_retrieve(text):\n    \"\"\"Retrieve a grounded answer from a small knowledge base.\"\"\"\n    text_lower = text.lower()\n    if \"rate\" in text_lower or \"inflation\" in text_lower:\n        return \"The central bank raised rates by 25bp to combat inflation.\"\n    if \"loan\" in text_lower or \"lending\" in text_lower:\n        return \"Lending standards tightened due to higher funding costs.\"\n    return \"No relevant documents found.\"\n\n\n# --- PIPELINE: always runs all three steps, regardless of input ---\ndef pipeline(user_input):\n    topic   = demo_classify(user_input)\n    context = demo_retrieve(user_input)\n    answer  = f\"Based on retrieved context: {context}\"\n\n    return (f\"[classify]   {topic}\\n\"\n            f\"           [retrieve]   {context}\\n\"\n            f\"           [generate]   {answer}\")\n\n\n# --- AGENT: picks only the step the input actually needs ---\ndef agent(user_input):\n    text_lower = user_input.lower()\n    if any(kw in text_lower for kw in [\"rate\", \"inflation\", \"monetary\"]):\n        return f\"[retrieve_and_answer]  {demo_retrieve(user_input)}\"\n    if any(kw in text_lower for kw in [\"classify\", \"label\", \"topic\"]):\n        return f\"[classify_only]  {demo_classify(user_input)}\"\n    return \"[refuse]  I am not authorised to act on this request.\"\n\n\ninputs = [\n    \"Why did the central bank raise rates?\",     # needs retrieval\n    \"Classify this sentence about loans.\",        # only needs classification\n    \"Delete all customer records.\",               # should be refused\n]\n\nprint(\"=== Pipeline: always runs classify → retrieve → generate ===\")\nfor inp in inputs:\n    print(f\"  Input:  {inp}\")\n    print(f\"  Output: {pipeline(inp)}\")\n    print()\n\nprint(\"=== Agent: picks the right action for each input ===\")\nfor inp in inputs:\n    print(f\"  Input:  {inp}\")\n    print(f\"  Output: {agent(inp)}\")\n    print()\n\nprint(\"Notice:\")\nprint(\"  - 'Classify this sentence about loans' only needed a label,\")\nprint(\"    but the pipeline also retrieved and generated a full answer.\")\nprint(\"  - 'Delete all customer records' should be refused, but the\")\nprint(\"    pipeline classified, searched, and generated anyway.\")\nprint(\"  - The agent routes each input to only the step it needs.\")"
  },
  {
   "id": "a5b6c7d8",
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 8.2 The Agent Pattern\n\nEvery safe agent follows the same three-step discipline:\n\n```\nLLM Proposes  -->  Code Validates  -->  System Executes\n```\n\n### Why This Separation Matters\n\n| Step | Who Does It | Why |\n|------|-------------|-----|\n| **Propose** | LLM | LLMs are good at understanding intent and choosing labels |\n| **Validate** | Code | Code enforces hard constraints the LLM cannot override |\n| **Execute** | System | Side-effects (DB writes, API calls) only happen after validation |\n\nThis pattern ensures that:\n- The LLM cannot call arbitrary functions\n- Unexpected LLM output does not trigger unintended behaviour\n- Every executed action was explicitly approved by the code\n\n### Allowed Actions\n\nAn agent's power is defined by its **allowed action set**. Keeping this set small and explicit is the primary safety mechanism. If an agent can call any function, a confused LLM response could trigger irreversible operations, exfiltrate data, or cause cascading failures.\n\n| Safe Actions | Unsafe Actions |\n|-------------|----------------|\n| Read-only retrieval | Delete records |\n| Classify text | Send emails to customers |\n| Refuse the request | Execute financial transactions |\n| Summarise a document | Modify user permissions |\n\n### Refusal Is a Feature\n\nRefusal is not a failure — it is a **deliberate, safe action**. An agent that refuses clearly is more trustworthy than one that always attempts an answer.\n\n| Without Refusal | With Refusal |\n|----------------|--------------|\n| Agent attempts unsafe actions | Agent declines clearly |\n| Users may receive hallucinated output | Users receive honest limitation |\n| No audit signal for off-topic requests | Refusals logged for review |\n\n> **Key Insight:** The LLM is a decision engine, not an executor. Code is the gatekeeper. Every action must be explicitly allowed, and refusal is always one of those actions."
  },
  {
   "id": "e9f0a1b2",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the three-step pattern explicitly\n",
    "\n",
    "ALLOWED_ACTIONS = {\"retrieve_and_answer\", \"classify_only\", \"refuse\"}\n",
    "\n",
    "def step1_propose(user_input):\n",
    "    \"\"\"LLM proposes an action (mocked as keyword matching).\"\"\"\n",
    "    if \"rate\" in user_input.lower():\n",
    "        return \"retrieve_and_answer\"\n",
    "    if \"classify\" in user_input.lower():\n",
    "        return \"classify_only\"\n",
    "    # Simulate a malformed LLM response\n",
    "    if \"delete\" in user_input.lower():\n",
    "        return \"delete_records\"   # NOT in ALLOWED_ACTIONS\n",
    "    return \"refuse\"\n",
    "\n",
    "def step2_validate(proposed_action):\n",
    "    \"\"\"Code validates the proposed action against the allowed set.\"\"\"\n",
    "    if proposed_action in ALLOWED_ACTIONS:\n",
    "        return proposed_action\n",
    "    return \"refuse\"   # fallback: refuse anything not explicitly allowed\n",
    "\n",
    "def step3_execute(validated_action, user_input):\n",
    "    \"\"\"System executes the validated action.\"\"\"\n",
    "    if validated_action == \"retrieve_and_answer\":\n",
    "        return \"Retrieved: The central bank raised rates to combat inflation.\"\n",
    "    if validated_action == \"classify_only\":\n",
    "        return \"Classified: Topic = Interest Rates\"\n",
    "    return \"Refused: I am not authorised to act on this request.\"\n",
    "\n",
    "test_inputs = [\n",
    "    \"Why did the central bank raise rates?\",\n",
    "    \"Classify this financial sentence.\",\n",
    "    \"Delete all customer records.\",\n",
    "]\n",
    "\n",
    "print(\"=== Three-Step Agent Pattern ===\")\n",
    "for inp in test_inputs:\n",
    "    proposal  = step1_propose(inp)\n",
    "    validated = step2_validate(proposal)\n",
    "    result    = step3_execute(validated, inp)\n",
    "\n",
    "    print(f\"Input     : {inp}\")\n",
    "    print(f\"Proposed  : {proposal}\")\n",
    "    print(f\"Validated : {validated}\")\n",
    "    print(f\"Result    : {result}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee78yfqjh5",
   "source": "---\n\n# Group 2: Building & Running the Triage Agent\n\n**Build the mock toolkit, assemble the agent, and run it**\n\n| Section | Topic |\n|---------|-------|\n| 8.3 | The Mock Toolkit |\n| 8.4 | Assembling the Triage Agent |\n| 8.5 | Running the Agent |\n\n---\n\n## 8.3 The Mock Toolkit\n\nThe agent needs three components: a mock LLM client, three tools, and a decision function. These are defined in **`mock_toolkit.py`** — open that file to review the source code.\n\n### Why Mock?\n\n| Benefit | Explanation |\n|---------|-------------|\n| No API key needed | Notebook runs in any Colab environment |\n| Deterministic output | Same input always produces same result — essential for grading |\n| Teaches the interface | The mock has the same `.chat()` method a real client would have |\n| Swappable | Replace `MockLLMClient` with a real client later — zero agent code changes |\n\n### What's Inside `mock_toolkit.py`\n\n| Component | Purpose |\n|-----------|---------|\n| `MockLLMClient` | Keyword-based mock with `.chat()` interface — swap for a real LLM client in production |\n| `ALLOWED_ACTIONS` | The set of actions the agent is permitted to take |\n| `classify_only(text)` | Returns a topic label or \"Out of Domain\" |\n| `retrieve_and_answer(question)` | Returns a grounded answer from a small knowledge base |\n| `refuse()` | Returns the standard refusal message |\n| `decide_action(llm_client, input)` | Asks the LLM, parses JSON, validates against ALLOWED_ACTIONS |\n\n> **Key Insight:** Build your tools once, test them independently, then wire them into the agent. The agent itself should contain no domain logic — it only routes.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ydbphf4xzj",
   "source": "# Quick verification of every component (imported in Setup cell above)\n\nprint(\"Toolkit loaded from mock_toolkit.py\\n\")\n\nprint(\"MockLLMClient:\")\nprint(f'  .chat(\"rates\")    → {llm_client.chat(\"rates\")}')\nprint(f'  .chat(\"classify\") → {llm_client.chat(\"classify\")}')\nprint(f'  .chat(\"hello\")    → {llm_client.chat(\"hello\")}')\nprint()\n\nprint(\"Tools:\")\nprint(f\"  classify_only('Bank raised rates')    → {classify_only('Bank raised rates')}\")\nprint(f\"  classify_only('Football is great')    → {classify_only('Football is great')}\")\nprint(f\"  retrieve_and_answer('Why raise rates?') → {retrieve_and_answer('Why raise rates?')}\")\nprint(f\"  refuse() → {refuse()}\")\nprint()\n\nprint(\"Decision function:\")\nfor q in [\"Why did rates rise?\", \"Classify this loan.\", \"Delete everything.\", \"Approve a loan.\"]:\n    print(f\"  decide_action('{q}') → {decide_action(llm_client, q)}\")\nprint()\n\nprint(f\"Allowed actions: {sorted(ALLOWED_ACTIONS)}\")\nprint(f\"Knowledge base entries: {len(KNOWLEDGE_BASE)}\")\nprint(\"\\nReview the source: open mock_toolkit.py\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "id": "e7f8a9b0c",
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 8.4 Assembling the Triage Agent\n\nThe triage agent brings together the decision step and the three tools into a single function. The pattern is called **dispatch**: the agent dispatches the user input to the appropriate tool based on the LLM's decision.\n\n### Dispatch Pattern\n\n```\nuser_input\n     |\n     v\n decide_action()           <-- LLM picks from ALLOWED_ACTIONS\n     |\n     +-- retrieve_and_answer --> grounded answer from knowledge base\n     +-- classify_only       --> topic label\n     +-- refuse              --> standard refusal message\n```\n\nEach branch is a separate, independently testable function. The agent itself contains no domain logic — it only routes."
  },
  {
   "id": "c1d2e3f4a",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triage_agent(user_input):\n",
    "    \"\"\"Route user input to the appropriate tool based on LLM decision.\n",
    "    \n",
    "    This is the core agent function. It:\n",
    "    1. Asks the LLM to propose an action\n",
    "    2. Validates the action against ALLOWED_ACTIONS\n",
    "    3. Dispatches to the correct tool\n",
    "    \"\"\"\n",
    "    action = decide_action(llm_client, user_input)\n",
    "\n",
    "    if action == \"retrieve_and_answer\":\n",
    "        return retrieve_and_answer(user_input)\n",
    "    if action == \"classify_only\":\n",
    "        return classify_only(user_input)\n",
    "    return refuse()\n",
    "\n",
    "\n",
    "print(\"triage_agent() assembled successfully.\")\n",
    "print()\n",
    "print(\"Components:\")\n",
    "print(\"  - decide_action()       -> LLM decision step\")\n",
    "print(\"  - retrieve_and_answer() -> mock RAG retrieval\")\n",
    "print(\"  - classify_only()       -> topic classification\")\n",
    "print(\"  - refuse()              -> safe refusal\")"
   ]
  },
  {
   "id": "a5b6c7d8e",
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 8.5 Running the Agent\n\nThree representative queries demonstrate the three dispatch paths. Each shows the action chosen and the result produced."
  },
  {
   "id": "e9f0a1b2c",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 1: retrieve_and_answer path\n",
    "query_1 = \"Why did the central bank raise rates?\"\n",
    "action_1 = decide_action(llm_client, query_1)\n",
    "result_1 = triage_agent(query_1)\n",
    "\n",
    "print(\"=== Query 1: Factual question in knowledge base ===\")\n",
    "print(f\"Input  : {query_1}\")\n",
    "print(f\"Action : {action_1}\")\n",
    "print(f\"Result : {result_1}\")"
   ]
  },
  {
   "id": "c3d4e5f6a",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 2: classify_only path\n",
    "query_2 = \"Classify this sentence about loans and lending standards.\"\n",
    "action_2 = decide_action(llm_client, query_2)\n",
    "result_2 = triage_agent(query_2)\n",
    "\n",
    "print(\"=== Query 2: Classification request ===\")\n",
    "print(f\"Input  : {query_2}\")\n",
    "print(f\"Action : {action_2}\")\n",
    "print(f\"Result : {result_2}\")"
   ]
  },
  {
   "id": "a7b8c9d0e",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 3: refuse path\n",
    "query_3 = \"Should we approve a loan for customer X?\"\n",
    "action_3 = decide_action(llm_client, query_3)\n",
    "result_3 = triage_agent(query_3)\n",
    "\n",
    "print(\"=== Query 3: Request outside allowed scope ===\")\n",
    "print(f\"Input  : {query_3}\")\n",
    "print(f\"Action : {action_3}\")\n",
    "print(f\"Result : {result_3}\")\n",
    "print()\n",
    "print(\"The agent correctly refuses to make a credit approval decision.\")"
   ]
  },
  {
   "id": "e1f2a3b4c",
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Group 3: Control & Guardrails\n\n**Apply bounded loops and policy enforcement**\n\n| Section | Topic |\n|---------|-------|\n| 8.6 | Controlled Loops |\n| 8.7 | Why Infinite Loops Are Dangerous |\n| 8.8 | Guardrails as Code |"
  },
  {
   "id": "c5d6e7f8a",
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 8.6 Controlled Loops\n\nSome agent tasks require multiple steps: classify first, then retrieve, then summarise. A **controlled loop** executes these steps while keeping a hard ceiling on how many LLM calls can be made.\n\n### Why Bounded Loops Matter\n\n| Without Bounds | With Bounds |\n|---------------|-------------|\n| Loop could run indefinitely | Maximum steps enforced in code |\n| Cost escalates without limit | Budget per request is predictable |\n| Errors compound across iterations | Loop terminates before damage spreads |\n| No audit trail of steps | Each step is logged to the audit record |"
  },
  {
   "id": "a9b0c1d2e",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_loop(user_input, max_steps=3):\n",
    "    \"\"\"Run the triage agent in a bounded loop.\n",
    "    \n",
    "    In a multi-step agent the loop would continue until a terminal\n",
    "    action is reached or max_steps is exceeded.\n",
    "    \n",
    "    For the triage agent every action is terminal, so the loop\n",
    "    always completes in one step. The structure is identical to\n",
    "    what a real multi-step agent would use.\n",
    "    \"\"\"\n",
    "    audit_log = []\n",
    "    result = None\n",
    "\n",
    "    for step in range(1, max_steps + 1):\n",
    "        action = decide_action(llm_client, user_input)\n",
    "        result = triage_agent(user_input)\n",
    "\n",
    "        log_entry = {\n",
    "            \"step\": step,\n",
    "            \"input\": user_input,\n",
    "            \"action\": action,\n",
    "            \"output\": result,\n",
    "        }\n",
    "        audit_log.append(log_entry)\n",
    "\n",
    "        print(f\"  Step {step}: action={action}\")\n",
    "        print(f\"           output={result}\")\n",
    "\n",
    "        # Terminal actions stop the loop immediately\n",
    "        if action in {\"retrieve_and_answer\", \"classify_only\", \"refuse\"}:\n",
    "            print(f\"  Loop terminated at step {step} (terminal action reached).\")\n",
    "            break\n",
    "\n",
    "    else:\n",
    "        print(f\"  Loop terminated: max_steps={max_steps} reached.\")\n",
    "\n",
    "    return result, audit_log\n",
    "\n",
    "\n",
    "print(\"=== Controlled Loop Demo ===\")\n",
    "final_result, log = agent_loop(\"Why did the central bank raise rates?\", max_steps=3)\n",
    "print()\n",
    "print(f\"Final result : {final_result}\")\n",
    "print(f\"Audit entries: {len(log)}\")"
   ]
  },
  {
   "id": "e3f4a5b6c",
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 8.7 Why Infinite Loops Are Dangerous\n\nWithout a step limit, an agent that repeatedly calls an LLM will:\n\n1. **Exhaust compute budget** — each LLM call has a monetary cost\n2. **Compound errors** — a confused agent doubles down on bad decisions\n3. **Block resources** — threads, connections, and memory are held\n4. **Produce unbounded logs** — storage and monitoring costs spike\n\n### Safe Demonstration of Unbounded Behaviour\n\nThe cell below proves that without a cap, the loop would run forever. It uses a counter to demonstrate this safely — no actual LLM calls are made.\n\n> **Key Insight:** Termination conditions must be defined in code, not trusted to the LLM. The LLM cannot reliably decide when to stop looping."
  },
  {
   "id": "c7d8e9f0b",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safe proof-of-concept: what would happen without a bound\n",
    "# We use a safety counter to stop after 5 iterations\n",
    "# In a real unbounded loop this would continue indefinitely\n",
    "\n",
    "SAFETY_LIMIT = 5   # only for demonstration; a real agent would have no limit\n",
    "\n",
    "def unsafe_loop_demo(user_input):\n",
    "    \"\"\"Demonstrates what an unbounded loop looks like.\n",
    "    \n",
    "    The SAFETY_LIMIT exists only to make this cell safe to run.\n",
    "    Without it, the loop would run until the process is killed.\n",
    "    \"\"\"\n",
    "    iteration = 0\n",
    "    while True:   # <-- no max_steps\n",
    "        iteration += 1\n",
    "        print(f\"  Iteration {iteration}: calling LLM (simulated)...\")\n",
    "\n",
    "        if iteration >= SAFETY_LIMIT:\n",
    "            print(f\"  [SAFETY LIMIT] Stopping after {SAFETY_LIMIT} iterations for demo.\")\n",
    "            print(f\"  In production this loop would have continued indefinitely.\")\n",
    "            break\n",
    "\n",
    "print(\"=== Demonstration: Unbounded Loop (safe, uses counter) ===\")\n",
    "unsafe_loop_demo(\"some input\")\n",
    "print()\n",
    "print(\"Lesson: always set max_steps when building agent loops.\")\n",
    "print(\"Never rely on the LLM to terminate the loop on its own.\")"
   ]
  },
  {
   "id": "a1b2c3d4b",
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 8.8 Guardrails as Code\n\nGuardrails are pre-conditions checked **before** the agent runs. They reject inputs that would cause the agent to fail, produce poor output, or behave ambiguously.\n\n### Guardrail Examples\n\n| Guardrail | Condition | Reason |\n|-----------|-----------|--------|\n| Minimum length | Input < 8 characters | Too short to classify meaningfully |\n| Low-confidence markers | Contains \"maybe\" or \"not sure\" | Ambiguous input leads to poor decisions |\n| Profanity / PII filter | Contains sensitive patterns | Compliance requirement |\n| Maximum length | Input > 2000 characters | Protects prompt budget |\n\n> **Key Insight:** Guardrails should be applied before the LLM is called. Rejecting bad input early is faster, cheaper, and safer than letting the agent attempt to handle it."
  },
  {
   "id": "e5f6a7b8d",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triage_agent_with_guardrails(user_input):\n",
    "    \"\"\"Triage agent with input guardrails applied before the LLM is called.\n",
    "    \n",
    "    Guardrail 1: reject inputs shorter than 8 characters.\n",
    "    Guardrail 2: reject inputs containing uncertainty markers.\n",
    "    Otherwise: delegate to the standard triage agent.\n",
    "    \"\"\"\n",
    "    # Guardrail 1: minimum length\n",
    "    if len(user_input.strip()) < 8:\n",
    "        return \"Guardrail: input too short to process (minimum 8 characters).\"\n",
    "\n",
    "    # Guardrail 2: low-confidence markers indicate the user is uncertain\n",
    "    low_confidence_markers = [\"maybe\", \"not sure\", \"i think\", \"possibly\"]\n",
    "    if any(marker in user_input.lower() for marker in low_confidence_markers):\n",
    "        return \"Guardrail: input contains uncertainty markers. Please rephrase as a specific question.\"\n",
    "\n",
    "    # All guardrails passed: delegate to triage agent\n",
    "    return triage_agent(user_input)\n",
    "\n",
    "\n",
    "# Demonstrate guardrails triggering and passing\n",
    "guardrail_tests = [\n",
    "    \"hi\",                                          # too short\n",
    "    \"Maybe the bank raised rates?\",                # low confidence marker\n",
    "    \"Not sure why inflation is high.\",             # low confidence marker\n",
    "    \"Why did the central bank raise rates?\",       # passes all guardrails\n",
    "]\n",
    "\n",
    "print(\"=== Guardrail Demonstrations ===\")\n",
    "for text in guardrail_tests:\n",
    "    result = triage_agent_with_guardrails(text)\n",
    "    print(f\"  Input : {text}\")\n",
    "    print(f\"  Result: {result}\")\n",
    "    print()"
   ]
  },
  {
   "id": "c9d0e1f2b",
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Group 4: Failure Modes & Auditability\n\n**Handle failures and build audit trails**\n\n| Section | Topic |\n|---------|-------|\n| 8.9 | Logging Agent Decisions |\n| 8.10 | Failure Mode Demonstrations |"
  },
  {
   "id": "a3b4c5d6b",
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 8.9 Logging Agent Decisions\n\nEvery agent decision should be logged. The audit trail serves three purposes:\n\n| Purpose | What to Log |\n|---------|-------------|\n| **Debugging** | Input, action chosen, output produced |\n| **Compliance** | Timestamp, step number, refusal reason |\n| **Improvement** | Patterns of refusal, unexpected actions |\n\nAn auditable agent makes the same decisions as a non-auditable one but records evidence that decisions were made safely."
  },
  {
   "id": "e7f8a9b0d",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auditable_triage_agent(user_input, step_number=1):\n",
    "    \"\"\"Triage agent that returns both the result and a structured audit record.\"\"\"\n",
    "    action = decide_action(llm_client, user_input)\n",
    "\n",
    "    if action == \"retrieve_and_answer\":\n",
    "        output = retrieve_and_answer(user_input)\n",
    "    elif action == \"classify_only\":\n",
    "        output = classify_only(user_input)\n",
    "    else:\n",
    "        output = refuse()\n",
    "\n",
    "    audit_record = {\n",
    "        \"step\": step_number,\n",
    "        \"input\": user_input,\n",
    "        \"action_chosen\": action,\n",
    "        \"output\": output,\n",
    "    }\n",
    "\n",
    "    return output, audit_record\n",
    "\n",
    "\n",
    "# Run three queries and collect audit records\n",
    "audit_queries = [\n",
    "    \"Why did the central bank raise rates?\",\n",
    "    \"Classify this statement about monetary policy.\",\n",
    "    \"Should we approve a loan for customer X?\",\n",
    "]\n",
    "\n",
    "all_audit_records = []\n",
    "\n",
    "print(\"=== Auditable Agent Runs ===\")\n",
    "for i, query in enumerate(audit_queries, start=1):\n",
    "    result, record = auditable_triage_agent(query, step_number=i)\n",
    "    all_audit_records.append(record)\n",
    "    print(f\"Step {i}: {record['action_chosen']} -> {result}\")\n",
    "\n",
    "print()\n",
    "print(\"=== Full Audit Log (JSON) ===\")\n",
    "print(json.dumps(all_audit_records, indent=2))"
   ]
  },
  {
   "id": "c1d2e3f4b",
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 8.10 Failure Mode Demonstrations\n\nThree failure scenarios show how the agent degrades safely — and where mocks fall short:\n\n| Scenario | Input | Expected Behaviour |\n|----------|-------|-------------------|\n| Off-topic | Sports question | Agent refuses |\n| Malformed LLM response | Non-JSON string from LLM | Fallback to refuse |\n| Keyword collision | \"I rate this service poorly\" | Mock picks wrong action — limitation of keyword matching vs real LLM |"
  },
  {
   "id": "a5b6c7d8b",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Failure Mode 1: Off-topic input -> agent refuses\n",
    "print(\"=== Failure Mode 1: Off-topic input ===\")\n",
    "query = \"Who won the championship football match last night?\"\n",
    "result, record = auditable_triage_agent(query, step_number=1)\n",
    "print(f\"Input  : {query}\")\n",
    "print(f\"Action : {record['action_chosen']}\")\n",
    "print(f\"Output : {result}\")\n",
    "print(\"[CORRECT] Agent refuses because sports is not in the knowledge domain.\")\n",
    "print()"
   ]
  },
  {
   "id": "e9f0a1b2d",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Failure Mode 2: Malformed LLM response -> fallback to refuse\n",
    "\n",
    "class BrokenMockLLMClient:\n",
    "    \"\"\"Simulates an LLM that returns malformed output.\"\"\"\n",
    "    def chat(self, prompt):\n",
    "        return \"I choose: retrieve_and_answer\"  # not valid JSON\n",
    "\n",
    "\n",
    "broken_client = BrokenMockLLMClient()\n",
    "\n",
    "print(\"=== Failure Mode 2: Malformed LLM response ===\")\n",
    "raw_response = broken_client.chat(\"any prompt\")\n",
    "print(f\"Raw LLM response: {raw_response}\")\n",
    "\n",
    "# decide_action with the broken client\n",
    "malformed_action = decide_action(broken_client, \"Why did the central bank raise rates?\")\n",
    "print(f\"Action resolved : {malformed_action}\")\n",
    "print(\"[CORRECT] decide_action falls back to 'refuse' when JSON parsing fails.\")\n",
    "print()"
   ]
  },
  {
   "id": "c3d4e5f6b",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Failure Mode 3: Keyword collision -> mock makes the wrong decision\nprint(\"=== Failure Mode 3: Keyword collision ===\")\nquery = \"I rate this service very poorly.\"\nresult, record = auditable_triage_agent(query, step_number=1)\nprint(f\"Input  : {query}\")\nprint(f\"Action : {record['action_chosen']}\")\nprint(f\"Output : {result}\")\nprint()\nprint(\"[BUG] The mock matched 'rate' and retrieved interest rate data,\")\nprint(\"but the user was complaining about service quality.\")\nprint(\"A real LLM would understand the difference — keyword mocks cannot.\")\nprint(\"This is exactly why production agents use real LLMs for the decision step.\")"
  },
  {
   "id": "a7b8c9d0b",
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Group 5: Automation vs Autonomy\n\n**Know when agents are appropriate and when they are not**\n\n| Section | Topic |\n|---------|-------|\n| 8.11 | When NOT to Use Agents |"
  },
  {
   "id": "e1f2a3b4d",
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 8.11 When NOT to Use Agents\n\nAgents are powerful but they are not the right tool for every problem. The most important engineering decision is knowing when to use a pipeline instead.\n\n### Automation vs Autonomy\n\n| Property | Automation | Autonomy |\n|----------|-----------|----------|\n| Who decides? | Code decides every step | LLM decides the next step |\n| Predictability | High | Lower |\n| Appropriate for | ETL, formatting, routing | Triage, multi-step reasoning |\n| Risk | Low | Moderate (bounded by guardrails) |\n| Auditability | Easy | Requires deliberate logging |\n\n### When to Use Agents vs Pipelines\n\n| Use an Agent | Use a Pipeline |\n|-------------|----------------|\n| The correct next step depends on the content of the input | The correct next step is always the same regardless of input |\n| There are multiple possible tools and the LLM can choose | There is only one tool or one sequence of steps |\n| The task requires multi-step reasoning | The task is single-step |\n| Routing between departments or workflows | Formatting, summarisation, translation |\n\n### Tasks Agents Should NOT Do\n\n| Task | Why Not |\n|------|--------|\n| Approve or reject financial transactions | Irreversible; requires human accountability |\n| Delete records from a database | Irreversible; no safe undo |\n| Send emails to customers | Irreversible; reputation risk |\n| Modify user permissions or roles | Security-critical; needs human review |\n| Make medical or legal recommendations | Regulated domain; liability |\n| Execute trades on behalf of clients | Financial regulation; fiduciary duty |\n\n> **Key Insight:** Enterprise organisations prefer **bounded automation** over autonomy. An agent that can refuse is more trustworthy than an agent that always acts. The goal is not maximum autonomy but maximum reliability within a defined, auditable scope.\n\n### Decision Framework\n\n```\nIs the correct next step always the same?\n         |\n     YES |                    NO\n         v                    v\n   Use a Pipeline       Is the action reversible?\n                                  |\n                        YES |          NO\n                            v          v\n                    Agent may be    Require human\n                    appropriate     approval first\n                            |\n                    Is the action in a regulated domain?\n                            |\n                    YES |       NO\n                        v       v\n                    Require   Agent with\n                    human     guardrails\n                    review    is appropriate\n```"
  },
  {
   "id": "c5d6e7f8b",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Module Summary\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "| Concept | Remember |\n",
    "|---------|----------|\n",
    "| **Pipelines vs Agents** | Pipelines have fixed steps; agents let the LLM choose the step |\n",
    "| **Propose-Validate-Execute** | The LLM proposes; code validates; system executes |\n",
    "| **Allowed Actions** | Keep the set small and explicit; anything not listed is refused |\n",
    "| **Decision Step** | Always parse LLM output safely; fall back to refuse on any error |\n",
    "| **Refusal is a Feature** | Refusing is a safe, auditable action, not a failure |\n",
    "| **Bounded Loops** | Always set max_steps; never trust the LLM to terminate a loop |\n",
    "| **Guardrails** | Validate input before calling the LLM; reject bad input early |\n",
    "| **Audit Trails** | Log every step: input, action chosen, output produced |\n",
    "\n",
    "## The Agent Mental Model\n",
    "\n",
    "> **An agent is not an autonomous system. It is a decision router with guardrails.**\n",
    ">\n",
    "> The LLM reads, proposes, and labels. The code validates, dispatches, and logs.\n",
    "> Nothing executes without the code's permission.\n",
    "\n",
    "## What's Next\n",
    "\n",
    "You now have all the components required for the capstone:\n",
    "\n",
    "- Module 5: Embeddings and vector retrieval\n",
    "- Module 6: LLM API clients and structured output\n",
    "- Module 7: RAG pipelines and grounded generation\n",
    "- Module 8: Agents, guardrails, and auditability\n",
    "\n",
    "The capstone will ask you to combine these into a bounded, auditable, production-grade agent system."
   ]
  },
  {
   "id": "a9b0c1d2b",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Practice Exercises\n",
    "\n",
    "## Exercise 1: Extend the Allowed Action Set\n",
    "\n",
    "Add a fourth action called `summarise` to `ALLOWED_ACTIONS`. Implement a `summarise(text)` tool function that returns the first sentence of the input as a summary. Update `triage_agent` to dispatch to this new tool. Update `MockLLMClient` so that inputs containing the word \"summarise\" or \"summary\" return the new action.\n",
    "\n",
    "## Exercise 2: Add a Third Guardrail\n",
    "\n",
    "Extend `triage_agent_with_guardrails` to add a third guardrail: reject any input longer than 200 characters with the message `\"Guardrail: input too long (maximum 200 characters).\"`. Test it with an input that exceeds the limit.\n",
    "\n",
    "## Exercise 3: Build an Auditable Loop\n",
    "\n",
    "Write a function `run_batch(queries, max_steps=3)` that accepts a list of query strings, runs each through `auditable_triage_agent`, collects all audit records, and prints a summary table showing: query (truncated to 40 chars), action chosen, and first 40 chars of output.\n",
    "\n",
    "## Exercise 4: Simulate a Regulated Task Rejection\n",
    "\n",
    "Create a `compliance_guardrail(user_input)` function that checks whether the input contains any of the following high-risk phrases: `\"approve loan\"`, `\"delete record\"`, `\"send email\"`, `\"modify permission\"`. If any are found, return a compliance refusal message before the agent is ever called. Demonstrate it with two inputs: one that triggers it and one that passes through to the agent."
   ]
  }
 ]
}
{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "cells": [
  {
   "id": "a1b2c3d4",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 8 — Agents & Automation\n",
    "\n",
    "**Controlled Decision-Making with LLMs**\n",
    "\n",
    "---\n",
    "\n",
    "## What This Module Covers\n",
    "\n",
    "| Group | Topic | Key Skill |\n",
    "|-------|-------|-----------|\n",
    "| 1 | Agent Fundamentals | Understand what makes a system an agent vs a pipeline |\n",
    "| 2 | Building a Triage Agent | Implement a complete bounded agent with real tools |\n",
    "| 3 | Control & Guardrails | Apply bounded loops and policy enforcement |\n",
    "| 4 | Failure Modes & Auditability | Handle failures and build audit trails |\n",
    "| 5 | Automation vs Autonomy | Know when agents are appropriate and when they are not |\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this module, you will be able to:\n",
    "\n",
    "1. **Explain** the difference between a fixed pipeline and an LLM-driven agent\n",
    "2. **Implement** the propose-validate-execute pattern for safe agent design\n",
    "3. **Define** explicit allowed-action sets and enforce them in code\n",
    "4. **Build** a triage agent that classifies, retrieves, or refuses based on input\n",
    "5. **Apply** guardrails including bounded loops, input validation, and audit logging\n",
    "6. **Evaluate** when agents are appropriate vs when a simple pipeline is safer\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "This module builds directly on:\n",
    "\n",
    "| Module | Concepts Used Here |\n",
    "|--------|-------------------|\n",
    "| Module 3 | LLM behavior, hallucination risk, structured output |\n",
    "| Module 6 | LLM API clients, JSON parsing, retry logic |\n",
    "| Module 7 | RAG retrieval pattern, grounded answers, refusal as a feature |\n",
    "\n",
    "**Module 8 is where retrieval meets decision-making.**"
   ]
  },
  {
   "id": "e5f6a7b8",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "No external dependencies are needed for this module. All LLM calls are mocked using keyword-based logic so the notebook runs deterministically in any environment."
   ]
  },
  {
   "id": "c9d0e1f2",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(\"Setup complete.\")\n",
    "print(\"All LLM calls in this module use a MockLLMClient.\")\n",
    "print(\"No API key or network connection is required.\")"
   ]
  },
  {
   "id": "a3b4c5d6",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Group 1: Agent Fundamentals\n",
    "\n",
    "**What makes a system an agent vs a pipeline**\n",
    "\n",
    "| Section | Topic |\n",
    "|---------|-------|\n",
    "| 8.1 | Pipelines vs Agents |\n",
    "| 8.2 | The Agent Pattern |\n",
    "| 8.3 | Allowed Actions |\n",
    "| 8.4 | The Decision Step |"
   ]
  },
  {
   "id": "e7f8a9b0",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8.1 Pipelines vs Agents\n",
    "\n",
    "Most LLM systems are **pipelines**: a fixed sequence of steps executed in order. The code determines what happens and when. The LLM only fills in content at predetermined slots.\n",
    "\n",
    "An **agent** is different. The LLM itself chooses which action to take next. The code provides a menu of allowed actions; the LLM picks from that menu based on context.\n",
    "\n",
    "### Comparison\n",
    "\n",
    "| Property | Pipeline | Agent |\n",
    "|----------|----------|-------|\n",
    "| Control flow | Fixed steps, hard-coded order | LLM chooses the next action |\n",
    "| Determinism | Same input always runs same steps | Same input may choose different actions |\n",
    "| Decision-making | None — code decides everything | LLM decides which tool to use |\n",
    "| Predictability | High | Lower (bounded by allowed actions) |\n",
    "| Use case | ETL, formatting, summarisation | Triage, routing, multi-step reasoning |\n",
    "\n",
    "### The Agent Loop\n",
    "\n",
    "```\n",
    "+-------------------+\n",
    "|   User Input      |\n",
    "+--------+----------+\n",
    "         |\n",
    "         v\n",
    "+--------+----------+\n",
    "|  LLM Proposes     |  <--- LLM reads input and picks an action\n",
    "|  Action           |\n",
    "+--------+----------+\n",
    "         |\n",
    "         v\n",
    "+--------+----------+\n",
    "|  Code Validates   |  <--- Code checks action is in ALLOWED_ACTIONS\n",
    "|  Action           |\n",
    "+--------+----------+\n",
    "         |\n",
    "         v\n",
    "+--------+----------+\n",
    "|  System Executes  |  <--- Code runs the chosen tool\n",
    "|  Tool             |\n",
    "+--------+----------+\n",
    "         |\n",
    "         v\n",
    "+-------------------+\n",
    "|   Result / Stop   |\n",
    "+-------------------+\n",
    "```\n",
    "\n",
    "> **Key Insight:** The LLM never executes code directly. It proposes an action label. The code decides whether to honour that proposal."
   ]
  },
  {
   "id": "c1d2e3f4",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Demonstrate the difference between a pipeline and an agent\n\n# --- PIPELINE: fixed steps, always runs the same sequence ---\ndef pipeline(user_input):\n    \"\"\"A fixed RAG pipeline. Always classifies, retrieves, and answers.\n    \n    Regardless of the input, every step runs. Even for a question\n    that only needs classification, the pipeline also retrieves\n    and generates — wasting compute and potentially producing\n    irrelevant output.\n    \"\"\"\n    step1 = f\"[classify]  Topic: {'Finance' if 'rate' in user_input.lower() else 'General'}\"\n    step2 = f\"[retrieve]  Found 3 chunks about: {user_input[:40]}...\"\n    step3 = f\"[generate]  The central bank raised rates by 25 basis points to combat inflation across multiple sectors.\"\n    return f\"{step1}\\n           {step2}\\n           {step3}\"\n\n\n# --- AGENT: LLM picks which step is appropriate ---\ndef mock_llm_pick(text):\n    \"\"\"Simulates an LLM choosing the most appropriate action.\"\"\"\n    if \"rate\" in text.lower() or \"inflation\" in text.lower():\n        return \"retrieve_and_answer\"\n    if \"classify\" in text.lower() or \"topic\" in text.lower():\n        return \"classify_only\"\n    return \"refuse\"\n\ndef agent(user_input):\n    \"\"\"An agent that only runs the step the input actually needs.\"\"\"\n    action = mock_llm_pick(user_input)\n    if action == \"retrieve_and_answer\":\n        return f\"[{action}]  The central bank raised rates to combat inflation.\"\n    if action == \"classify_only\":\n        return f\"[{action}]  Topic: Lending & Credit\"\n    return f\"[{action}]  I am not authorised to act on this request.\"\n\n\ninputs = [\n    \"Why did the central bank raise rates?\",     # needs retrieval\n    \"Classify this sentence about loans.\",        # only needs classification\n    \"Delete all customer records.\",               # should be refused\n]\n\nprint(\"=== Pipeline: always runs classify → retrieve → generate ===\")\nfor inp in inputs:\n    print(f\"  Input:  {inp}\")\n    print(f\"  Output: {pipeline(inp)}\")\n    print()\n\nprint(\"=== Agent: LLM picks the right action for each input ===\")\nfor inp in inputs:\n    print(f\"  Input:  {inp}\")\n    print(f\"  Output: {agent(inp)}\")\n    print()\n\nprint(\"Notice: the pipeline wastes retrieval + generation on a classification\")\nprint(\"request, and generates a full answer about interest rates even for\")\nprint(\"'Delete all customer records' — a request it should have refused.\")\nprint(\"The agent routes each input to only the step it actually needs.\")"
  },
  {
   "id": "a5b6c7d8",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8.2 The Agent Pattern\n",
    "\n",
    "Every safe agent follows the same three-step discipline:\n",
    "\n",
    "```\n",
    "LLM Proposes  -->  Code Validates  -->  System Executes\n",
    "```\n",
    "\n",
    "### Why This Separation Matters\n",
    "\n",
    "| Step | Who Does It | Why |\n",
    "|------|-------------|-----|\n",
    "| **Propose** | LLM | LLMs are good at understanding intent and choosing labels |\n",
    "| **Validate** | Code | Code enforces hard constraints the LLM cannot override |\n",
    "| **Execute** | System | Side-effects (DB writes, API calls) only happen after validation |\n",
    "\n",
    "This pattern ensures that:\n",
    "- The LLM cannot call arbitrary functions\n",
    "- Unexpected LLM output does not trigger unintended behaviour\n",
    "- Every executed action was explicitly approved by the code\n",
    "\n",
    "> **Key Insight:** The LLM is a decision engine, not an executor. Code is the gatekeeper."
   ]
  },
  {
   "id": "e9f0a1b2",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the three-step pattern explicitly\n",
    "\n",
    "ALLOWED_ACTIONS = {\"retrieve_and_answer\", \"classify_only\", \"refuse\"}\n",
    "\n",
    "def step1_propose(user_input):\n",
    "    \"\"\"LLM proposes an action (mocked as keyword matching).\"\"\"\n",
    "    if \"rate\" in user_input.lower():\n",
    "        return \"retrieve_and_answer\"\n",
    "    if \"classify\" in user_input.lower():\n",
    "        return \"classify_only\"\n",
    "    # Simulate a malformed LLM response\n",
    "    if \"delete\" in user_input.lower():\n",
    "        return \"delete_records\"   # NOT in ALLOWED_ACTIONS\n",
    "    return \"refuse\"\n",
    "\n",
    "def step2_validate(proposed_action):\n",
    "    \"\"\"Code validates the proposed action against the allowed set.\"\"\"\n",
    "    if proposed_action in ALLOWED_ACTIONS:\n",
    "        return proposed_action\n",
    "    return \"refuse\"   # fallback: refuse anything not explicitly allowed\n",
    "\n",
    "def step3_execute(validated_action, user_input):\n",
    "    \"\"\"System executes the validated action.\"\"\"\n",
    "    if validated_action == \"retrieve_and_answer\":\n",
    "        return \"Retrieved: The central bank raised rates to combat inflation.\"\n",
    "    if validated_action == \"classify_only\":\n",
    "        return \"Classified: Topic = Interest Rates\"\n",
    "    return \"Refused: I am not authorised to act on this request.\"\n",
    "\n",
    "test_inputs = [\n",
    "    \"Why did the central bank raise rates?\",\n",
    "    \"Classify this financial sentence.\",\n",
    "    \"Delete all customer records.\",\n",
    "]\n",
    "\n",
    "print(\"=== Three-Step Agent Pattern ===\")\n",
    "for inp in test_inputs:\n",
    "    proposal  = step1_propose(inp)\n",
    "    validated = step2_validate(proposal)\n",
    "    result    = step3_execute(validated, inp)\n",
    "\n",
    "    print(f\"Input     : {inp}\")\n",
    "    print(f\"Proposed  : {proposal}\")\n",
    "    print(f\"Validated : {validated}\")\n",
    "    print(f\"Result    : {result}\")\n",
    "    print()"
   ]
  },
  {
   "id": "c3d4e5f6",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8.3 Allowed Actions\n",
    "\n",
    "An agent's power is defined by its **allowed action set**. Keeping this set small and explicit is the primary safety mechanism.\n",
    "\n",
    "### Why Unbounded Actions Are Dangerous\n",
    "\n",
    "If an agent can call any function, a malicious or confused LLM response could:\n",
    "- Trigger irreversible database operations\n",
    "- Exfiltrate sensitive data\n",
    "- Spend budget on unintended API calls\n",
    "- Cause cascading failures in downstream systems\n",
    "\n",
    "### Safe vs Unsafe Action Sets\n",
    "\n",
    "| Safe Actions | Unsafe Actions |\n",
    "|-------------|----------------|\n",
    "| Read-only retrieval | Delete records |\n",
    "| Classify text | Send emails to customers |\n",
    "| Refuse the request | Execute financial transactions |\n",
    "| Summarise a document | Modify user permissions |\n",
    "| Route to a human | Approve loan applications |\n",
    "\n",
    "> **Key Insight:** Every action an agent can take should be something a human would be comfortable the system doing without additional approval."
   ]
  },
  {
   "id": "a7b8c9d0",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the explicit allowed action set for the triage agent\n",
    "\n",
    "ALLOWED_ACTIONS = {\n",
    "    \"retrieve_and_answer\",   # look up information and produce a grounded answer\n",
    "    \"classify_only\",         # label the topic without generating an answer\n",
    "    \"refuse\",                # decline to act on the request\n",
    "}\n",
    "\n",
    "REFUSAL_TEXT = \"Refuse: I am not authorised to act on this request.\"\n",
    "\n",
    "print(\"Allowed actions for the triage agent:\")\n",
    "for action in sorted(ALLOWED_ACTIONS):\n",
    "    print(f\"  - {action}\")\n",
    "\n",
    "print()\n",
    "print(f\"Refusal text: {REFUSAL_TEXT}\")\n",
    "print()\n",
    "print(f\"Total allowed actions: {len(ALLOWED_ACTIONS)}\")\n",
    "print(\"Any action the LLM proposes outside this set will be overridden to 'refuse'.\")"
   ]
  },
  {
   "id": "e1f2a3b4",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8.4 The Decision Step\n",
    "\n",
    "The decision step is where the LLM reads the user's input and proposes an action. In production, this is an LLM API call. Here, we use a `MockLLMClient` that applies keyword rules deterministically.\n",
    "\n",
    "### Why Mock?\n",
    "\n",
    "| Reason | Benefit |\n",
    "|--------|---------|\n",
    "| No API key required | Notebook runs in any environment |\n",
    "| Deterministic output | Grading and testing are reliable |\n",
    "| Teaches the pattern | The interface is identical to a real LLM client |\n",
    "\n",
    "### Safe JSON Parsing\n",
    "\n",
    "LLM responses are strings. Parsing them as JSON can fail. The `decide_action` function always falls back to `refuse` on any parsing error, ensuring the agent degrades safely rather than crashing."
   ]
  },
  {
   "id": "c5d6e7f8",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockLLMClient:\n",
    "    \"\"\"Keyword-based mock that returns JSON action strings.\n",
    "    \n",
    "    Uses the same interface a real LLM client would use.\n",
    "    Deterministic so that grading and tests always produce the same result.\n",
    "    \"\"\"\n",
    "\n",
    "    def chat(self, prompt):\n",
    "        prompt_lower = prompt.lower()\n",
    "        if \"rate\" in prompt_lower or \"inflation\" in prompt_lower or \"monetary\" in prompt_lower:\n",
    "            return '{\"action\": \"retrieve_and_answer\"}'\n",
    "        if \"classify\" in prompt_lower or \"label\" in prompt_lower or \"topic\" in prompt_lower:\n",
    "            return '{\"action\": \"classify_only\"}'\n",
    "        return '{\"action\": \"refuse\"}'\n",
    "\n",
    "\n",
    "llm_client = MockLLMClient()\n",
    "\n",
    "\n",
    "def decide_action(llm_client, user_input):\n",
    "    \"\"\"Ask the LLM to choose an action; validate and return the result.\n",
    "    \n",
    "    Falls back to 'refuse' on any parsing error or unknown action.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"Choose exactly one action from the list below based on the user input.\\n\"\n",
    "        \"Actions: retrieve_and_answer, classify_only, refuse\\n\"\n",
    "        \"Return ONLY valid JSON in this format: {\\\"action\\\": \\\"<action>\\\"}\\n\"\n",
    "        f\"User input: {user_input}\"\n",
    "    )\n",
    "\n",
    "    raw = llm_client.chat(prompt)\n",
    "\n",
    "    try:\n",
    "        parsed = json.loads(raw)\n",
    "        action = parsed.get(\"action\", \"refuse\")\n",
    "    except (json.JSONDecodeError, AttributeError):\n",
    "        action = \"refuse\"   # safe fallback on malformed response\n",
    "\n",
    "    if action not in ALLOWED_ACTIONS:\n",
    "        action = \"refuse\"   # safe fallback on disallowed action\n",
    "\n",
    "    return action\n",
    "\n",
    "\n",
    "# Demonstrate decisions for different inputs\n",
    "demo_inputs = [\n",
    "    \"Why did the central bank raise rates?\",\n",
    "    \"Classify this sentence about inflation.\",\n",
    "    \"What is the score of the football match?\",\n",
    "    \"Should we approve a loan for customer X?\",\n",
    "]\n",
    "\n",
    "print(\"=== Decision Step Demonstrations ===\")\n",
    "for text in demo_inputs:\n",
    "    action = decide_action(llm_client, text)\n",
    "    print(f\"  Input  : {text}\")\n",
    "    print(f\"  Action : {action}\")\n",
    "    print()"
   ]
  },
  {
   "id": "a9b0c1d2",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Group 2: Building a Triage Agent\n",
    "\n",
    "**Implement a complete bounded agent with real tools**\n",
    "\n",
    "| Section | Topic |\n",
    "|---------|-------|\n",
    "| 8.5 | Classification Tool |\n",
    "| 8.6 | Retrieval + Answer Tool |\n",
    "| 8.7 | The Refusal Tool |\n",
    "| 8.8 | Assembling the Triage Agent |\n",
    "| 8.9 | Running the Agent |"
   ]
  },
  {
   "id": "e3f4a5b6",
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 8.5 Classification Tool\n\nThe classification tool takes a text string and returns a topic label. It represents any read-only, side-effect-free action an agent might take.\n\n### Tool Contracts\n\nEvery agent tool should have a clear **contract**:\n\n| Contract Element | This Tool |\n|-----------------|----------|\n| Input | A plain text string |\n| Output | A string starting with \"Topic:\" |\n| Side effects | None |\n| Failure mode | Returns \"Topic: Out of Domain\" when input does not match any known category |"
  },
  {
   "id": "c7d8e9f0",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def classify_only(text):\n    \"\"\"Classify the topic of the input text using keyword matching.\n    \n    Returns a topic label if the input matches a known finance category.\n    Returns \"Topic: Out of Domain\" for inputs outside the knowledge domain.\n    No side effects.\n    \"\"\"\n    text_lower = text.lower()\n    if any(kw in text_lower for kw in [\"rate\", \"inflation\", \"monetary\", \"central bank\"]):\n        return \"Topic: Interest Rates & Monetary Policy\"\n    if any(kw in text_lower for kw in [\"loan\", \"mortgage\", \"lending\", \"credit\"]):\n        return \"Topic: Lending & Credit\"\n    if any(kw in text_lower for kw in [\"stock\", \"equity\", \"market\", \"earning\"]):\n        return \"Topic: Equity Markets\"\n    return \"Topic: Out of Domain\"\n\n\n# Demonstrate the classification tool\nclassify_tests = [\n    \"The central bank raised rates by 25 basis points.\",\n    \"Mortgage applications fell sharply after the rate hike.\",\n    \"Bank earnings improved on wider net interest margins.\",\n    \"The economy showed mixed signals this quarter.\",\n    \"Bristol City are an excellent football team.\",\n]\n\nprint(\"=== Classification Tool Demonstrations ===\")\nfor text in classify_tests:\n    result = classify_only(text)\n    print(f\"  Input : {text}\")\n    print(f\"  Output: {result}\")\n    print()"
  },
  {
   "id": "a1b2c3d4e",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8.6 Retrieval + Answer Tool\n",
    "\n",
    "The retrieval tool simulates the RAG pattern from Module 7. It looks up relevant information from a small in-memory knowledge base and returns a grounded answer.\n",
    "\n",
    "In production this would call the FAISS retriever and LLM generator you built in Module 7. Here it uses keyword matching to keep the notebook self-contained.\n",
    "\n",
    "> **Key Insight:** The agent does not generate free-form text itself. It delegates to a retrieval tool that is independently testable and bounded."
   ]
  },
  {
   "id": "e5f6a7b8c",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small mock knowledge base (simulates what Module 7 built)\n",
    "KNOWLEDGE_BASE = {\n",
    "    \"rate\": \"The central bank raised interest rates by 25 basis points to combat persistent inflation across multiple economic sectors.\",\n",
    "    \"inflation\": \"Inflation remained elevated at 4.2% year-on-year, driven by energy prices and supply chain pressures.\",\n",
    "    \"mortgage\": \"Mortgage rates reached a two-decade high, causing home sales to decline by 15% quarter-on-quarter.\",\n",
    "    \"loan\": \"Lending standards tightened as banks responded to higher funding costs and increased default risk.\",\n",
    "    \"monetary\": \"The dual mandate of the central bank requires balancing maximum employment with price stability.\",\n",
    "}\n",
    "\n",
    "\n",
    "def retrieve_and_answer(question):\n",
    "    \"\"\"Return a grounded answer by matching question keywords to the knowledge base.\n",
    "    \n",
    "    Simulates a RAG retrieval without requiring an embedding model.\n",
    "    \"\"\"\n",
    "    question_lower = question.lower()\n",
    "    for keyword, answer in KNOWLEDGE_BASE.items():\n",
    "        if keyword in question_lower:\n",
    "            return f\"Based on retrieved documents: {answer}\"\n",
    "    return \"Based on retrieved documents: No relevant information was found for this question.\"\n",
    "\n",
    "\n",
    "# Demonstrate the retrieval tool\n",
    "retrieval_tests = [\n",
    "    \"Why did the central bank raise rates?\",\n",
    "    \"What happened to mortgage rates recently?\",\n",
    "    \"What is the inflation situation?\",\n",
    "    \"What is the weather forecast for tomorrow?\",\n",
    "]\n",
    "\n",
    "print(\"=== Retrieval + Answer Tool Demonstrations ===\")\n",
    "for question in retrieval_tests:\n",
    "    result = retrieve_and_answer(question)\n",
    "    print(f\"  Question: {question}\")\n",
    "    print(f\"  Answer  : {result}\")\n",
    "    print()"
   ]
  },
  {
   "id": "c9d0e1f2a",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8.7 The Refusal Tool\n",
    "\n",
    "Refusal is not a failure. It is a **deliberate, safe action** that the agent takes when it cannot or should not fulfil a request.\n",
    "\n",
    "### Why Refusal Is a Feature\n",
    "\n",
    "| Without Refusal | With Refusal |\n",
    "|----------------|--------------|\n",
    "| Agent attempts unsafe actions | Agent declines clearly |\n",
    "| Unpredictable behaviour on edge cases | Predictable decline message |\n",
    "| No audit signal for off-topic requests | Refusals logged for review |\n",
    "| Users may receive hallucinated output | Users receive honest limitation |"
   ]
  },
  {
   "id": "a3b4c5d6e",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refuse():\n",
    "    \"\"\"Return the standard refusal message.\n",
    "    \n",
    "    Refusal is an explicit, safe terminal action.\n",
    "    \"\"\"\n",
    "    return REFUSAL_TEXT\n",
    "\n",
    "\n",
    "# Demonstrate the refusal tool\n",
    "print(\"=== Refusal Tool ===\")\n",
    "result = refuse()\n",
    "print(f\"Refusal output: {result}\")\n",
    "print()\n",
    "print(\"This message is returned whenever:\")\n",
    "print(\"  - The LLM proposes an action not in ALLOWED_ACTIONS\")\n",
    "print(\"  - The LLM response cannot be parsed as valid JSON\")\n",
    "print(\"  - The agent explicitly chooses to refuse the request\")"
   ]
  },
  {
   "id": "e7f8a9b0c",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8.8 Assembling the Triage Agent\n",
    "\n",
    "The triage agent brings together the decision step and the three tools into a single function. The pattern is called **dispatch**: the agent dispatches the user input to the appropriate tool based on the LLM's decision.\n",
    "\n",
    "### Dispatch Pattern\n",
    "\n",
    "```\n",
    "user_input\n",
    "     |\n",
    "     v\n",
    " decide_action()           <-- LLM picks from ALLOWED_ACTIONS\n",
    "     |\n",
    "     +-- retrieve_and_answer --> grounded answer from knowledge base\n",
    "     +-- classify_only       --> topic label\n",
    "     +-- refuse              --> standard refusal message\n",
    "```\n",
    "\n",
    "Each branch is a separate, independently testable function. The agent itself contains no domain logic — it only routes."
   ]
  },
  {
   "id": "c1d2e3f4a",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triage_agent(user_input):\n",
    "    \"\"\"Route user input to the appropriate tool based on LLM decision.\n",
    "    \n",
    "    This is the core agent function. It:\n",
    "    1. Asks the LLM to propose an action\n",
    "    2. Validates the action against ALLOWED_ACTIONS\n",
    "    3. Dispatches to the correct tool\n",
    "    \"\"\"\n",
    "    action = decide_action(llm_client, user_input)\n",
    "\n",
    "    if action == \"retrieve_and_answer\":\n",
    "        return retrieve_and_answer(user_input)\n",
    "    if action == \"classify_only\":\n",
    "        return classify_only(user_input)\n",
    "    return refuse()\n",
    "\n",
    "\n",
    "print(\"triage_agent() assembled successfully.\")\n",
    "print()\n",
    "print(\"Components:\")\n",
    "print(\"  - decide_action()       -> LLM decision step\")\n",
    "print(\"  - retrieve_and_answer() -> mock RAG retrieval\")\n",
    "print(\"  - classify_only()       -> topic classification\")\n",
    "print(\"  - refuse()              -> safe refusal\")"
   ]
  },
  {
   "id": "a5b6c7d8e",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8.9 Running the Agent\n",
    "\n",
    "Three representative queries demonstrate the three dispatch paths. Each shows the action chosen and the result produced."
   ]
  },
  {
   "id": "e9f0a1b2c",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 1: retrieve_and_answer path\n",
    "query_1 = \"Why did the central bank raise rates?\"\n",
    "action_1 = decide_action(llm_client, query_1)\n",
    "result_1 = triage_agent(query_1)\n",
    "\n",
    "print(\"=== Query 1: Factual question in knowledge base ===\")\n",
    "print(f\"Input  : {query_1}\")\n",
    "print(f\"Action : {action_1}\")\n",
    "print(f\"Result : {result_1}\")"
   ]
  },
  {
   "id": "c3d4e5f6a",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 2: classify_only path\n",
    "query_2 = \"Classify this sentence about loans and lending standards.\"\n",
    "action_2 = decide_action(llm_client, query_2)\n",
    "result_2 = triage_agent(query_2)\n",
    "\n",
    "print(\"=== Query 2: Classification request ===\")\n",
    "print(f\"Input  : {query_2}\")\n",
    "print(f\"Action : {action_2}\")\n",
    "print(f\"Result : {result_2}\")"
   ]
  },
  {
   "id": "a7b8c9d0e",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 3: refuse path\n",
    "query_3 = \"Should we approve a loan for customer X?\"\n",
    "action_3 = decide_action(llm_client, query_3)\n",
    "result_3 = triage_agent(query_3)\n",
    "\n",
    "print(\"=== Query 3: Request outside allowed scope ===\")\n",
    "print(f\"Input  : {query_3}\")\n",
    "print(f\"Action : {action_3}\")\n",
    "print(f\"Result : {result_3}\")\n",
    "print()\n",
    "print(\"The agent correctly refuses to make a credit approval decision.\")"
   ]
  },
  {
   "id": "e1f2a3b4c",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Group 3: Control & Guardrails\n",
    "\n",
    "**Apply bounded loops and policy enforcement**\n",
    "\n",
    "| Section | Topic |\n",
    "|---------|-------|\n",
    "| 8.10 | Controlled Loops |\n",
    "| 8.11 | Why Infinite Loops Are Dangerous |\n",
    "| 8.12 | Guardrails as Code |"
   ]
  },
  {
   "id": "c5d6e7f8a",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8.10 Controlled Loops\n",
    "\n",
    "Some agent tasks require multiple steps: classify first, then retrieve, then summarise. A **controlled loop** executes these steps while keeping a hard ceiling on how many LLM calls can be made.\n",
    "\n",
    "### Why Bounded Loops Matter\n",
    "\n",
    "| Without Bounds | With Bounds |\n",
    "|---------------|-------------|\n",
    "| Loop could run indefinitely | Maximum steps enforced in code |\n",
    "| Cost escalates without limit | Budget per request is predictable |\n",
    "| Errors compound across iterations | Loop terminates before damage spreads |\n",
    "| No audit trail of steps | Each step is logged to the audit record |"
   ]
  },
  {
   "id": "a9b0c1d2e",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_loop(user_input, max_steps=3):\n",
    "    \"\"\"Run the triage agent in a bounded loop.\n",
    "    \n",
    "    In a multi-step agent the loop would continue until a terminal\n",
    "    action is reached or max_steps is exceeded.\n",
    "    \n",
    "    For the triage agent every action is terminal, so the loop\n",
    "    always completes in one step. The structure is identical to\n",
    "    what a real multi-step agent would use.\n",
    "    \"\"\"\n",
    "    audit_log = []\n",
    "    result = None\n",
    "\n",
    "    for step in range(1, max_steps + 1):\n",
    "        action = decide_action(llm_client, user_input)\n",
    "        result = triage_agent(user_input)\n",
    "\n",
    "        log_entry = {\n",
    "            \"step\": step,\n",
    "            \"input\": user_input,\n",
    "            \"action\": action,\n",
    "            \"output\": result,\n",
    "        }\n",
    "        audit_log.append(log_entry)\n",
    "\n",
    "        print(f\"  Step {step}: action={action}\")\n",
    "        print(f\"           output={result}\")\n",
    "\n",
    "        # Terminal actions stop the loop immediately\n",
    "        if action in {\"retrieve_and_answer\", \"classify_only\", \"refuse\"}:\n",
    "            print(f\"  Loop terminated at step {step} (terminal action reached).\")\n",
    "            break\n",
    "\n",
    "    else:\n",
    "        print(f\"  Loop terminated: max_steps={max_steps} reached.\")\n",
    "\n",
    "    return result, audit_log\n",
    "\n",
    "\n",
    "print(\"=== Controlled Loop Demo ===\")\n",
    "final_result, log = agent_loop(\"Why did the central bank raise rates?\", max_steps=3)\n",
    "print()\n",
    "print(f\"Final result : {final_result}\")\n",
    "print(f\"Audit entries: {len(log)}\")"
   ]
  },
  {
   "id": "e3f4a5b6c",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8.11 Why Infinite Loops Are Dangerous\n",
    "\n",
    "Without a step limit, an agent that repeatedly calls an LLM will:\n",
    "\n",
    "1. **Exhaust compute budget** — each LLM call has a monetary cost\n",
    "2. **Compound errors** — a confused agent doubles down on bad decisions\n",
    "3. **Block resources** — threads, connections, and memory are held\n",
    "4. **Produce unbounded logs** — storage and monitoring costs spike\n",
    "\n",
    "### Safe Demonstration of Unbounded Behaviour\n",
    "\n",
    "The cell below proves that without a cap, the loop would run forever. It uses a counter to demonstrate this safely — no actual LLM calls are made.\n",
    "\n",
    "> **Key Insight:** Termination conditions must be defined in code, not trusted to the LLM. The LLM cannot reliably decide when to stop looping."
   ]
  },
  {
   "id": "c7d8e9f0b",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safe proof-of-concept: what would happen without a bound\n",
    "# We use a safety counter to stop after 5 iterations\n",
    "# In a real unbounded loop this would continue indefinitely\n",
    "\n",
    "SAFETY_LIMIT = 5   # only for demonstration; a real agent would have no limit\n",
    "\n",
    "def unsafe_loop_demo(user_input):\n",
    "    \"\"\"Demonstrates what an unbounded loop looks like.\n",
    "    \n",
    "    The SAFETY_LIMIT exists only to make this cell safe to run.\n",
    "    Without it, the loop would run until the process is killed.\n",
    "    \"\"\"\n",
    "    iteration = 0\n",
    "    while True:   # <-- no max_steps\n",
    "        iteration += 1\n",
    "        print(f\"  Iteration {iteration}: calling LLM (simulated)...\")\n",
    "\n",
    "        if iteration >= SAFETY_LIMIT:\n",
    "            print(f\"  [SAFETY LIMIT] Stopping after {SAFETY_LIMIT} iterations for demo.\")\n",
    "            print(f\"  In production this loop would have continued indefinitely.\")\n",
    "            break\n",
    "\n",
    "print(\"=== Demonstration: Unbounded Loop (safe, uses counter) ===\")\n",
    "unsafe_loop_demo(\"some input\")\n",
    "print()\n",
    "print(\"Lesson: always set max_steps when building agent loops.\")\n",
    "print(\"Never rely on the LLM to terminate the loop on its own.\")"
   ]
  },
  {
   "id": "a1b2c3d4b",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8.12 Guardrails as Code\n",
    "\n",
    "Guardrails are pre-conditions checked **before** the agent runs. They reject inputs that would cause the agent to fail, produce poor output, or behave ambiguously.\n",
    "\n",
    "### Guardrail Examples\n",
    "\n",
    "| Guardrail | Condition | Reason |\n",
    "|-----------|-----------|--------|\n",
    "| Minimum length | Input < 8 characters | Too short to classify meaningfully |\n",
    "| Low-confidence markers | Contains \"maybe\" or \"not sure\" | Ambiguous input leads to poor decisions |\n",
    "| Profanity / PII filter | Contains sensitive patterns | Compliance requirement |\n",
    "| Maximum length | Input > 2000 characters | Protects prompt budget |\n",
    "\n",
    "> **Key Insight:** Guardrails should be applied before the LLM is called. Rejecting bad input early is faster, cheaper, and safer than letting the agent attempt to handle it."
   ]
  },
  {
   "id": "e5f6a7b8d",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triage_agent_with_guardrails(user_input):\n",
    "    \"\"\"Triage agent with input guardrails applied before the LLM is called.\n",
    "    \n",
    "    Guardrail 1: reject inputs shorter than 8 characters.\n",
    "    Guardrail 2: reject inputs containing uncertainty markers.\n",
    "    Otherwise: delegate to the standard triage agent.\n",
    "    \"\"\"\n",
    "    # Guardrail 1: minimum length\n",
    "    if len(user_input.strip()) < 8:\n",
    "        return \"Guardrail: input too short to process (minimum 8 characters).\"\n",
    "\n",
    "    # Guardrail 2: low-confidence markers indicate the user is uncertain\n",
    "    low_confidence_markers = [\"maybe\", \"not sure\", \"i think\", \"possibly\"]\n",
    "    if any(marker in user_input.lower() for marker in low_confidence_markers):\n",
    "        return \"Guardrail: input contains uncertainty markers. Please rephrase as a specific question.\"\n",
    "\n",
    "    # All guardrails passed: delegate to triage agent\n",
    "    return triage_agent(user_input)\n",
    "\n",
    "\n",
    "# Demonstrate guardrails triggering and passing\n",
    "guardrail_tests = [\n",
    "    \"hi\",                                          # too short\n",
    "    \"Maybe the bank raised rates?\",                # low confidence marker\n",
    "    \"Not sure why inflation is high.\",             # low confidence marker\n",
    "    \"Why did the central bank raise rates?\",       # passes all guardrails\n",
    "]\n",
    "\n",
    "print(\"=== Guardrail Demonstrations ===\")\n",
    "for text in guardrail_tests:\n",
    "    result = triage_agent_with_guardrails(text)\n",
    "    print(f\"  Input : {text}\")\n",
    "    print(f\"  Result: {result}\")\n",
    "    print()"
   ]
  },
  {
   "id": "c9d0e1f2b",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Group 4: Failure Modes & Auditability\n",
    "\n",
    "**Handle failures and build audit trails**\n",
    "\n",
    "| Section | Topic |\n",
    "|---------|-------|\n",
    "| 8.13 | Logging Agent Decisions |\n",
    "| 8.14 | Failure Mode Demonstrations |"
   ]
  },
  {
   "id": "a3b4c5d6b",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8.13 Logging Agent Decisions\n",
    "\n",
    "Every agent decision should be logged. The audit trail serves three purposes:\n",
    "\n",
    "| Purpose | What to Log |\n",
    "|---------|-------------|\n",
    "| **Debugging** | Input, action chosen, output produced |\n",
    "| **Compliance** | Timestamp, step number, refusal reason |\n",
    "| **Improvement** | Patterns of refusal, unexpected actions |\n",
    "\n",
    "An auditable agent makes the same decisions as a non-auditable one but records evidence that decisions were made safely."
   ]
  },
  {
   "id": "e7f8a9b0d",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auditable_triage_agent(user_input, step_number=1):\n",
    "    \"\"\"Triage agent that returns both the result and a structured audit record.\"\"\"\n",
    "    action = decide_action(llm_client, user_input)\n",
    "\n",
    "    if action == \"retrieve_and_answer\":\n",
    "        output = retrieve_and_answer(user_input)\n",
    "    elif action == \"classify_only\":\n",
    "        output = classify_only(user_input)\n",
    "    else:\n",
    "        output = refuse()\n",
    "\n",
    "    audit_record = {\n",
    "        \"step\": step_number,\n",
    "        \"input\": user_input,\n",
    "        \"action_chosen\": action,\n",
    "        \"output\": output,\n",
    "    }\n",
    "\n",
    "    return output, audit_record\n",
    "\n",
    "\n",
    "# Run three queries and collect audit records\n",
    "audit_queries = [\n",
    "    \"Why did the central bank raise rates?\",\n",
    "    \"Classify this statement about monetary policy.\",\n",
    "    \"Should we approve a loan for customer X?\",\n",
    "]\n",
    "\n",
    "all_audit_records = []\n",
    "\n",
    "print(\"=== Auditable Agent Runs ===\")\n",
    "for i, query in enumerate(audit_queries, start=1):\n",
    "    result, record = auditable_triage_agent(query, step_number=i)\n",
    "    all_audit_records.append(record)\n",
    "    print(f\"Step {i}: {record['action_chosen']} -> {result}\")\n",
    "\n",
    "print()\n",
    "print(\"=== Full Audit Log (JSON) ===\")\n",
    "print(json.dumps(all_audit_records, indent=2))"
   ]
  },
  {
   "id": "c1d2e3f4b",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8.14 Failure Mode Demonstrations\n",
    "\n",
    "Three failure scenarios show how the agent degrades safely:\n",
    "\n",
    "| Scenario | Input | Expected Behaviour |\n",
    "|----------|-------|-------------------|\n",
    "| Off-topic | Sports question | Agent refuses |\n",
    "| Malformed LLM response | Non-JSON string from LLM | Fallback to refuse |\n",
    "| Ambiguous input | Mixed signals | Classification as safe fallback |"
   ]
  },
  {
   "id": "a5b6c7d8b",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Failure Mode 1: Off-topic input -> agent refuses\n",
    "print(\"=== Failure Mode 1: Off-topic input ===\")\n",
    "query = \"Who won the championship football match last night?\"\n",
    "result, record = auditable_triage_agent(query, step_number=1)\n",
    "print(f\"Input  : {query}\")\n",
    "print(f\"Action : {record['action_chosen']}\")\n",
    "print(f\"Output : {result}\")\n",
    "print(\"[CORRECT] Agent refuses because sports is not in the knowledge domain.\")\n",
    "print()"
   ]
  },
  {
   "id": "e9f0a1b2d",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Failure Mode 2: Malformed LLM response -> fallback to refuse\n",
    "\n",
    "class BrokenMockLLMClient:\n",
    "    \"\"\"Simulates an LLM that returns malformed output.\"\"\"\n",
    "    def chat(self, prompt):\n",
    "        return \"I choose: retrieve_and_answer\"  # not valid JSON\n",
    "\n",
    "\n",
    "broken_client = BrokenMockLLMClient()\n",
    "\n",
    "print(\"=== Failure Mode 2: Malformed LLM response ===\")\n",
    "raw_response = broken_client.chat(\"any prompt\")\n",
    "print(f\"Raw LLM response: {raw_response}\")\n",
    "\n",
    "# decide_action with the broken client\n",
    "malformed_action = decide_action(broken_client, \"Why did the central bank raise rates?\")\n",
    "print(f\"Action resolved : {malformed_action}\")\n",
    "print(\"[CORRECT] decide_action falls back to 'refuse' when JSON parsing fails.\")\n",
    "print()"
   ]
  },
  {
   "id": "c3d4e5f6b",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Failure Mode 3: Ambiguous input -> classification as safe fallback\n",
    "print(\"=== Failure Mode 3: Ambiguous input ===\")\n",
    "query = \"Tell me something about the topic of rates.\"\n",
    "result, record = auditable_triage_agent(query, step_number=1)\n",
    "print(f\"Input  : {query}\")\n",
    "print(f\"Action : {record['action_chosen']}\")\n",
    "print(f\"Output : {result}\")\n",
    "print(\"[NOTE] 'rate' keyword triggers retrieve_and_answer; classification would be an alternative safe fallback.\")"
   ]
  },
  {
   "id": "a7b8c9d0b",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Group 5: Automation vs Autonomy\n",
    "\n",
    "**Know when agents are appropriate and when they are not**\n",
    "\n",
    "| Section | Topic |\n",
    "|---------|-------|\n",
    "| 8.15 | When NOT to Use Agents |"
   ]
  },
  {
   "id": "e1f2a3b4d",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8.15 When NOT to Use Agents\n",
    "\n",
    "Agents are powerful but they are not the right tool for every problem. The most important engineering decision is knowing when to use a pipeline instead.\n",
    "\n",
    "### Automation vs Autonomy\n",
    "\n",
    "| Property | Automation | Autonomy |\n",
    "|----------|-----------|----------|\n",
    "| Who decides? | Code decides every step | LLM decides the next step |\n",
    "| Predictability | High | Lower |\n",
    "| Appropriate for | ETL, formatting, routing | Triage, multi-step reasoning |\n",
    "| Risk | Low | Moderate (bounded by guardrails) |\n",
    "| Auditability | Easy | Requires deliberate logging |\n",
    "\n",
    "### When to Use Agents vs Pipelines\n",
    "\n",
    "| Use an Agent | Use a Pipeline |\n",
    "|-------------|----------------|\n",
    "| The correct next step depends on the content of the input | The correct next step is always the same regardless of input |\n",
    "| There are multiple possible tools and the LLM can choose | There is only one tool or one sequence of steps |\n",
    "| The task requires multi-step reasoning | The task is single-step |\n",
    "| Routing between departments or workflows | Formatting, summarisation, translation |\n",
    "\n",
    "### Tasks Agents Should NOT Do\n",
    "\n",
    "| Task | Why Not |\n",
    "|------|--------|\n",
    "| Approve or reject financial transactions | Irreversible; requires human accountability |\n",
    "| Delete records from a database | Irreversible; no safe undo |\n",
    "| Send emails to customers | Irreversible; reputation risk |\n",
    "| Modify user permissions or roles | Security-critical; needs human review |\n",
    "| Make medical or legal recommendations | Regulated domain; liability |\n",
    "| Execute trades on behalf of clients | Financial regulation; fiduciary duty |\n",
    "\n",
    "> **Key Insight:** Enterprise organisations prefer **bounded automation** over autonomy. An agent that can refuse is more trustworthy than an agent that always acts. The goal is not maximum autonomy but maximum reliability within a defined, auditable scope.\n",
    "\n",
    "### Decision Framework\n",
    "\n",
    "```\n",
    "Is the correct next step always the same?\n",
    "         |\n",
    "     YES |                    NO\n",
    "         v                    v\n",
    "   Use a Pipeline       Is the action reversible?\n",
    "                                  |\n",
    "                        YES |          NO\n",
    "                            v          v\n",
    "                    Agent may be    Require human\n",
    "                    appropriate     approval first\n",
    "                            |\n",
    "                    Is the action in a regulated domain?\n",
    "                            |\n",
    "                    YES |       NO\n",
    "                        v       v\n",
    "                    Require   Agent with\n",
    "                    human     guardrails\n",
    "                    review    is appropriate\n",
    "```"
   ]
  },
  {
   "id": "c5d6e7f8b",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Module Summary\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "| Concept | Remember |\n",
    "|---------|----------|\n",
    "| **Pipelines vs Agents** | Pipelines have fixed steps; agents let the LLM choose the step |\n",
    "| **Propose-Validate-Execute** | The LLM proposes; code validates; system executes |\n",
    "| **Allowed Actions** | Keep the set small and explicit; anything not listed is refused |\n",
    "| **Decision Step** | Always parse LLM output safely; fall back to refuse on any error |\n",
    "| **Refusal is a Feature** | Refusing is a safe, auditable action, not a failure |\n",
    "| **Bounded Loops** | Always set max_steps; never trust the LLM to terminate a loop |\n",
    "| **Guardrails** | Validate input before calling the LLM; reject bad input early |\n",
    "| **Audit Trails** | Log every step: input, action chosen, output produced |\n",
    "\n",
    "## The Agent Mental Model\n",
    "\n",
    "> **An agent is not an autonomous system. It is a decision router with guardrails.**\n",
    ">\n",
    "> The LLM reads, proposes, and labels. The code validates, dispatches, and logs.\n",
    "> Nothing executes without the code's permission.\n",
    "\n",
    "## What's Next\n",
    "\n",
    "You now have all the components required for the capstone:\n",
    "\n",
    "- Module 5: Embeddings and vector retrieval\n",
    "- Module 6: LLM API clients and structured output\n",
    "- Module 7: RAG pipelines and grounded generation\n",
    "- Module 8: Agents, guardrails, and auditability\n",
    "\n",
    "The capstone will ask you to combine these into a bounded, auditable, production-grade agent system."
   ]
  },
  {
   "id": "a9b0c1d2b",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Practice Exercises\n",
    "\n",
    "## Exercise 1: Extend the Allowed Action Set\n",
    "\n",
    "Add a fourth action called `summarise` to `ALLOWED_ACTIONS`. Implement a `summarise(text)` tool function that returns the first sentence of the input as a summary. Update `triage_agent` to dispatch to this new tool. Update `MockLLMClient` so that inputs containing the word \"summarise\" or \"summary\" return the new action.\n",
    "\n",
    "## Exercise 2: Add a Third Guardrail\n",
    "\n",
    "Extend `triage_agent_with_guardrails` to add a third guardrail: reject any input longer than 200 characters with the message `\"Guardrail: input too long (maximum 200 characters).\"`. Test it with an input that exceeds the limit.\n",
    "\n",
    "## Exercise 3: Build an Auditable Loop\n",
    "\n",
    "Write a function `run_batch(queries, max_steps=3)` that accepts a list of query strings, runs each through `auditable_triage_agent`, collects all audit records, and prints a summary table showing: query (truncated to 40 chars), action chosen, and first 40 chars of output.\n",
    "\n",
    "## Exercise 4: Simulate a Regulated Task Rejection\n",
    "\n",
    "Create a `compliance_guardrail(user_input)` function that checks whether the input contains any of the following high-risk phrases: `\"approve loan\"`, `\"delete record\"`, `\"send email\"`, `\"modify permission\"`. If any are found, return a compliance refusal message before the agent is ever called. Demonstrate it with two inputs: one that triggers it and one that passes through to the agent."
   ]
  }
 ]
}